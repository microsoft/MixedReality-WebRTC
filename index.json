{
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrame.html",
    "title": "Struct Argb32VideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct Argb32VideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). The ARGB components are in the order of a little endian 32-bit integer, so 0xAARRGGBB, or (B, G, R, A) as a sequence of bytes in memory with B first and A last. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public ref struct Argb32VideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source data Pointer to the data buffer containing the ARBG data for each pixel. Declaration public IntPtr data Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source stride Stride in bytes between the ARGB rows. Declaration public int stride Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameDelegate.html",
    "title": "Delegate Argb32VideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate Argb32VideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void Argb32VideoFrameDelegate(Argb32VideoFrame frame); Parameters Type Name Description Argb32VideoFrame frame The newly available ARGB-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate.html",
    "title": "Delegate Argb32VideoFrameRequestDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate Argb32VideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void Argb32VideoFrameRequestDelegate(in FrameRequest request); Parameters Type Name Description FrameRequest request The request to fulfill with a new ARGB32 video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.html",
    "title": "Class Argb32VideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Class Argb32VideoFrameStorage Storage for a video frame encoded in ARGB format. Inheritance Object Argb32VideoFrameStorage Implements IVideoFrameStorage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class Argb32VideoFrameStorage : object, IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw byte buffer containing the frame data. Declaration public byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Total capacity of the storage, in bytes. This can be assigned to resize the storage. Declaration public ulong Capacity { get; set; } Property Value Type Description UInt64 Remarks Reading this property is equivalent to reading the property of Buffer . | Improve this Doc View Source Height Frame height, in pixels. Declaration public uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration public uint Width { get; set; } Property Value Type Description UInt32 Implements IVideoFrameStorage"
  },
  "api/Microsoft.MixedReality.WebRTC.AudioDeviceModule.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioDeviceModule.html",
    "title": "Enum AudioDeviceModule | MixedReality-WebRTC Documentation",
    "keywords": "Enum AudioDeviceModule Audio device module for Windows Desktop platform. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum AudioDeviceModule : byte Fields Name Description DefaultModule New CoreAudio based audio device module (ADM2). This is the default. LegacyModule Legacy audio device module (ADM1); not recommended unless there is an issue with the new default one."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrame.html",
    "title": "Struct AudioFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct AudioFrame Single raw uncompressed audio frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public ref struct AudioFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source audioData Buffer of audio samples for all channels. Declaration public IntPtr audioData Field Value Type Description IntPtr | Improve this Doc View Source bitsPerSample Number of bits per sample, generally 8 or 16. Declaration public uint bitsPerSample Field Value Type Description UInt32 | Improve this Doc View Source channelCount Number of audio channels. Declaration public uint channelCount Field Value Type Description UInt32 | Improve this Doc View Source sampleCount Number of consecutive samples in the audio data buffer. WebRTC generally delivers frames in 10ms chunks, so for e.g. a 16 kHz sample rate the sample count would be 1000. Declaration public uint sampleCount Field Value Type Description UInt32 | Improve this Doc View Source sampleRate Sample rate, in Hz. Generally in the range 8-48 kHz. Declaration public uint sampleRate Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html",
    "title": "Delegate AudioFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void AudioFrameDelegate(AudioFrame frame); Parameters Type Name Description AudioFrame frame The newly available audio frame."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.html",
    "title": "Class AudioTrackReadBuffer | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioTrackReadBuffer High level interface for consuming WebRTC audio tracks. Enqueues audio frames for a RemoteAudioTrack in an internal buffer as they arrive. Users should call Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) to read samples from the buffer when needed. Inheritance Object AudioTrackReadBuffer Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class AudioTrackReadBuffer : IDisposable Methods | Improve this Doc View Source Dispose() Release the buffer. Declaration public void Dispose() | Improve this Doc View Source Read(Int32, Int32, Single[], AudioTrackReadBuffer.PadBehavior) Fill samplesOut with samples from the internal buffer. See Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) . Declaration public void Read(int sampleRate, int channels, float[] samplesOut, AudioTrackReadBuffer.PadBehavior padBehavior = default(AudioTrackReadBuffer.PadBehavior)) Parameters Type Name Description Int32 sampleRate Int32 channels Single [] samplesOut AudioTrackReadBuffer.PadBehavior padBehavior | Improve this Doc View Source Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) Fill samplesOut with samples from the internal buffer. Declaration public void Read(int sampleRate, int numChannels, float[] samplesOut, out int numSamplesRead, out bool hasOverrun, AudioTrackReadBuffer.PadBehavior padBehavior = default(AudioTrackReadBuffer.PadBehavior)) Parameters Type Name Description Int32 sampleRate Desired sample rate. Data in the buffer is resampled if this is different from the native track rate. Int32 numChannels Desired number of channels. Should be 1 or 2. Data in the buffer is split/averaged if this is different from the native track channels number. Single [] samplesOut Will be filled with the samples read from the internal buffer. The function will try to fill the entire length of the array. Int32 numSamplesRead Set to the effective number of samples read. This will be generally equal to the length of samplesOut , but can be less in case of underrun. Boolean hasOverrun Set to true if frames have been dropped from the internal buffer between the previous call to Read and this. AudioTrackReadBuffer.PadBehavior padBehavior Controls how samplesOut is padded in case of underrun. Remarks This method reads the internal buffer starting from the oldest data. If the internal buffer is exhausted (underrun), samplesOut is padded according to the value of padBehavior . This method should be called regularly to consume the audio data as it is received. Note that the internal buffer can overrun (and some frames can be dropped) if this is not called frequently enough. See Also CreateReadBuffer()"
  },
  "api/Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior.html",
    "title": "Enum AudioTrackReadBuffer.PadBehavior | MixedReality-WebRTC Documentation",
    "keywords": "Enum AudioTrackReadBuffer.PadBehavior Controls the padding behavior of Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) on underrun. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum PadBehavior : int Fields Name Description DoNotPad Do not pad the samples array. PadWithSine Pad with a sine function. PadWithZero Pad with zeros (silence)."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioTrackSource.html",
    "title": "Class AudioTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioTrackSource Audio source for WebRTC audio tracks. The audio source is not bound to any peer connection, and can therefore be shared by multiple audio tracks from different peer connections. This is especially useful to share local audio capture devices (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology (one connection per pair of peers). The user owns the audio track source, and is in charge of keeping it alive until after all tracks using it are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still using it is undefined. The Tracks property contains the list of tracks currently using the source. Inheritance Object AudioTrackSource DeviceAudioTrackSource Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public abstract class AudioTrackSource : IDisposable Properties | Improve this Doc View Source Name A name for the audio track source, used for logging and debugging. Declaration public string Name { get; set; } Property Value Type Description String | Improve this Doc View Source Tracks List of local audio tracks this source is providing raw audio frames to. Declaration public IReadOnlyList<LocalAudioTrack> Tracks { get; } Property Value Type Description IReadOnlyList < LocalAudioTrack > Methods | Improve this Doc View Source Dispose() Declaration public void Dispose() | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String See Also LocalAudioTrack"
  },
  "api/Microsoft.MixedReality.WebRTC.BufferTooSmallException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.BufferTooSmallException.html",
    "title": "Class BufferTooSmallException | MixedReality-WebRTC Documentation",
    "keywords": "Class BufferTooSmallException Exception raised when a buffer is too small to perform the current operation. Generally the buffer was provided by the caller, and this indicates that the caller must provide a larger buffer. Inheritance Object BufferTooSmallException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class BufferTooSmallException : Exception Constructors | Improve this Doc View Source BufferTooSmallException() Declaration public BufferTooSmallException() | Improve this Doc View Source BufferTooSmallException(String) Declaration public BufferTooSmallException(string message) Parameters Type Name Description String message | Improve this Doc View Source BufferTooSmallException(String, Exception) Declaration public BufferTooSmallException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.BundlePolicy.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.BundlePolicy.html",
    "title": "Enum BundlePolicy | MixedReality-WebRTC Documentation",
    "keywords": "Enum BundlePolicy Bundle policy. See https://www.w3.org/TR/webrtc/#rtcbundlepolicy-enum . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum BundlePolicy : int Fields Name Description Balanced Gather ICE candidates for each media type in use (audio, video, and data). If the remote endpoint is not bundle-aware, negotiate only one audio and video track on separate transports. MaxBundle Gather ICE candidates for only one track. If the remote endpoint is not bundle-aware, negotiate only one media track. MaxCompat Gather ICE candidates for each track. If the remote endpoint is not bundle-aware, negotiate all media tracks on separate transports."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.BufferingChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.BufferingChangedDelegate.html",
    "title": "Delegate DataChannel.BufferingChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate DataChannel.BufferingChangedDelegate Delegate for the BufferingChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void BufferingChangedDelegate(ulong previous, ulong current, ulong limit); Parameters Type Name Description UInt64 previous Previous buffering size, in bytes. UInt64 current New buffering size, in bytes. UInt64 limit Maximum buffering size, in bytes."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html",
    "title": "Enum DataChannel.ChannelState | MixedReality-WebRTC Documentation",
    "keywords": "Enum DataChannel.ChannelState Connection state of a data channel. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum ChannelState : int Fields Name Description Closed The data channel reached end of life and can be destroyed. It cannot be re-connected; instead a new data channel must be created. Closing The data channel is being closed, and is not available anymore for data exchange. Connecting The data channel has just been created, and negotiating is underway to establish a link between the peers. The data channel cannot be used to send/receive yet. Open The data channel is open and ready to send and receive messages."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.html",
    "title": "Class DataChannel | MixedReality-WebRTC Documentation",
    "keywords": "Class DataChannel Encapsulates a data channel of a peer connection. A data channel is a \"pipe\" allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ https://www.w3.org/TR/webrtc/ An instance of DataChannel is created either by manually calling or one of its variants, or automatically by the implementation when a new data channel is created in-band by the remote peer ( DataChannelAdded ). DataChannel cannot be instantiated directly. Inheritance Object DataChannel Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DataChannel : object Properties | Improve this Doc View Source ID The unique identifier of the data channel in the current connection. Declaration public int ID { get; } Property Value Type Description Int32 | Improve this Doc View Source Label The data channel name in the current connection. Declaration public string Label { get; } Property Value Type Description String | Improve this Doc View Source Ordered Indicates whether the data channel messages are ordered or not. Ordered messages are delivered in the order they are sent, at the cost of delaying later messages delivery to the application (via MessageReceived ) when internally arriving out of order. Declaration public bool Ordered { get; } Property Value Type Description Boolean true if messages are ordered. See Also Reliable | Improve this Doc View Source PeerConnection The PeerConnection object this data channel was created from and is attached to. Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection | Improve this Doc View Source Reliable Indicates whether the data channel messages are reliably delivered. Reliable messages are guaranteed to be delivered as long as the connection is not dropped. Unreliable messages may be silently dropped for whatever reason, and the implementation will not try to detect this nor resend them. Declaration public bool Reliable { get; } Property Value Type Description Boolean true if messages are reliable. See Also Ordered | Improve this Doc View Source State The channel connection state represents the connection status. Changes to this state are notified via the StateChanged event. Declaration public DataChannel.ChannelState State { get; } Property Value Type Description DataChannel.ChannelState The channel connection state. Remarks The code handling this event should unwind the stack before using any other MR-WebRTC APIs; re-entrancy is not supported. See Also StateChanged Methods | Improve this Doc View Source SendMessage(IntPtr, UInt64) Send a message through the data channel. If the message cannot be sent, for example because of congestion control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted. The internal buffering is monitored via the BufferingChanged event. Declaration public void SendMessage(IntPtr message, ulong size) Parameters Type Name Description IntPtr message The message to send to the remote peer. UInt64 size The size of the message to send in octects. See Also InitializeAsync(PeerConnectionConfiguration, CancellationToken) BufferingChanged | Improve this Doc View Source SendMessage(Byte[]) Send a message through the data channel. If the message cannot be sent, for example because of congestion control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted. The internal buffering is monitored via the BufferingChanged event. Declaration public void SendMessage(byte[] message) Parameters Type Name Description Byte [] message The message to send to the remote peer. Exceptions Type Condition DataChannelNotOpenException The data channel is not open yet. See Also InitializeAsync(PeerConnectionConfiguration, CancellationToken) BufferingChanged | Improve this Doc View Source SendMessageEx(DataChannel.MessageKind, IntPtr, UInt64) Send a message through the data channel with the specified kind. If the message cannot be sent, for example because of congestion control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted. The internal buffering is monitored via the BufferingChanged event. Declaration public void SendMessageEx(DataChannel.MessageKind messageKind, IntPtr message, ulong size) Parameters Type Name Description DataChannel.MessageKind messageKind The kind of message to send to the remote peer. IntPtr message The message to send to the remote peer. UInt64 size The size of the message to send in octects. Exceptions Type Condition DataChannelNotOpenException The data channel is not open yet. See Also InitializeAsync(PeerConnectionConfiguration, CancellationToken) BufferingChanged Events | Improve this Doc View Source BufferingChanged Event triggered when the data channel buffering changes. Users should monitor this to ensure calls to SendMessage(Byte[]) do not fail. Internally the data channel contains a buffer of messages to send that could not be sent immediately, for example due to congestion control. Once this buffer is full, any further call to SendMessage(Byte[]) will fail until some messages are processed and removed to make space. Declaration public event DataChannel.BufferingChangedDelegate BufferingChanged Event Type Type Description DataChannel.BufferingChangedDelegate Remarks The code handling this event should unwind the stack before using any other MR-WebRTC APIs; re-entrancy is not supported. See Also SendMessage(Byte[]) | Improve this Doc View Source MessageReceived Event triggered when a message is received through the data channel. Declaration public event Action<byte[]> MessageReceived Event Type Type Description Action < Byte []> See Also SendMessage(Byte[]) | Improve this Doc View Source MessageReceivedEx Event triggered when a message is received through the data channel, includes information if the message is binary or text. Declaration public event Action<DataChannel.MessageKind, byte[]> MessageReceivedEx Event Type Type Description Action < DataChannel.MessageKind , Byte []> See Also SendMessage(Byte[]) | Improve this Doc View Source MessageReceivedUnsafe Event fires when a message is received through the data channel. Declaration public event Action<IntPtr, ulong> MessageReceivedUnsafe Event Type Type Description Action < IntPtr , UInt64 > See Also SendMessage(IntPtr, UInt64) | Improve this Doc View Source StateChanged Event triggered when the data channel state changes. The new state is available in State . Declaration public event Action StateChanged Event Type Type Description Action Remarks The code handling this event should unwind the stack before using any other MR-WebRTC APIs; re-entrancy is not supported. See Also State See Also DataChannelAdded"
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.MessageKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.MessageKind.html",
    "title": "Enum DataChannel.MessageKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum DataChannel.MessageKind Type of message sent or received through the data channel. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum MessageKind : int Fields Name Description Binary The message is a binary message. Text The message is a text message (UTF-8 encoded string)."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannelNotOpenException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannelNotOpenException.html",
    "title": "Class DataChannelNotOpenException | MixedReality-WebRTC Documentation",
    "keywords": "Class DataChannelNotOpenException Exception thrown when trying to use a data channel that is not open. The user should listen to the StateChanged event until the State property is Open before trying to send some message with SendMessage(Byte[]) . Inheritance Object DataChannelNotOpenException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DataChannelNotOpenException : Exception Constructors | Improve this Doc View Source DataChannelNotOpenException() Declaration public DataChannelNotOpenException() | Improve this Doc View Source DataChannelNotOpenException(String) Declaration public DataChannelNotOpenException(string message) Parameters Type Name Description String message | Improve this Doc View Source DataChannelNotOpenException(String, Exception) Declaration public DataChannelNotOpenException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource.html",
    "title": "Class DeviceAudioTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class DeviceAudioTrackSource Implementation of an audio track source producing frames captured from an audio capture device (microphone). Inheritance Object AudioTrackSource DeviceAudioTrackSource Inherited Members AudioTrackSource.Name AudioTrackSource.Tracks AudioTrackSource.Dispose() Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DeviceAudioTrackSource : AudioTrackSource Methods | Improve this Doc View Source CreateAsync(LocalAudioDeviceInitConfig) Create an audio track source using a local audio capture device (microphone). Declaration public static Task<DeviceAudioTrackSource> CreateAsync(LocalAudioDeviceInitConfig initConfig = null) Parameters Type Name Description LocalAudioDeviceInitConfig initConfig Optional configuration to initialize the audio capture on the device. Returns Type Description Task < DeviceAudioTrackSource > The newly create audio track source. See Also CreateFromSource(AudioTrackSource, LocalAudioTrackInitConfig) | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides AudioTrackSource.ToString() See Also LocalAudioTrack"
  },
  "api/Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.html",
    "title": "Class DeviceVideoTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class DeviceVideoTrackSource Implementation of a video track source producing frames captured from a video capture device (webcam). Inheritance Object VideoTrackSource DeviceVideoTrackSource Implements IVideoSource IDisposable Inherited Members VideoTrackSource.Name VideoTrackSource.Tracks VideoTrackSource.I420AVideoFrameReady VideoTrackSource.Argb32VideoFrameReady VideoTrackSource.Enabled VideoTrackSource.Dispose() Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DeviceVideoTrackSource : VideoTrackSource, IVideoSource Properties | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration public override VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding Overrides VideoTrackSource.FrameEncoding Methods | Improve this Doc View Source CreateAsync(LocalVideoDeviceInitConfig) Create a video track source using a local video capture device (webcam). The video track source produces raw video frames by capturing them from a capture device accessible from the local host machine, generally a USB webcam or built-in device camera. The video source initially starts in the capturing state, and will remain live for as long as the source is alive. Once the source is not live anymore (ended), it cannot be restarted. A new source must be created to use the same video capture device again. The source can be used to create one or more local video tracks ( LocalVideoTrack ), which once added to a video transceiver allow the video frames to be sent to a remote peer. The source itself is not associated with any peer connection, and can be used to create local video tracks from multiple peer connections at once, thereby being shared amongst those peer connections. The source is owned by the user, who must ensure it stays alive while being in use by at least one local video track. Once it is not used anymore, the user is in charge of disposing of the source. Disposing of a source still in use by a local video track is undefined behavior. Declaration public static Task<DeviceVideoTrackSource> CreateAsync(LocalVideoDeviceInitConfig initConfig = null) Parameters Type Name Description LocalVideoDeviceInitConfig initConfig Optional configuration to initialize the video capture on the device. Returns Type Description Task < DeviceVideoTrackSource > The newly create video track source. Remarks On UWP this requires the \"webcam\" capability. See https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations for more details. The video capture device may be accessed several times during the initializing process, generally once for listing and validating the capture format, and once for actually starting the video capture. This is a limitation of the OS and/or hardware. Note that the capture device must support a capture format with the given constraints of profile ID or kind, capture resolution, and framerate, otherwise the call will fail. That is, there is no fallback mechanism selecting a closest match. Developers should use GetCaptureFormatsAsync(String) to list the supported formats ahead of calling CreateAsync(LocalVideoDeviceInitConfig) , and can build their own fallback mechanism on top of this call if needed. Examples Create a video track source with Mixed Reality Capture (MRC) enabled. This assumes that the platform supports MRC. Note that if MRC is not available the call will still succeed, but will return a track without MRC enabled. var initConfig = new LocalVideoDeviceInitConfig { enableMrc = true }; var videoSource = await VideoTrackSource.CreateFromDeviceAsync(initConfig); Create a video track source from a local webcam, asking for a capture format suited for video conferencing, and a target framerate of 30 frames per second (FPS). The implementation will select an appropriate capture resolution. This assumes that the device supports video profiles, and has at least one capture format supporting exactly 30 FPS capture associated with the VideoConferencing profile. Otherwise the call will fail. var initConfig = new LocalVideoDeviceInitConfig { videoProfileKind = VideoProfileKind.VideoConferencing, framerate = 30.0 }; var videoSource = await VideoTrackSource.CreateFromDeviceAsync(initConfig); See Also CreateFromSource(VideoTrackSource, LocalVideoTrackInitConfig) | Improve this Doc View Source GetCaptureDevicesAsync() Get the list of video capture devices available on the local host machine. Declaration public static Task<IReadOnlyList<VideoCaptureDevice>> GetCaptureDevicesAsync() Returns Type Description Task < IReadOnlyList < VideoCaptureDevice >> The list of available video capture devices. Remarks Assign one of the returned VideoCaptureDevice to the videoDevice field to force a local video track to use that device when creating it with CreateAsync(LocalVideoDeviceInitConfig) . | Improve this Doc View Source GetCaptureFormatsAsync(String) Enumerate the video capture formats for the specified video capture device. Declaration public static Task<IReadOnlyList<VideoCaptureFormat>> GetCaptureFormatsAsync(string deviceId) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetCaptureDevicesAsync() . Returns Type Description Task < IReadOnlyList < VideoCaptureFormat >> The list of available video capture formats for the specified video capture device. | Improve this Doc View Source GetCaptureFormatsAsync(String, VideoProfileKind) Enumerate the video capture formats for the specified video capture device and video profile. Declaration public static Task<IReadOnlyList<VideoCaptureFormat>> GetCaptureFormatsAsync(string deviceId, VideoProfileKind profileKind) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetCaptureDevicesAsync() . VideoProfileKind profileKind Kind of video profile to enumerate the capture formats of. Returns Type Description Task < IReadOnlyList < VideoCaptureFormat >> The list of available video capture formats for the specified video capture device. | Improve this Doc View Source GetCaptureFormatsAsync(String, String) Enumerate the video capture formats for the specified video capture device and video profile. Declaration public static Task<IReadOnlyList<VideoCaptureFormat>> GetCaptureFormatsAsync(string deviceId, string profileId) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetCaptureDevicesAsync() . String profileId Unique identifier of the video profile to enumerate the capture formats of, as retrieved from id field of a capture device enumerated with GetCaptureDevicesAsync() . Returns Type Description Task < IReadOnlyList < VideoCaptureFormat >> The list of available video capture formats for the specified video capture device. | Improve this Doc View Source GetCaptureProfilesAsync(String) Enumerate all the video profiles associated with the specified video capture device, if any. Declaration public static Task<IReadOnlyList<VideoProfile>> GetCaptureProfilesAsync(string deviceId) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetCaptureDevicesAsync() . Returns Type Description Task < IReadOnlyList < VideoProfile >> The list of available video profiles for the specified video capture device. Remarks If the video capture device does not support video profiles, the function succeeds and returns an empty list. This is equivalent to: GetCaptureProfilesAsync(deviceId, VideoProfileKind.Unspecified); See Also GetCaptureProfilesAsync(String, VideoProfileKind) | Improve this Doc View Source GetCaptureProfilesAsync(String, VideoProfileKind) Enumerate the video profiles associated with the specified video capture device, if any, and restricted to the specified video profile kind. Declaration public static Task<IReadOnlyList<VideoProfile>> GetCaptureProfilesAsync(string deviceId, VideoProfileKind profileKind) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetCaptureDevicesAsync() . VideoProfileKind profileKind Kind of video profile to enumerate. Specify Unspecified to enumerate all profiles. Returns Type Description Task < IReadOnlyList < VideoProfile >> The list of available video profiles for the specified video capture device. Remarks If the video capture device does not support video profiles, the function succeeds and returns an empty list. See Also GetCaptureProfilesAsync(String) | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides VideoTrackSource.ToString() Implements IVideoSource IDisposable"
  },
  "api/Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.html",
    "title": "Class ExternalVideoTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class ExternalVideoTrackSource Video source for WebRTC video tracks based on a custom source of video frames managed by the user and external to the WebRTC implementation. This class is used to inject into the WebRTC engine a video track whose frames are produced by a user-managed source the WebRTC engine knows nothing about, like programmatically generated frames, including frames not strictly of video origin like a 3D rendered scene, or frames coming from a specific capture device not supported natively by WebRTC. This class serves as an adapter for such video frame sources. Inheritance Object VideoTrackSource ExternalVideoTrackSource Implements IVideoSource IDisposable Inherited Members VideoTrackSource.Name VideoTrackSource.Tracks VideoTrackSource.I420AVideoFrameReady VideoTrackSource.Argb32VideoFrameReady VideoTrackSource.Enabled Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class ExternalVideoTrackSource : VideoTrackSource, IVideoSource Fields | Improve this Doc View Source _frameRequestCallbackArgsHandle GC handle to frame request callback args keeping the delegate alive while the callback is registered with the native implementation. Declaration protected IntPtr _frameRequestCallbackArgsHandle Field Value Type Description IntPtr Properties | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration public override VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding Overrides VideoTrackSource.FrameEncoding Methods | Improve this Doc View Source CompleteFrameRequest(UInt32, Int64, in Argb32VideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) . Declaration public void CompleteFrameRequest(uint requestId, long timestampMs, in Argb32VideoFrame frame) Parameters Type Name Description UInt32 requestId The original request ID. Int64 timestampMs The video frame timestamp. Argb32VideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CompleteFrameRequest(UInt32, Int64, in I420AVideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromI420ACallback(I420AVideoFrameRequestDelegate) . Declaration public void CompleteFrameRequest(uint requestId, long timestampMs, in I420AVideoFrame frame) Parameters Type Name Description UInt32 requestId The original request ID. Int64 timestampMs The video frame timestamp. I420AVideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) Create a new external video track source from a given user callback providing ARGB32-encoded frames. Declaration public static ExternalVideoTrackSource CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate frameCallback) Parameters Type Name Description Argb32VideoFrameRequestDelegate frameCallback The callback that will be used to request frames for tracks. Returns Type Description ExternalVideoTrackSource The newly created track source. | Improve this Doc View Source CreateFromI420ACallback(I420AVideoFrameRequestDelegate) Create a new external video track source from a given user callback providing I420A-encoded frames. Declaration public static ExternalVideoTrackSource CreateFromI420ACallback(I420AVideoFrameRequestDelegate frameCallback) Parameters Type Name Description I420AVideoFrameRequestDelegate frameCallback The callback that will be used to request frames for tracks. Returns Type Description ExternalVideoTrackSource The newly created track source. | Improve this Doc View Source Dispose() Declaration public override void Dispose() Overrides VideoTrackSource.Dispose() | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides VideoTrackSource.ToString() Implements IVideoSource IDisposable"
  },
  "api/Microsoft.MixedReality.WebRTC.FrameRequest.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.FrameRequest.html",
    "title": "Struct FrameRequest | MixedReality-WebRTC Documentation",
    "keywords": "Struct FrameRequest Request sent to an external video source via its registered callback to generate a new video frame for the track(s) connected to it. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public ref struct FrameRequest Fields | Improve this Doc View Source RequestId Unique request identifier, for error checking. Declaration public uint RequestId Field Value Type Description UInt32 | Improve this Doc View Source Source Video track source this request is associated with. Declaration public ExternalVideoTrackSource Source Field Value Type Description ExternalVideoTrackSource | Improve this Doc View Source TimestampMs Frame timestamp, in milliseconds. This corresponds to the time when the request was made to the native video track source. Declaration public long TimestampMs Field Value Type Description Int64 Methods | Improve this Doc View Source CompleteRequest(in Argb32VideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) . Declaration public void CompleteRequest(in Argb32VideoFrame frame) Parameters Type Name Description Argb32VideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CompleteRequest(in I420AVideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromI420ACallback(I420AVideoFrameRequestDelegate) . Declaration public void CompleteRequest(in I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame The video frame used to complete the request."
  },
  "api/Microsoft.MixedReality.WebRTC.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC Classes Argb32VideoFrameStorage Storage for a video frame encoded in ARGB format. AudioTrackReadBuffer High level interface for consuming WebRTC audio tracks. Enqueues audio frames for a RemoteAudioTrack in an internal buffer as they arrive. Users should call Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) to read samples from the buffer when needed. AudioTrackSource Audio source for WebRTC audio tracks. The audio source is not bound to any peer connection, and can therefore be shared by multiple audio tracks from different peer connections. This is especially useful to share local audio capture devices (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology (one connection per pair of peers). The user owns the audio track source, and is in charge of keeping it alive until after all tracks using it are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still using it is undefined. The Tracks property contains the list of tracks currently using the source. BufferTooSmallException Exception raised when a buffer is too small to perform the current operation. Generally the buffer was provided by the caller, and this indicates that the caller must provide a larger buffer. DataChannel Encapsulates a data channel of a peer connection. A data channel is a \"pipe\" allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ https://www.w3.org/TR/webrtc/ An instance of DataChannel is created either by manually calling or one of its variants, or automatically by the implementation when a new data channel is created in-band by the remote peer ( DataChannelAdded ). DataChannel cannot be instantiated directly. DataChannelNotOpenException Exception thrown when trying to use a data channel that is not open. The user should listen to the StateChanged event until the State property is Open before trying to send some message with SendMessage(Byte[]) . DeviceAudioTrackSource Implementation of an audio track source producing frames captured from an audio capture device (microphone). DeviceVideoTrackSource Implementation of a video track source producing frames captured from a video capture device (webcam). ExternalVideoTrackSource Video source for WebRTC video tracks based on a custom source of video frames managed by the user and external to the WebRTC implementation. This class is used to inject into the WebRTC engine a video track whose frames are produced by a user-managed source the WebRTC engine knows nothing about, like programmatically generated frames, including frames not strictly of video origin like a 3D rendered scene, or frames coming from a specific capture device not supported natively by WebRTC. This class serves as an adapter for such video frame sources. I420AVideoFrameStorage Storage for a video frame encoded in I420+Alpha format. IceCandidate ICE candidate to send to a remote peer or received from it. IceServer ICE server configuration (STUN and/or TURN). InvalidInteropNativeHandleException Exception thrown when an API function expects an interop handle to a valid native object, but receives an invalid handle instead. Library Container for library-wise global settings of MixedReality-WebRTC. LocalAudioDeviceInitConfig Configuration to initialize capture on a local audio device (microphone). LocalAudioTrack Audio track sending to the remote peer audio frames originating from a local track source (local microphone or other audio recording device). LocalAudioTrackInitConfig Settings for adding a local audio track backed by a local audio capture device (e.g. microphone). LocalMediaTrack Base class for media tracks sending to the remote peer. LocalVideoDeviceInitConfig Configuration to initialize capture on a local video device (webcam). LocalVideoTrack Video track sending to the remote peer video frames originating from a local track source. LocalVideoTrackInitConfig Settings for creating a new local video track. Logging Logging utilities. MediaTrack Base class for media tracks sending to or receiving from the remote peer. MovingAverage Utility to manage a moving average of a time series. PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. PeerConnection.StatsReport Snapshot of the statistics relative to a peer connection/track. The various stats objects can be read through GetStats<T>() . PeerConnectionConfiguration Configuration to initialize a PeerConnection . RemoteAudioTrack Audio track receiving audio frames from the remote peer. RemoteVideoTrack Video track receiving video frames from the remote peer. SctpNotNegotiatedException Exception thrown when trying to add a data channel to a peer connection after a connection to a remote peer was established without an SCTP handshake. When using data channels, at least one data channel must be added to the peer connection before calling CreateOffer() to signal to the implementation the intent to use data channels and the need to perform a SCTP handshake during the connection. SdpMessage SDP message passed between the local and remote peers via the user's signaling solution. TaskExtensions Collection of extension methods for Task . Transceiver Transceiver of a peer connection. A transceiver is a media \"pipe\" connecting the local and remote peers, and used to transmit media data (audio or video) between the peers. The transceiver has a media flow direction indicating whether it is sending and/or receiving any media, or is inactive. When sending some media, the transceiver's local track is used as the source of that media. Conversely, when receiving some media, that media is delivered to the remote media track of the transceiver. As a convenience, the local track can be null if the local peer does not have anything to send. In that case some empty media is automatically sent instead (black frames for video, silence for audio) at very reduced rate. To completely stop sending, the media direction must be changed instead. Transceivers are owned by the peer connection which creates them, and cannot be destroyed nor removed from the peer connection. They become invalid when the peer connection is closed, and should not be used after that. TransceiverInitSettings Settings to create a new transceiver wrapper. VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. VideoProfile Video profile. VideoTrackSource Video source for WebRTC video tracks. The video source is not bound to any peer connection, and can therefore be shared by multiple video tracks from different peer connections. This is especially useful to share local video capture devices (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology (one connection per pair of peers). The user owns the video track source, and is in charge of keeping it alive until after all tracks using it are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still using it is undefined. The Tracks property contains the list of tracks currently using the source. Structs Argb32VideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). The ARGB components are in the order of a little endian 32-bit integer, so 0xAARRGGBB, or (B, G, R, A) as a sequence of bytes in memory with B first and A last. AudioFrame Single raw uncompressed audio frame. FrameRequest Request sent to an external video source via its registered callback to generate a new video frame for the track(s) connected to it. I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV with optional alpha plane). See e.g. https://wiki.videolan.org/YUV/#I420 for details. The I420 format uses chroma downsampling in both directions, resulting in 12 bits per pixel. With the optional alpha plane, the size increases to 20 bits per pixel. PeerConnection.AudioReceiverStats Subset of RTCMediaStreamTrack (audio receiver) and RTCInboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#aststats-dict* and https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict* . PeerConnection.AudioSenderStats Subset of RTCMediaStreamTrack (audio sender) and RTCOutboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#raststats-dict* and https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict* . PeerConnection.DataChannelStats Subset of RTCDataChannelStats. See https://www.w3.org/TR/webrtc-stats/#dcstats-dict* PeerConnection.H264Config Configuration for the Media Foundation H.264 encoder. PeerConnection.TransportStats Subset of RTCTransportStats. See https://www.w3.org/TR/webrtc-stats/#transportstats-dict* . PeerConnection.VideoReceiverStats Subset of RTCMediaStreamTrack (video receiver) + RTCInboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#rvststats-dict* and https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict* PeerConnection.VideoSenderStats Subset of RTCMediaStreamTrack (video sender) and RTCOutboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#vsstats-dict* and https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict* . VideoCaptureDevice Identifier for a video capture device. VideoCaptureFormat Capture format for a video track. Interfaces IAudioSource Interface for audio sources, whether local sources/tracks or remote tracks. ILogSink Interface for a sink receiving log messages. The sink can be registered with AddSink(ILogSink, LogSeverity) to receive logging messages. IVideoFrameQueue Interface for a queue of video frames. IVideoFrameStorage Interface for a storage of a single video frame. IVideoSource Interface for video sources, whether local or remote. Enums AudioDeviceModule Audio device module for Windows Desktop platform. AudioTrackReadBuffer.PadBehavior Controls the padding behavior of Read(Int32, Int32, Single[], out Int32, out Boolean, AudioTrackReadBuffer.PadBehavior) on underrun. BundlePolicy Bundle policy. See https://www.w3.org/TR/webrtc/#rtcbundlepolicy-enum . DataChannel.ChannelState Connection state of a data channel. DataChannel.MessageKind Type of message sent or received through the data channel. IceConnectionState State of an ICE connection. IceGatheringState State of an ICE gathering process. IceTransportType Type of ICE candidates offered to the remote peer. Library.ShutdownOptionsFlags Options for library shutdown. LogSeverity Log message severity. MediaKind Type of media track or media transceiver. PeerConnection.FrameHeightRoundMode Frame height round mode. PeerConnection.H264Profile H.264 Encoding profile. PeerConnection.H264RcMode Rate control mode for the Media Foundation H.264. See https://docs.microsoft.com/en-us/windows/win32/medfound/h-264-video-encoder for details. PeerConnection.TrackKind Kind of WebRTC track. SdpMessageType Type of SDP message. SdpSemantic SDP semantic used for (re)negotiating a peer connection. Transceiver.Direction Direction of the media flowing inside the transceiver. VideoEncoding Enumeration of video encodings. VideoProfileKind Kind of video profile. This corresponds to the enum of the API. Delegates Argb32VideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. Argb32VideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. DataChannel.BufferingChangedDelegate Delegate for the BufferingChanged event. I420AVideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. I420AVideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. PeerConnection.AudioTrackAddedDelegate Delegate for AudioTrackAdded event. PeerConnection.AudioTrackRemovedDelegate Delegate for AudioTrackRemoved event. PeerConnection.DataChannelAddedDelegate Delegate for DataChannelAdded event. PeerConnection.DataChannelRemovedDelegate Delegate for DataChannelRemoved event. PeerConnection.IceCandidateReadytoSendDelegate Delegate for the IceCandidateReadytoSend event. PeerConnection.IceGatheringStateChangedDelegate Delegate for the IceGatheringStateChanged event. PeerConnection.IceStateChangedDelegate Delegate for the IceStateChanged event. PeerConnection.LocalSdpReadyToSendDelegate Delegate for LocalSdpReadytoSend event. PeerConnection.TransceiverAddedDelegate Delegate for TransceiverAdded event. PeerConnection.VideoTrackAddedDelegate Delegate for VideoTrackAdded event. PeerConnection.VideoTrackRemovedDelegate Delegate for VideoTrackRemoved event. TransceiverAssociatedDelegate Delegate for the Associated event. TransceiverDirectionChangedDelegate Delegate for the DirectionChanged event."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html",
    "title": "Struct I420AVideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV with optional alpha plane). See e.g. https://wiki.videolan.org/YUV/#I420 for details. The I420 format uses chroma downsampling in both directions, resulting in 12 bits per pixel. With the optional alpha plane, the size increases to 20 bits per pixel. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public ref struct I420AVideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. The alpha plane is generically supported in this data structure, but actual support in the video tracks depend on the underlying implementation and the video codec used, and is generally not available. Fields | Improve this Doc View Source dataA Optional pointer to the alpha plane buffer, if any, or null if the frame has no alpha plane. Declaration public IntPtr dataA Field Value Type Description IntPtr | Improve this Doc View Source dataU Pointer to the U plane buffer. Declaration public IntPtr dataU Field Value Type Description IntPtr | Improve this Doc View Source dataV Pointer to the V plane buffer. Declaration public IntPtr dataV Field Value Type Description IntPtr | Improve this Doc View Source dataY Pointer to the Y plane buffer. Declaration public IntPtr dataY Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source strideA Stride in bytes between rows of the A plane, if present. Declaration public int strideA Field Value Type Description Int32 | Improve this Doc View Source strideU Stride in bytes between rows of the U plane. Declaration public int strideU Field Value Type Description Int32 | Improve this Doc View Source strideV Stride in bytes between rows of the V plane. Declaration public int strideV Field Value Type Description Int32 | Improve this Doc View Source strideY Stride in bytes between rows of the Y plane. Declaration public int strideY Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32 Methods | Improve this Doc View Source CopyTo(Byte[]) Copy the frame content to a Byte [] buffer as a contiguous block of memory containing the Y, U, and V planes one after another, and the alpha plane at the end if present. Declaration public void CopyTo(byte[] buffer) Parameters Type Name Description Byte [] buffer The destination buffer to copy the frame to."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameDelegate.html",
    "title": "Delegate I420AVideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate I420AVideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void I420AVideoFrameDelegate(I420AVideoFrame frame); Parameters Type Name Description I420AVideoFrame frame The newly available I420-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate.html",
    "title": "Delegate I420AVideoFrameRequestDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate I420AVideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void I420AVideoFrameRequestDelegate(in FrameRequest request); Parameters Type Name Description FrameRequest request The request to fulfill with a new I420A video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.html",
    "title": "Class I420AVideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Class I420AVideoFrameStorage Storage for a video frame encoded in I420+Alpha format. Inheritance Object I420AVideoFrameStorage Implements IVideoFrameStorage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class I420AVideoFrameStorage : object, IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw byte buffer containing the frame data. Declaration public byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Total capacity of the storage, in bytes. This can be assigned to resize the storage. Declaration public ulong Capacity { get; set; } Property Value Type Description UInt64 Remarks Reading this property is equivalent to reading the property of Buffer . | Improve this Doc View Source Height Frame height, in pixels. Declaration public uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration public uint Width { get; set; } Property Value Type Description UInt32 Implements IVideoFrameStorage"
  },
  "api/Microsoft.MixedReality.WebRTC.IAudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IAudioSource.html",
    "title": "Interface IAudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Interface IAudioSource Interface for audio sources, whether local sources/tracks or remote tracks. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IAudioSource Properties | Improve this Doc View Source Enabled Enabled status of the source. If enabled, produces audio frames as expected. If disabled, produces silence instead. Declaration bool Enabled { get; } Property Value Type Description Boolean Methods | Improve this Doc View Source CreateReadBuffer() Starts buffering the audio frames from in an AudioTrackReadBuffer . Declaration AudioTrackReadBuffer CreateReadBuffer() Returns Type Description AudioTrackReadBuffer Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady instead. Events | Improve this Doc View Source AudioFrameReady Event that occurs when a new audio frame is available from the source, either because the source produced it locally ( AudioTrackSource , LocalAudioTrack ) or because it received it from the remote peer ( RemoteAudioTrack ). Declaration event AudioFrameDelegate AudioFrameReady Event Type Type Description AudioFrameDelegate Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady . If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . See Also AudioTrackSource LocalAudioTrack RemoteAudioTrack"
  },
  "api/Microsoft.MixedReality.WebRTC.IceCandidate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceCandidate.html",
    "title": "Class IceCandidate | MixedReality-WebRTC Documentation",
    "keywords": "Class IceCandidate ICE candidate to send to a remote peer or received from it. Inheritance Object IceCandidate Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class IceCandidate : object Fields | Improve this Doc View Source Content Candidate raw content. Declaration public string Content Field Value Type Description String | Improve this Doc View Source SdpMid Media ID (m=) of the candidate. Declaration public string SdpMid Field Value Type Description String | Improve this Doc View Source SdpMlineIndex Index of the media line associated with the candidate. Declaration public int SdpMlineIndex Field Value Type Description Int32"
  },
  "api/Microsoft.MixedReality.WebRTC.IceConnectionState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceConnectionState.html",
    "title": "Enum IceConnectionState | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceConnectionState State of an ICE connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceConnectionState : int Remarks Due to the underlying implementation, this is currently a mix of the RTPIceGatheringState and the RTPPeerConnectionState from the WebRTC 1.0 standard. Fields Name Description Checking ICE connection received an offer, but transports are not writable yet. Closed The peer connection was closed entirely. Completed ICE connection finished establishing. Connected Transports are writable. Disconnected ICE connection is disconnected, there is no more writable transport. Failed Failed establishing an ICE connection. New Newly created ICE connection. This is the starting state. See Also https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum https://www.w3.org/TR/webrtc/#rtcpeerconnectionstate-enum"
  },
  "api/Microsoft.MixedReality.WebRTC.IceGatheringState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceGatheringState.html",
    "title": "Enum IceGatheringState | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceGatheringState State of an ICE gathering process. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceGatheringState : int Remarks See RTPIceGatheringState from the WebRTC 1.0 standard. Fields Name Description Complete The gathering process is complete. At least one ICE transport was active, and all transports finished gathering ICE candidates. Gathering The gathering process started. At least one ICE transport is active and gathering some ICE candidates. New There is no ICE transport, or none of them started gathering ICE candidates. See Also https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum"
  },
  "api/Microsoft.MixedReality.WebRTC.IceServer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceServer.html",
    "title": "Class IceServer | MixedReality-WebRTC Documentation",
    "keywords": "Class IceServer ICE server configuration (STUN and/or TURN). Inheritance Object IceServer Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class IceServer : object Fields | Improve this Doc View Source TurnPassword Optional TURN server credentials. Declaration public string TurnPassword Field Value Type Description String | Improve this Doc View Source TurnUserName Optional TURN server username. Declaration public string TurnUserName Field Value Type Description String | Improve this Doc View Source Urls List of TURN and/or STUN server URLs to use for NAT bypass, in order of preference. The scheme is defined in the core WebRTC implementation, and is in short: stunURI = stunScheme \":\" stun-host [ \":\" stun-port ] stunScheme = \"stun\" / \"stuns\" turnURI = turnScheme \":\" turn-host [ \":\" turn-port ] [ \"?transport=\" transport ] turnScheme = \"turn\" / \"turns\" Declaration public List<string> Urls Field Value Type Description List < String > Methods | Improve this Doc View Source ToString() Format the ICE server data according to the encoded marshalling of the C++ API. Declaration public override string ToString() Returns Type Description String The encoded string of ICE servers."
  },
  "api/Microsoft.MixedReality.WebRTC.IceTransportType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceTransportType.html",
    "title": "Enum IceTransportType | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceTransportType Type of ICE candidates offered to the remote peer. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceTransportType : int Fields Name Description All Offer all types of ICE candidates. NoHost None No ICE candidate offered. Relay Only advertize relay-type candidates, like TURN servers, to avoid leaking the IP address of the client."
  },
  "api/Microsoft.MixedReality.WebRTC.ILogSink.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ILogSink.html",
    "title": "Interface ILogSink | MixedReality-WebRTC Documentation",
    "keywords": "Interface ILogSink Interface for a sink receiving log messages. The sink can be registered with AddSink(ILogSink, LogSeverity) to receive logging messages. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface ILogSink Methods | Improve this Doc View Source LogMessage(LogSeverity, String) Callback invoked when a log message is received. Declaration void LogMessage(LogSeverity severity, string message) Parameters Type Name Description LogSeverity severity Message severity. String message Message description. See Also AddSink(ILogSink, LogSeverity) RemoveSink(ILogSink)"
  },
  "api/Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.html",
    "title": "Class InvalidInteropNativeHandleException | MixedReality-WebRTC Documentation",
    "keywords": "Class InvalidInteropNativeHandleException Exception thrown when an API function expects an interop handle to a valid native object, but receives an invalid handle instead. Inheritance Object InvalidInteropNativeHandleException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class InvalidInteropNativeHandleException : Exception Constructors | Improve this Doc View Source InvalidInteropNativeHandleException() Declaration public InvalidInteropNativeHandleException() | Improve this Doc View Source InvalidInteropNativeHandleException(String) Declaration public InvalidInteropNativeHandleException(string message) Parameters Type Name Description String message | Improve this Doc View Source InvalidInteropNativeHandleException(String, Exception) Declaration public InvalidInteropNativeHandleException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html",
    "title": "Interface IVideoFrameQueue | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameQueue Interface for a queue of video frames. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameQueue Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration float QueuedFramesPerSecond { get; } Property Value Type Description Single"
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html",
    "title": "Interface IVideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameStorage Interface for a storage of a single video frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw storage buffer of capacity Capacity . Declaration byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Storage capacity, in bytes. Declaration ulong Capacity { get; set; } Property Value Type Description UInt64 | Improve this Doc View Source Height Frame height, in pixels. Declaration uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration uint Width { get; set; } Property Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoSource.html",
    "title": "Interface IVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoSource Interface for video sources, whether local or remote. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoSource Properties | Improve this Doc View Source Enabled Enabled status of the source. If enabled, produces video frames as expected. If disabled, produces black frames instead. Declaration bool Enabled { get; } Property Value Type Description Boolean | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding Events | Improve this Doc View Source Argb32VideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration event Argb32VideoFrameDelegate Argb32VideoFrameReady Event Type Type Description Argb32VideoFrameDelegate Remarks The event delivers to the handlers an ARGB32-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. | Improve this Doc View Source I420AVideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration event I420AVideoFrameDelegate I420AVideoFrameReady Event Type Type Description I420AVideoFrameDelegate Remarks The event delivers to the handlers an I420-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. See Also VideoTrackSource LocalVideoTrack RemoteVideoTrack"
  },
  "api/Microsoft.MixedReality.WebRTC.Library.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Library.html",
    "title": "Class Library | MixedReality-WebRTC Documentation",
    "keywords": "Class Library Container for library-wise global settings of MixedReality-WebRTC. Inheritance Object Library Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public static class Library : object Properties | Improve this Doc View Source ShutdownOptions Options used when shutting down the MixedReality-WebRTC library. disposed, to shutdown the internal threads and release the global resources, and allow the library's module to be unloaded. Declaration public static Library.ShutdownOptionsFlags ShutdownOptions { get; set; } Property Value Type Description Library.ShutdownOptionsFlags | Improve this Doc View Source UsedAudioDeviceModule Audio device module to use on Windows Desktop. Declaration public static AudioDeviceModule UsedAudioDeviceModule { get; set; } Property Value Type Description AudioDeviceModule Remarks The ADM can only be changed before any peer connection is created/initialized; otherwise an is raised. This has no effect on UWP or non-Windows platforms. Methods | Improve this Doc View Source ForceShutdown() Forcefully shutdown the MixedReality-WebRTC library. This shall not be used under normal circumstances, but can be useful e.g. in the Unity editor when a test fails and proper clean-up is not ensured (in particular, disposing objects), to allow the shared module to shutdown and terminate its native threads, and be unloaded. Declaration public static void ForceShutdown() | Improve this Doc View Source ReportLiveObjects() Report all objects currently alive and tracked by the native implementation. This is a live report, which generally gets outdated as soon as the function returned, as new objects are created and others destroyed. Nonetheless this is may be helpful to diagnose issues with disposing objects. Declaration public static uint ReportLiveObjects() Returns Type Description UInt32 Returns the number of live objects at the time of the call."
  },
  "api/Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.html",
    "title": "Enum Library.ShutdownOptionsFlags | MixedReality-WebRTC Documentation",
    "keywords": "Enum Library.ShutdownOptionsFlags Options for library shutdown. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum ShutdownOptionsFlags : uint Fields Name Description DebugBreakOnForceShutdown When forcing shutdown, either because ForceShutdown() is called or because the program terminates, and some objects are still alive, attempt to break into the debugger. This is not available for all platforms. Default Default options. LogLiveObjects Log with ReportLiveObjects() all objects still alive, to help debugging. This is recommended to prevent deadlocks during shutdown. None Do nothing specific."
  },
  "api/Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig.html",
    "title": "Class LocalAudioDeviceInitConfig | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalAudioDeviceInitConfig Configuration to initialize capture on a local audio device (microphone). Inheritance Object LocalAudioDeviceInitConfig Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalAudioDeviceInitConfig : object Fields | Improve this Doc View Source AutoGainControl Enable automated gain control (AGC) on the audio device capture pipeline. Declaration public bool? AutoGainControl Field Value Type Description Nullable < Boolean >"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalAudioTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalAudioTrack.html",
    "title": "Class LocalAudioTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalAudioTrack Audio track sending to the remote peer audio frames originating from a local track source (local microphone or other audio recording device). Inheritance Object MediaTrack LocalMediaTrack LocalAudioTrack Implements IDisposable IAudioSource Inherited Members MediaTrack.Transceiver MediaTrack.PeerConnection MediaTrack.Name Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalAudioTrack : LocalMediaTrack, IAudioSource Properties | Improve this Doc View Source Enabled Enabled status of the track. If enabled, send local audio frames to the remote peer as expected. If disabled, send only black frames instead. Declaration public bool Enabled { get; set; } Property Value Type Description Boolean Remarks Reading the value of this property after the track has been disposed is valid, and returns false . Writing to this property after the track has been disposed throws an exception. | Improve this Doc View Source Source Audio track source this track is pulling its audio frames from. Declaration public AudioTrackSource Source { get; } Property Value Type Description AudioTrackSource Methods | Improve this Doc View Source CreateFromSource(AudioTrackSource, LocalAudioTrackInitConfig) Create an audio track from an existing audio track source. This does not add the track to any peer connection. Instead, the track must be added manually to an audio transceiver to be attached to a peer connection and transmitted to a remote peer. Declaration public static LocalAudioTrack CreateFromSource(AudioTrackSource source, LocalAudioTrackInitConfig initConfig) Parameters Type Name Description AudioTrackSource source The track source which provides the raw audio frames to the newly created track. LocalAudioTrackInitConfig initConfig Configuration to initialize the track being created. Returns Type Description LocalAudioTrack Asynchronous task completed once the track is created. | Improve this Doc View Source CreateReadBuffer() Starts buffering the audio frames from in an AudioTrackReadBuffer . Declaration public AudioTrackReadBuffer CreateReadBuffer() Returns Type Description AudioTrackReadBuffer Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady instead. | Improve this Doc View Source Dispose() Remove the track from the associated Transceiver (if there is one) and release the corresponding resources. Declaration public override void Dispose() Overrides LocalMediaTrack.Dispose() | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides MediaTrack.ToString() Events | Improve this Doc View Source AudioFrameReady Event that occurs when a new audio frame is available from the source, either because the source produced it locally ( AudioTrackSource , LocalAudioTrack ) or because it received it from the remote peer ( RemoteAudioTrack ). Declaration public event AudioFrameDelegate AudioFrameReady Event Type Type Description AudioFrameDelegate Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady . If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . Implements IDisposable IAudioSource"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig.html",
    "title": "Class LocalAudioTrackInitConfig | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalAudioTrackInitConfig Settings for adding a local audio track backed by a local audio capture device (e.g. microphone). Inheritance Object LocalAudioTrackInitConfig Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalAudioTrackInitConfig : object Fields | Improve this Doc View Source trackName Name of the track to create, as used for the SDP negotiation. This name needs to comply with the requirements of an SDP token, as described in the SDP RFC https://tools.ietf.org/html/rfc4566#page-43 . In particular the name cannot contain spaces nor double quotes \" . The track name can optionally be empty, in which case the implementation will create a valid random track name. Declaration public string trackName Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalMediaTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalMediaTrack.html",
    "title": "Class LocalMediaTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalMediaTrack Base class for media tracks sending to the remote peer. Inheritance Object MediaTrack LocalMediaTrack LocalAudioTrack LocalVideoTrack Implements IDisposable Inherited Members MediaTrack.Transceiver MediaTrack.PeerConnection MediaTrack.Name MediaTrack.ToString() Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public abstract class LocalMediaTrack : MediaTrack Methods | Improve this Doc View Source Dispose() Remove the track from the associated Transceiver (if there is one) and release the corresponding resources. Declaration public abstract void Dispose() Implements IDisposable"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.html",
    "title": "Class LocalVideoDeviceInitConfig | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoDeviceInitConfig Configuration to initialize capture on a local video device (webcam). Inheritance Object LocalVideoDeviceInitConfig Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalVideoDeviceInitConfig : object Fields | Improve this Doc View Source enableMrc Enable Mixed Reality Capture (MRC) on devices supporting the feature. This setting is silently ignored on device not supporting MRC. Declaration public bool enableMrc Field Value Type Description Boolean Remarks This is only supported on UWP. | Improve this Doc View Source enableMrcRecordingIndicator Display the on-screen recording indicator while MRC is enabled. This setting is silently ignored on device not supporting MRC, or if enableMrc is set to false . Declaration public bool enableMrcRecordingIndicator Field Value Type Description Boolean Remarks This is only supported on UWP. | Improve this Doc View Source framerate Optional capture frame rate, in frames per second (FPS). This must be a capture framerate the device supports. Declaration public double? framerate Field Value Type Description Nullable < Double > Remarks This is compared by strict equality, so is best left unspecified or to an exact value retrieved by GetCaptureFormatsAsync(String) . | Improve this Doc View Source height Optional capture resolution height, in pixels. This must be a resolution width the device supports. Declaration public uint? height Field Value Type Description Nullable < UInt32 > | Improve this Doc View Source videoDevice Optional video capture device to use for capture. Use the default device if not specified. Declaration public VideoCaptureDevice videoDevice Field Value Type Description VideoCaptureDevice | Improve this Doc View Source videoProfileId Optional unique identifier of the video profile to use for capture, if the device supports video profiles, as retrieved by one of: This requires videoDevice to be specified. Declaration public string videoProfileId Field Value Type Description String | Improve this Doc View Source videoProfileKind Optional video profile kind to restrict the list of video profiles to consider. Note that this is not exclusive with videoProfileId , although in practice it is recommended to specify only one or the other. This requires videoDevice to be specified. Declaration public VideoProfileKind videoProfileKind Field Value Type Description VideoProfileKind | Improve this Doc View Source width Optional capture resolution width, in pixels. This must be a resolution width the device supports. Declaration public uint? width Field Value Type Description Nullable < UInt32 >"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalVideoTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalVideoTrack.html",
    "title": "Class LocalVideoTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoTrack Video track sending to the remote peer video frames originating from a local track source. Inheritance Object MediaTrack LocalMediaTrack LocalVideoTrack Implements IDisposable IVideoSource Inherited Members MediaTrack.Transceiver MediaTrack.PeerConnection MediaTrack.Name Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalVideoTrack : LocalMediaTrack, IVideoSource Properties | Improve this Doc View Source Enabled Enabled status of the track. If enabled, send local video frames to the remote peer as expected. If disabled, send only black frames instead. Declaration public bool Enabled { get; set; } Property Value Type Description Boolean Remarks Reading the value of this property after the track has been disposed is valid, and returns false . Writing to this property after the track has been disposed throws an exception. | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration public VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding | Improve this Doc View Source Source Video track source this track is pulling its video frames from. Declaration public VideoTrackSource Source { get; } Property Value Type Description VideoTrackSource Methods | Improve this Doc View Source CreateFromSource(VideoTrackSource, LocalVideoTrackInitConfig) Create a video track from an existing video track source. This does not add the track to any peer connection. Instead, the track must be added manually to a video transceiver to be attached to a peer connection and transmitted to a remote peer. Declaration public static LocalVideoTrack CreateFromSource(VideoTrackSource source, LocalVideoTrackInitConfig initConfig) Parameters Type Name Description VideoTrackSource source The track source which provides the raw video frames to the newly created track. LocalVideoTrackInitConfig initConfig Configuration to initialize the track being created. Returns Type Description LocalVideoTrack Asynchronous task completed once the track is created. | Improve this Doc View Source Dispose() Remove the track from the associated Transceiver (if there is one) and release the corresponding resources. Declaration public override void Dispose() Overrides LocalMediaTrack.Dispose() | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides MediaTrack.ToString() Events | Improve this Doc View Source Argb32VideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event Argb32VideoFrameDelegate Argb32VideoFrameReady Event Type Type Description Argb32VideoFrameDelegate Remarks The event delivers to the handlers an ARGB32-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. | Improve this Doc View Source I420AVideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event I420AVideoFrameDelegate I420AVideoFrameReady Event Type Type Description I420AVideoFrameDelegate Remarks The event delivers to the handlers an I420-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. Implements IDisposable IVideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig.html",
    "title": "Class LocalVideoTrackInitConfig | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoTrackInitConfig Settings for creating a new local video track. Inheritance Object LocalVideoTrackInitConfig Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalVideoTrackInitConfig : object Fields | Improve this Doc View Source trackName Name of the track to create, as used for the SDP negotiation. This name needs to comply with the requirements of an SDP token, as described in the SDP RFC https://tools.ietf.org/html/rfc4566#page-43 . In particular the name cannot contain spaces nor double quotes \" . The track name can optionally be empty, in which case the implementation will create a valid random track name. Declaration public string trackName Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.Logging.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Logging.html",
    "title": "Class Logging | MixedReality-WebRTC Documentation",
    "keywords": "Class Logging Logging utilities. Inheritance Object Logging Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public static class Logging : object Methods | Improve this Doc View Source AddSink(ILogSink, LogSeverity) Add a log sink receiving messages. Declaration public static void AddSink(ILogSink sink, LogSeverity minimumSeverity) Parameters Type Name Description ILogSink sink The sink to register. LogSeverity minimumSeverity Minimum severity of messages to forward to the sink. | Improve this Doc View Source LogMessage(LogSeverity, String) Log a message with a given severity. The message will be logged alongside the messages generated by the implementation, and received by any registered sink callback like internal messages. Declaration public static void LogMessage(LogSeverity severity, string message) Parameters Type Name Description LogSeverity severity Message severity. String message Message content. | Improve this Doc View Source RemoveSink(ILogSink) Remove a log sink receiving messages. Declaration public static void RemoveSink(ILogSink sink) Parameters Type Name Description ILogSink sink The sink to unregister."
  },
  "api/Microsoft.MixedReality.WebRTC.LogSeverity.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LogSeverity.html",
    "title": "Enum LogSeverity | MixedReality-WebRTC Documentation",
    "keywords": "Enum LogSeverity Log message severity. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum LogSeverity : int Fields Name Description Error Error message about unexpected action or result. Info Informational message for diagnosing. None Logging disabled. Unknown Unknown severity level, could not be retrieved/assigned. Verbose Diagnostic message for debugging. Warning Warning message about some event to potentially investigate."
  },
  "api/Microsoft.MixedReality.WebRTC.MediaKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.MediaKind.html",
    "title": "Enum MediaKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum MediaKind Type of media track or media transceiver. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum MediaKind : uint Remarks This is the projection of mrsMediaKind from the interop API. Fields Name Description Audio Audio data. Video Video data."
  },
  "api/Microsoft.MixedReality.WebRTC.MediaTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.MediaTrack.html",
    "title": "Class MediaTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaTrack Base class for media tracks sending to or receiving from the remote peer. Inheritance Object MediaTrack LocalMediaTrack RemoteAudioTrack RemoteVideoTrack Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public abstract class MediaTrack : object Properties | Improve this Doc View Source Name Track name as specified during creation. This property is immutable. For remote tracks the property is specified by the remote peer. Declaration public string Name { get; } Property Value Type Description String | Improve this Doc View Source PeerConnection Peer connection this media track is added to, if any. This is null after the track has been removed from the peer connection. Declaration public PeerConnection PeerConnection { get; protected set; } Property Value Type Description PeerConnection | Improve this Doc View Source Transceiver Transceiver this track is attached to, if any. Declaration public Transceiver Transceiver { get; protected set; } Property Value Type Description Transceiver Methods | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.MovingAverage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.MovingAverage.html",
    "title": "Class MovingAverage | MixedReality-WebRTC Documentation",
    "keywords": "Class MovingAverage Utility to manage a moving average of a time series. Inheritance Object MovingAverage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class MovingAverage : object Constructors | Improve this Doc View Source MovingAverage(Int32) Create a new moving average with a given window size. Declaration public MovingAverage(int capacity) Parameters Type Name Description Int32 capacity The capacity of the sample window. Properties | Improve this Doc View Source Average Average value of the samples. Declaration public float Average { get; } Property Value Type Description Single | Improve this Doc View Source Capacity Number of samples in the moving average window. Declaration public int Capacity { get; } Property Value Type Description Int32 Methods | Improve this Doc View Source Clear() Clear the moving average and discard all cached samples. Declaration public void Clear() | Improve this Doc View Source Push(Single) Push a new sample and recalculate the current average. Declaration public void Push(float value) Parameters Type Name Description Single value The new value to add."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.html",
    "title": "Struct PeerConnection.AudioReceiverStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.AudioReceiverStats Subset of RTCMediaStreamTrack (audio receiver) and RTCInboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#aststats-dict* and https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict* . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct AudioReceiverStats Fields | Improve this Doc View Source AudioLevel Linear audio level of the receiving track, in [0:1] range, averaged over a small interval. Declaration public double AudioLevel Field Value Type Description Double | Improve this Doc View Source BytesReceived Total number of bytes received for this SSRC. Declaration public ulong BytesReceived Field Value Type Description UInt64 | Improve this Doc View Source PacketsReceived Total number of RTP packets received for this SSRC. Declaration public uint PacketsReceived Field Value Type Description UInt32 | Improve this Doc View Source RtpStatsTimestampUs Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long RtpStatsTimestampUs Field Value Type Description Int64 | Improve this Doc View Source TotalAudioEnergy Total audio energy of the received track. For multi-channel sources (stereo, etc.) this is the highest energy of any of the channels for each sample. Declaration public double TotalAudioEnergy Field Value Type Description Double | Improve this Doc View Source TotalSamplesDuration Total duration in seconds of all the samples received (and thus counted by TotalSamplesReceived ). Like TotalAudioEnergy this is not affected by the number of channels per sample. Declaration public double TotalSamplesDuration Field Value Type Description Double | Improve this Doc View Source TotalSamplesReceived Total number of RTP samples received for this audio stream. Like TotalAudioEnergy this is not affected by the number of channels per sample. Declaration public double TotalSamplesReceived Field Value Type Description Double | Improve this Doc View Source TrackIdentifier Track identifier. Declaration public string TrackIdentifier Field Value Type Description String | Improve this Doc View Source TrackStatsTimestampUs Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TrackStatsTimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.html",
    "title": "Struct PeerConnection.AudioSenderStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.AudioSenderStats Subset of RTCMediaStreamTrack (audio sender) and RTCOutboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#raststats-dict* and https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict* . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct AudioSenderStats Fields | Improve this Doc View Source AudioLevel Linear audio level of the media source, in [0:1] range, averaged over a small interval. Declaration public double AudioLevel Field Value Type Description Double | Improve this Doc View Source BytesSent Total number of bytes sent for this SSRC. Declaration public ulong BytesSent Field Value Type Description UInt64 | Improve this Doc View Source PacketsSent Total number of RTP packets sent for this SSRC. Declaration public uint PacketsSent Field Value Type Description UInt32 | Improve this Doc View Source RtpStatsTimestampUs Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long RtpStatsTimestampUs Field Value Type Description Int64 | Improve this Doc View Source TotalAudioEnergy Total audio energy of the media source. For multi-channel sources (stereo, etc.) this is the highest energy of any of the channels for each sample. Declaration public double TotalAudioEnergy Field Value Type Description Double | Improve this Doc View Source TotalSamplesDuration Total duration in seconds of all the samples produced by the media source for the lifetime of the underlying internal statistics object. Like TotalAudioEnergy this is not affected by the number of channels per sample. Declaration public double TotalSamplesDuration Field Value Type Description Double | Improve this Doc View Source TrackIdentifier Track identifier. Declaration public string TrackIdentifier Field Value Type Description String | Improve this Doc View Source TrackStatsTimestampUs Unix timestamp (time since Epoch) of the audio statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TrackStatsTimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAddedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAddedDelegate.html",
    "title": "Delegate PeerConnection.AudioTrackAddedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.AudioTrackAddedDelegate Delegate for AudioTrackAdded event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void AudioTrackAddedDelegate(RemoteAudioTrack track); Parameters Type Name Description RemoteAudioTrack track The newly added audio track."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackRemovedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackRemovedDelegate.html",
    "title": "Delegate PeerConnection.AudioTrackRemovedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.AudioTrackRemovedDelegate Delegate for AudioTrackRemoved event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void AudioTrackRemovedDelegate(Transceiver transceiver, RemoteAudioTrack track); Parameters Type Name Description Transceiver transceiver The audio transceiver the track was removed from. RemoteAudioTrack track The audio track just removed."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAddedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAddedDelegate.html",
    "title": "Delegate PeerConnection.DataChannelAddedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.DataChannelAddedDelegate Delegate for DataChannelAdded event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void DataChannelAddedDelegate(DataChannel channel); Parameters Type Name Description DataChannel channel The newly added data channel."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemovedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemovedDelegate.html",
    "title": "Delegate PeerConnection.DataChannelRemovedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.DataChannelRemovedDelegate Delegate for DataChannelRemoved event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void DataChannelRemovedDelegate(DataChannel channel); Parameters Type Name Description DataChannel channel The data channel just removed."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.html",
    "title": "Struct PeerConnection.DataChannelStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.DataChannelStats Subset of RTCDataChannelStats. See https://www.w3.org/TR/webrtc-stats/#dcstats-dict* Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct DataChannelStats Fields | Improve this Doc View Source BytesReceived Total number of payload bytes received, excluding headers and paddings. Declaration public ulong BytesReceived Field Value Type Description UInt64 | Improve this Doc View Source BytesSent Total number of payload bytes sent, excluding headers and paddings. Declaration public ulong BytesSent Field Value Type Description UInt64 | Improve this Doc View Source DataChannelIdentifier ID of the data channel associated with these statistics. Declaration public long DataChannelIdentifier Field Value Type Description Int64 | Improve this Doc View Source MessagesReceived Total number of API message events received. Declaration public uint MessagesReceived Field Value Type Description UInt32 | Improve this Doc View Source MessagesSent Total number of API message event sent. Declaration public uint MessagesSent Field Value Type Description UInt32 | Improve this Doc View Source TimestampUs Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.html",
    "title": "Enum PeerConnection.FrameHeightRoundMode | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.FrameHeightRoundMode Frame height round mode. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum FrameHeightRoundMode : int Fields Name Description Crop Crop frame height to the nearest multiple of 16. ((height - nearestLowerMultipleOf16) / 2) rows are cropped from the top and (height - nearestLowerMultipleOf16 - croppedRowsTop) rows are cropped from the bottom. None Leave frames unchanged. Pad Pad frame height to the nearest multiple of 16. ((nearestHigherMultipleOf16 - height) / 2) rows are added symmetrically at the top and (nearestHigherMultipleOf16 - height - addedRowsTop) rows are added symmetrically at the bottom. See Also SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode)"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264Config.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264Config.html",
    "title": "Struct PeerConnection.H264Config | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.H264Config Configuration for the Media Foundation H.264 encoder. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct H264Config Fields | Improve this Doc View Source MaxQp If set to a value between 0 and 51, determines the max QP to use for encoding. Declaration public int? MaxQp Field Value Type Description Nullable < Int32 > | Improve this Doc View Source Profile H.264 profile. Note : by default we should use what's passed by WebRTC on codec initialization (which seems to be always ConstrainedBaseline), but we use Baseline to avoid changing behavior compared to earlier versions. Declaration public PeerConnection.H264Profile Profile Field Value Type Description PeerConnection.H264Profile | Improve this Doc View Source Quality If set to a value between 0 and 100, determines the target quality value. The effect of this depends on the encoder and on the rate control mode chosen. In the Quality RC mode this will be the target for the whole stream, while in VBR it might be used as a target for individual frames while the average quality of the stream is determined by the target bitrate. Declaration public int? Quality Field Value Type Description Nullable < Int32 > | Improve this Doc View Source RcMode Rate control mode. Declaration public PeerConnection.H264RcMode? RcMode Field Value Type Description Nullable < PeerConnection.H264RcMode >"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264Profile.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264Profile.html",
    "title": "Enum PeerConnection.H264Profile | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.H264Profile H.264 Encoding profile. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum H264Profile : int Fields Name Description Baseline Baseline profile. ConstrainedBaseline Constrained Baseline profile. ConstrainedHigh Constrained High profile. High High profile. Main Main profile."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264RcMode.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.H264RcMode.html",
    "title": "Enum PeerConnection.H264RcMode | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.H264RcMode Rate control mode for the Media Foundation H.264. See https://docs.microsoft.com/en-us/windows/win32/medfound/h-264-video-encoder for details. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum H264RcMode : int Fields Name Description CBR Constant Bit Rate. Quality Constant quality. VBR Variable Bit Rate."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. Inheritance Object PeerConnection Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class PeerConnection : IDisposable Constructors | Improve this Doc View Source PeerConnection() Create a new peer connection object. The object is initially created empty, and cannot be used until InitializeAsync(PeerConnectionConfiguration, CancellationToken) has completed successfully. Declaration public PeerConnection() Fields | Improve this Doc View Source PreferredAudioCodec Name of the preferred audio codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredAudioCodec Field Value Type Description String | Improve this Doc View Source PreferredAudioCodecExtraParamsLocal Advanced use only. List of additional codec-specific arguments set on the local endpoint. Declaration public string PreferredAudioCodecExtraParamsLocal Field Value Type Description String Remarks This must be a semicolon-separated list of \"key=value\" pairs. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. Arguments are set locally by adding them to the audio codec section of SDP messages received from the remote endpoint. This is ignored if PreferredAudioCodec is an empty string, or is not a valid codec name found in the SDP message offer. | Improve this Doc View Source PreferredAudioCodecExtraParamsRemote Advanced use only. List of additional codec-specific arguments requested to the remote endpoint. Declaration public string PreferredAudioCodecExtraParamsRemote Field Value Type Description String Remarks This must be a semicolon-separated list of \"key=value\" pairs. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. Arguments are added to the audio codec section of SDP messages sent to the remote endpoint. This is ignored if PreferredAudioCodec is an empty string, or is not a valid codec name found in the SDP message offer. | Improve this Doc View Source PreferredVideoCodec Name of the preferred video codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredVideoCodec Field Value Type Description String | Improve this Doc View Source PreferredVideoCodecExtraParamsLocal Advanced use only. List of additional codec-specific arguments set on the local endpoint. Declaration public string PreferredVideoCodecExtraParamsLocal Field Value Type Description String Remarks This must be a semicolon-separated list of \"key=value\" pairs. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. Arguments are set locally by adding them to the video codec section of SDP messages received from the remote endpoint. This is ignored if PreferredVideoCodec is an empty string, or is not a valid codec name found in the SDP message offer. | Improve this Doc View Source PreferredVideoCodecExtraParamsRemote Advanced use only. List of additional codec-specific arguments requested to the remote endpoint. Declaration public string PreferredVideoCodecExtraParamsRemote Field Value Type Description String Remarks This must be a semicolon-separated list of \"key=value\" pairs. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. Arguments are added to the video codec section of SDP messages sent to the remote endpoint. This is ignored if PreferredVideoCodec is an empty string, or is not a valid codec name found in the SDP message offer. Properties | Improve this Doc View Source AssociatedTransceivers Collection of transceivers which have already been associated with a media line. A transceiver is associated with a media line when a local or remote offer is applied to the peer connection, respectively during CreateOffer() and SetRemoteDescriptionAsync(SdpMessage) . Declaration public IEnumerable<Transceiver> AssociatedTransceivers { get; } Property Value Type Description IEnumerable < Transceiver > | Improve this Doc View Source DataChannels Collection of data channels for the peer connection. Data channels are either manually added by calling AddDataChannelAsync(String, Boolean, Boolean, CancellationToken) or AddDataChannelAsync(UInt16, String, Boolean, Boolean, CancellationToken) , or are created by the implementation while applying a remote offer when the remote peer created a new in-band data channel. Declaration public List<DataChannel> DataChannels { get; } Property Value Type Description List < DataChannel > | Improve this Doc View Source Initialized Boolean property indicating whether the peer connection has been initialized. Warning This property will be deprecated and later removed in future versions. The value of this property is undefined while the asynchronous task resulting from a call to InitializeAsync(PeerConnectionConfiguration, CancellationToken) is pending. This means its value is only relevant before the call (and then it is false ) or after the asynchronous call completed (and then it is true ), but not while the initialization is underway. For this reason, it is recommended NOT to use this property, and instead to rely on logic around InitializeAsync(PeerConnectionConfiguration, CancellationToken) and Close() alone. Generally this means awaiting the initialize call ( await operator) before using the peer connection object for anything else. Declaration public bool Initialized { get; } Property Value Type Description Boolean | Improve this Doc View Source IsConnected Indicates whether the peer connection is established and can exchange some track content (audio/video/data) with the remote peer. Declaration public bool IsConnected { get; } Property Value Type Description Boolean Remarks This does not indicate whether the ICE exchange is done, as it may continue after the peer connection negotiated a first session. For ICE connection status, see the IceStateChanged event. | Improve this Doc View Source LocalAudioTracks Collection of local audio tracks attached to the peer connection. Declaration public IEnumerable<LocalAudioTrack> LocalAudioTracks { get; } Property Value Type Description IEnumerable < LocalAudioTrack > | Improve this Doc View Source LocalVideoTracks Collection of local video tracks attached to the peer connection. Declaration public IEnumerable<LocalVideoTrack> LocalVideoTracks { get; } Property Value Type Description IEnumerable < LocalVideoTrack > | Improve this Doc View Source Name A name for the peer connection, used for logging and debugging. Declaration public string Name { get; set; } Property Value Type Description String | Improve this Doc View Source RemoteAudioTracks Collection of remote audio tracks attached to the peer connection. Declaration public IEnumerable<RemoteAudioTrack> RemoteAudioTracks { get; } Property Value Type Description IEnumerable < RemoteAudioTrack > | Improve this Doc View Source RemoteVideoTracks Collection of remote video tracks attached to the peer connection. Declaration public IEnumerable<RemoteVideoTrack> RemoteVideoTracks { get; } Property Value Type Description IEnumerable < RemoteVideoTrack > | Improve this Doc View Source Transceivers Collection of transceivers for the peer connection. Once a transceiver is added to the peer connection, it cannot be removed, but its tracks can be changed. Adding a transceiver or changing its direction require some new session negotiation. Declaration public List<Transceiver> Transceivers { get; } Property Value Type Description List < Transceiver > Methods | Improve this Doc View Source AddDataChannelAsync(String, Boolean, Boolean, CancellationToken) Add a new in-band data channel whose ID will be determined by the implementation. A data channel is branded in-band when one peer requests its creation to the WebRTC core, and the implementation negotiates with the remote peer an appropriate ID by sending some SDP offer message. In that case once accepted the other peer will automatically create the appropriate data channel on its side with that same ID, and the ID will be returned on both sides to the user for information. Compared to out-of-band messages, this requires exchanging some SDP messages, but avoids having to agree on a common unused ID and having to explicitly open the data channel on both sides. Declaration public Task<DataChannel> AddDataChannelAsync(string label, bool ordered, bool reliable, CancellationToken cancellationToken = null) Parameters Type Name Description String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). CancellationToken cancellationToken Cancellation token for the task returned. Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. Remarks See the critical remark about SCTP handshake in AddDataChannelAsync(UInt16, String, Boolean, Boolean, CancellationToken) . Exceptions Type Condition SctpNotNegotiatedException SCTP not negotiated. Call CreateOffer() first. | Improve this Doc View Source AddDataChannelAsync(UInt16, String, Boolean, Boolean, CancellationToken) Add a new out-of-band data channel with the given ID. A data channel is branded out-of-band when the peers agree on an identifier by any mean not known to WebRTC, and both open a data channel with that ID. The WebRTC will match the incoming and outgoing pipes by this ID to allow sending and receiving through that channel. This requires some external mechanism to agree on an available identifier not otherwise taken by another channel, and also requires to ensure that both peers explicitly open that channel. The advantage of in-band data channels is that no SDP session renegotiation is needed, except for the very first data channel added (in-band or out-of-band) which requires a negotiation for the SCTP handshake (see remarks). Declaration public Task<DataChannel> AddDataChannelAsync(ushort id, string label, bool ordered, bool reliable, CancellationToken cancellationToken = null) Parameters Type Name Description UInt16 id The unique data channel identifier to use. String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). CancellationToken cancellationToken Cancellation token for the task returned. Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. Remarks Data channels use DTLS over SCTP, which ensure in particular that messages are encrypted. To that end, while establishing a connection with the remote peer, some specific SCTP handshake must occur. This handshake is only performed if at least one data channel was added to the peer connection when the connection starts its negotiation with CreateOffer() . Therefore, if the user wants to use a data channel at any point during the lifetime of this peer connection, it is critical to add at least one data channel before CreateOffer() is called. Otherwise all calls will fail with an SctpNotNegotiatedException exception. Exceptions Type Condition SctpNotNegotiatedException SCTP not negotiated. Call CreateOffer() first. | Improve this Doc View Source AddIceCandidate(IceCandidate) Inform the WebRTC peer connection of a newly received ICE candidate. Declaration public void AddIceCandidate(IceCandidate candidate) Parameters Type Name Description IceCandidate candidate The ICE candidate received from the remote peer. | Improve this Doc View Source AddTransceiver(MediaKind, TransceiverInitSettings) Add to the current connection a new media transceiver. A transceiver is a container for a pair of media tracks, one local sending to the remote peer, and one remote receiving from the remote peer. Both are optional, and the transceiver can be in receive-only mode (no local track), in send-only mode (no remote track), or inactive (neither local nor remote track). Once a transceiver is added to the peer connection, it cannot be removed, but its tracks can be changed (this requires some renegotiation). Declaration public Transceiver AddTransceiver(MediaKind mediaKind, TransceiverInitSettings settings = null) Parameters Type Name Description MediaKind mediaKind Kind of media the transeiver is transporting. TransceiverInitSettings settings Settings to initialize the new transceiver. Returns Type Description Transceiver The newly created transceiver. | Improve this Doc View Source Close() Close the peer connection and destroy the underlying native resources. Declaration public void Close() Remarks This is equivalent to Dispose() . See Also Dispose() | Improve this Doc View Source CreateAnswer() Create an SDP answer message to a previously-received offer, to accept a connection. Once the message is ready to be sent, the LocalSdpReadytoSend event is fired to allow the user to send that message to the remote peer via its selected signaling solution. Note that this cannot be called before SetRemoteDescriptionAsync(SdpMessage) successfully completed and applied the remote offer. Declaration public bool CreateAnswer() Returns Type Description Boolean true if the answer creation task was successfully submitted. Remarks The SDP answer message is not successfully created until the LocalSdpReadytoSend event is triggered, and may still fail even if this method returns true , for example if the peer connection is not in a valid state to create an answer. | Improve this Doc View Source CreateOffer() Create an SDP offer message as an attempt to establish a connection. Once the message is ready to be sent, the LocalSdpReadytoSend event is fired to allow the user to send that message to the remote peer via its selected signaling solution. Declaration public bool CreateOffer() Returns Type Description Boolean true if the offer creation task was successfully submitted. Remarks The SDP offer message is not successfully created until the LocalSdpReadytoSend event is triggered, and may still fail even if this method returns true , for example if the peer connection is not in a valid state to create an offer. | Improve this Doc View Source Dispose() Dispose of native resources by closing the peer connection. Declaration public void Dispose() Remarks This is equivalent to Close() . See Also Close() | Improve this Doc View Source GetSimpleStatsAsync() Get a snapshot of the statistics relative to the peer connection. Declaration public Task<PeerConnection.StatsReport> GetSimpleStatsAsync() Returns Type Description Task < PeerConnection.StatsReport > | Improve this Doc View Source InitializeAsync(PeerConnectionConfiguration, CancellationToken) Initialize the current peer connection object asynchronously. Most other methods will fail unless this call completes successfully, as it initializes the underlying native implementation object required to create and manipulate the peer connection. Once this call asynchronously completed, the Initialized property is true . Declaration public Task InitializeAsync(PeerConnectionConfiguration config = null, CancellationToken token = null) Parameters Type Name Description PeerConnectionConfiguration config Configuration for initializing the peer connection. CancellationToken token Optional cancellation token for the initialize task. This is only used if the singleton task was created by this call, and not a prior call. Returns Type Description Task The singleton task used to initialize the underlying native peer connection. Remarks This method is multi-thread safe, and will always return the same task object from the first call to it until the peer connection object is deinitialized. This allows multiple callers to all execute some action following the initialization, without the need to force a single caller and to synchronize with it. | Improve this Doc View Source RemoveDataChannel(DataChannel) Remove an existing data channel from the peer connection and destroy its native implementation. Declaration public void RemoveDataChannel(DataChannel dataChannel) Parameters Type Name Description DataChannel dataChannel The data channel to remove and destroy. | Improve this Doc View Source SetBitrate(Nullable<UInt32>, Nullable<UInt32>, Nullable<UInt32>) Set the bitrate allocated to all RTP streams sent by this connection. Other limitations might affect these limits and are respected (for example \"b=AS\" in SDP). Declaration public void SetBitrate(uint? minBitrateBps = null, uint? startBitrateBps = null, uint? maxBitrateBps = null) Parameters Type Name Description Nullable < UInt32 > minBitrateBps Minimum bitrate in bits per second. Nullable < UInt32 > startBitrateBps Start/current target bitrate in bits per second. Nullable < UInt32 > maxBitrateBps Maximum bitrate in bits per second. | Improve this Doc View Source SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode) [HoloLens 1 only] Use this function to select whether resolutions where height is not multiple of 16 pixels should be cropped, padded, or left unchanged. Default is Crop to avoid severe artifacts produced by the H.264 hardware encoder on HoloLens 1 due to a bug with the encoder. This is the recommended value, and should be used unless cropping discards valuable data in the top and bottom rows for a given usage, in which case Pad can be used as a replacement but may still produce some mild artifacts. This has no effect on other platforms. Declaration public static void SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode value) Parameters Type Name Description PeerConnection.FrameHeightRoundMode value The rounding mode for video frames. | Improve this Doc View Source SetH264Config(PeerConnection.H264Config) Set the configuration used by the H.264 encoder. The passed value will apply to all tracks that start streaming, from any PeerConnection created by the application, after the call to this function. Declaration public static void SetH264Config(PeerConnection.H264Config config) Parameters Type Name Description PeerConnection.H264Config config | Improve this Doc View Source SetRemoteDescriptionAsync(SdpMessage) Pass the given SDP description received from the remote peer via signaling to the underlying WebRTC implementation, which will parse and use it. This must be called by the signaler when receiving a message. Once this operation has completed, it is safe to call CreateAnswer() . Declaration public Task SetRemoteDescriptionAsync(SdpMessage message) Parameters Type Name Description SdpMessage message The SDP message Returns Type Description Task Returns a task which completes once the remote description has been applied and transceivers have been updated. | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Events | Improve this Doc View Source AudioTrackAdded Event that occurs when a remote audio track is added to the current connection. Declaration public event PeerConnection.AudioTrackAddedDelegate AudioTrackAdded Event Type Type Description PeerConnection.AudioTrackAddedDelegate | Improve this Doc View Source AudioTrackRemoved Event that occurs when a remote audio track is removed from the current connection. Declaration public event PeerConnection.AudioTrackRemovedDelegate AudioTrackRemoved Event Type Type Description PeerConnection.AudioTrackRemovedDelegate | Improve this Doc View Source Connected Event fired when a connection is established. Declaration public event Action Connected Event Type Type Description Action | Improve this Doc View Source DataChannelAdded Event fired when a data channel is added to the peer connection. This event is always fired, whether the data channel is created by the local peer or the remote peer, and is negotiated (out-of-band) or not (in-band). If an in-band data channel is created by the local peer, the ID field is not yet available when this event is fired, because the ID has not been agreed upon with the remote peer yet. Declaration public event PeerConnection.DataChannelAddedDelegate DataChannelAdded Event Type Type Description PeerConnection.DataChannelAddedDelegate | Improve this Doc View Source DataChannelRemoved Event fired when a data channel is removed from the peer connection. This event is always fired, whatever its creation method (negotiated or not) and original creator (local or remote peer). Declaration public event PeerConnection.DataChannelRemovedDelegate DataChannelRemoved Event Type Type Description PeerConnection.DataChannelRemovedDelegate | Improve this Doc View Source IceCandidateReadytoSend Event that occurs when a local ICE candidate is ready to be transmitted. Declaration public event PeerConnection.IceCandidateReadytoSendDelegate IceCandidateReadytoSend Event Type Type Description PeerConnection.IceCandidateReadytoSendDelegate | Improve this Doc View Source IceGatheringStateChanged Event that occurs when the state of the ICE gathering changed. Declaration public event PeerConnection.IceGatheringStateChangedDelegate IceGatheringStateChanged Event Type Type Description PeerConnection.IceGatheringStateChangedDelegate | Improve this Doc View Source IceStateChanged Event that occurs when the state of the ICE connection changed. Declaration public event PeerConnection.IceStateChangedDelegate IceStateChanged Event Type Type Description PeerConnection.IceStateChangedDelegate | Improve this Doc View Source LocalSdpReadytoSend Event that occurs when a local SDP message is ready to be transmitted. Declaration public event PeerConnection.LocalSdpReadyToSendDelegate LocalSdpReadytoSend Event Type Type Description PeerConnection.LocalSdpReadyToSendDelegate | Improve this Doc View Source RenegotiationNeeded Event that occurs when a renegotiation of the session is needed. This generally occurs as a result of adding or removing tracks, and the user should call CreateOffer() to actually start a renegotiation. Declaration public event Action RenegotiationNeeded Event Type Type Description Action | Improve this Doc View Source TransceiverAdded Event that occurs when a transceiver is added to the peer connection, either manually using AddTransceiver(MediaKind, TransceiverInitSettings) , or automatically as a result of a new session negotiation. Declaration public event PeerConnection.TransceiverAddedDelegate TransceiverAdded Event Type Type Description PeerConnection.TransceiverAddedDelegate Remarks Transceivers cannot be removed from the peer connection, so there is no TransceiverRemoved event. | Improve this Doc View Source VideoTrackAdded Event that occurs when a remote video track is added to the current connection. Declaration public event PeerConnection.VideoTrackAddedDelegate VideoTrackAdded Event Type Type Description PeerConnection.VideoTrackAddedDelegate | Improve this Doc View Source VideoTrackRemoved Event that occurs when a remote video track is removed from the current connection. Declaration public event PeerConnection.VideoTrackRemovedDelegate VideoTrackRemoved Event Type Type Description PeerConnection.VideoTrackRemovedDelegate"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html",
    "title": "Delegate PeerConnection.IceCandidateReadytoSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceCandidateReadytoSendDelegate Delegate for the IceCandidateReadytoSend event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceCandidateReadytoSendDelegate(IceCandidate candidate); Parameters Type Name Description IceCandidate candidate The ICE candidate to send."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChangedDelegate.html",
    "title": "Delegate PeerConnection.IceGatheringStateChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceGatheringStateChangedDelegate Delegate for the IceGatheringStateChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceGatheringStateChangedDelegate(IceGatheringState newState); Parameters Type Name Description IceGatheringState newState The new ICE gathering state."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChangedDelegate.html",
    "title": "Delegate PeerConnection.IceStateChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceStateChangedDelegate Delegate for the IceStateChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceStateChangedDelegate(IceConnectionState newState); Parameters Type Name Description IceConnectionState newState The new ICE connection state."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html",
    "title": "Delegate PeerConnection.LocalSdpReadyToSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.LocalSdpReadyToSendDelegate Delegate for LocalSdpReadytoSend event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void LocalSdpReadyToSendDelegate(SdpMessage message); Parameters Type Name Description SdpMessage message SDP message to send."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport.html",
    "title": "Class PeerConnection.StatsReport | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection.StatsReport Snapshot of the statistics relative to a peer connection/track. The various stats objects can be read through GetStats<T>() . Inheritance Object PeerConnection.StatsReport Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class StatsReport : IDisposable Methods | Improve this Doc View Source Dispose() Dispose of the report. Declaration public void Dispose() | Improve this Doc View Source GetStats<T>() Get all the instances of a specific stats type in the report. Declaration public IEnumerable<T> GetStats<T>() Returns Type Description IEnumerable <T> Type Parameters Name Description T Must be one of PeerConnection.DataChannelStats , PeerConnection.AudioSenderStats , PeerConnection.AudioReceiverStats , PeerConnection.VideoSenderStats , PeerConnection.VideoReceiverStats , PeerConnection.TransportStats ."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.html",
    "title": "Enum PeerConnection.TrackKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.TrackKind Kind of WebRTC track. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum TrackKind : uint Fields Name Description Audio Audio track. Data Data track. Unknown Unknown track kind. Generally not initialized or error. Video Video track."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAddedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAddedDelegate.html",
    "title": "Delegate PeerConnection.TransceiverAddedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.TransceiverAddedDelegate Delegate for TransceiverAdded event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void TransceiverAddedDelegate(Transceiver transceiver); Parameters Type Name Description Transceiver transceiver The newly added transceiver."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats.html",
    "title": "Struct PeerConnection.TransportStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.TransportStats Subset of RTCTransportStats. See https://www.w3.org/TR/webrtc-stats/#transportstats-dict* . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct TransportStats Fields | Improve this Doc View Source BytesReceived Total number of payload bytes received on this PeerConnection , excluding headers and paddings. Declaration public ulong BytesReceived Field Value Type Description UInt64 | Improve this Doc View Source BytesSent Total number of payload bytes sent on this PeerConnection , excluding headers and paddings. Declaration public ulong BytesSent Field Value Type Description UInt64 | Improve this Doc View Source TimestampUs Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.html",
    "title": "Struct PeerConnection.VideoReceiverStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.VideoReceiverStats Subset of RTCMediaStreamTrack (video receiver) + RTCInboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#rvststats-dict* and https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict* Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoReceiverStats Fields | Improve this Doc View Source BytesReceived Total number of bytes received for this SSRC. Declaration public ulong BytesReceived Field Value Type Description UInt64 | Improve this Doc View Source FramesDecoded Total number of frames correctly decoded for this RTP stream, that would be displayed if no frames are dropped. Declaration public uint FramesDecoded Field Value Type Description UInt32 | Improve this Doc View Source FramesDropped Total number since the receiver was created of frames dropped prior to decode or dropped because the frame missed its display deadline for this receiver's track. Declaration public uint FramesDropped Field Value Type Description UInt32 | Improve this Doc View Source FramesReceived Total number of complete frames received on this RTP stream. Declaration public uint FramesReceived Field Value Type Description UInt32 | Improve this Doc View Source PacketsReceived Total number of RTP packets received for this SSRC. Declaration public uint PacketsReceived Field Value Type Description UInt32 | Improve this Doc View Source RtpStatsTimestampUs Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long RtpStatsTimestampUs Field Value Type Description Int64 | Improve this Doc View Source TrackIdentifier Track identifier. Declaration public string TrackIdentifier Field Value Type Description String | Improve this Doc View Source TrackStatsTimestampUs Unix timestamp (time since Epoch) of the track statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TrackStatsTimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.html",
    "title": "Struct PeerConnection.VideoSenderStats | MixedReality-WebRTC Documentation",
    "keywords": "Struct PeerConnection.VideoSenderStats Subset of RTCMediaStreamTrack (video sender) and RTCOutboundRTPStreamStats. See https://www.w3.org/TR/webrtc-stats/#vsstats-dict* and https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict* . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoSenderStats Fields | Improve this Doc View Source BytesSent Total number of bytes sent for this SSRC. Declaration public ulong BytesSent Field Value Type Description UInt64 | Improve this Doc View Source FramesEncoded Total number of frames successfully encoded for this RTP media stream. Declaration public uint FramesEncoded Field Value Type Description UInt32 | Improve this Doc View Source FramesSent Total number of frames sent on this RTP stream. Declaration public uint FramesSent Field Value Type Description UInt32 | Improve this Doc View Source HugeFramesSent Total number of huge frames sent by this RTP stream. Huge frames are frames that have an encoded size at least 2.5 times the average size of the frames. Declaration public uint HugeFramesSent Field Value Type Description UInt32 | Improve this Doc View Source PacketsSent Total number of RTP packets sent for this SSRC. Declaration public uint PacketsSent Field Value Type Description UInt32 | Improve this Doc View Source RtpStatsTimestampUs Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long RtpStatsTimestampUs Field Value Type Description Int64 | Improve this Doc View Source TrackIdentifier Track identifier. Declaration public string TrackIdentifier Field Value Type Description String | Improve this Doc View Source TrackStatsTimestampUs Unix timestamp (time since Epoch) of the track statistics. For remote statistics, this is the time at which the information reached the local endpoint. Declaration public long TrackStatsTimestampUs Field Value Type Description Int64"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackAddedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackAddedDelegate.html",
    "title": "Delegate PeerConnection.VideoTrackAddedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.VideoTrackAddedDelegate Delegate for VideoTrackAdded event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void VideoTrackAddedDelegate(RemoteVideoTrack track); Parameters Type Name Description RemoteVideoTrack track The newly added video track."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackRemovedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackRemovedDelegate.html",
    "title": "Delegate PeerConnection.VideoTrackRemovedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.VideoTrackRemovedDelegate Delegate for VideoTrackRemoved event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void VideoTrackRemovedDelegate(Transceiver transceiver, RemoteVideoTrack track); Parameters Type Name Description Transceiver transceiver The video transceiver the track was removed from. RemoteVideoTrack track The video track just removed."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.html",
    "title": "Class PeerConnectionConfiguration | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnectionConfiguration Configuration to initialize a PeerConnection . Inheritance Object PeerConnectionConfiguration Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class PeerConnectionConfiguration : object Fields | Improve this Doc View Source BundlePolicy Bundle policy for the connection. Declaration public BundlePolicy BundlePolicy Field Value Type Description BundlePolicy | Improve this Doc View Source IceServers List of TURN and/or STUN servers to use for NAT bypass, in order of preference. Declaration public List<IceServer> IceServers Field Value Type Description List < IceServer > | Improve this Doc View Source IceTransportType ICE transport policy for the connection. Declaration public IceTransportType IceTransportType Field Value Type Description IceTransportType | Improve this Doc View Source SdpSemantic SDP semantic for the connection. Declaration public SdpSemantic SdpSemantic Field Value Type Description SdpSemantic Remarks Plan B is deprecated, do not use it."
  },
  "api/Microsoft.MixedReality.WebRTC.RemoteAudioTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.RemoteAudioTrack.html",
    "title": "Class RemoteAudioTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteAudioTrack Audio track receiving audio frames from the remote peer. Inheritance Object MediaTrack RemoteAudioTrack Implements IAudioSource Inherited Members MediaTrack.Transceiver MediaTrack.PeerConnection MediaTrack.Name Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class RemoteAudioTrack : MediaTrack, IAudioSource Remarks Instances of this class are created by PeerConnection when a negotiation adds tracks sent by the remote peer. New tracks are automatically played on the system audio device after AudioTrackAdded is fired on track creation. To avoid the track being played, call OutputToDevice(Boolean) in a AudioTrackAdded handler (or later). Properties | Improve this Doc View Source Enabled Enabled status of the track. If enabled, receives audio frames from the remote peer as expected. If disabled, does not receive anything (silence). Declaration public bool Enabled { get; } Property Value Type Description Boolean Remarks Reading the value of this property after the track has been disposed is valid, and returns false . The remote audio track enabled status is controlled by the remote peer only. Methods | Improve this Doc View Source CreateReadBuffer() Starts buffering the audio frames from in an AudioTrackReadBuffer . Declaration public AudioTrackReadBuffer CreateReadBuffer() Returns Type Description AudioTrackReadBuffer Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady instead. | Improve this Doc View Source IsOutputToDevice() Returns whether the track is output directly to the system audio device. Declaration public bool IsOutputToDevice() Returns Type Description Boolean | Improve this Doc View Source OutputToDevice(Boolean) Output the audio track to the WebRTC audio device. Declaration public void OutputToDevice(bool output) Parameters Type Name Description Boolean output Remarks The default behavior is for every remote audio frame to be passed to remote audio frame callbacks, as well as output automatically to the audio device used by WebRTC. If |false| is passed to this function, remote audio frames will still be received and passed to callbacks, but won't be output to the audio device. NOTE: Changing the default behavior is not supported on UWP. | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides MediaTrack.ToString() Events | Improve this Doc View Source AudioFrameReady Event that occurs when a new audio frame is available from the source, either because the source produced it locally ( AudioTrackSource , LocalAudioTrack ) or because it received it from the remote peer ( RemoteAudioTrack ). Declaration public event AudioFrameDelegate AudioFrameReady Event Type Type Description AudioFrameDelegate Remarks WebRTC audio tracks produce an audio frame every 10 ms. If you want to process the audio frames as soon as they are received, without conversions, subscribe to AudioFrameReady . If you want the audio frames to be buffered (and optionally resampled) automatically, and you want the application to control when new audio data is read, create an AudioTrackReadBuffer using CreateReadBuffer() . Implements IAudioSource"
  },
  "api/Microsoft.MixedReality.WebRTC.RemoteVideoTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.RemoteVideoTrack.html",
    "title": "Class RemoteVideoTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteVideoTrack Video track receiving video frames from the remote peer. Inheritance Object MediaTrack RemoteVideoTrack Implements IVideoSource Inherited Members MediaTrack.Transceiver MediaTrack.PeerConnection MediaTrack.Name Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class RemoteVideoTrack : MediaTrack, IVideoSource Properties | Improve this Doc View Source Enabled Enabled status of the track. If enabled, receives video frames from the remote peer as expected. If disabled, receives only black frames instead. Declaration public bool Enabled { get; } Property Value Type Description Boolean Remarks Reading the value of this property after the track has been disposed is valid, and returns false . The remote video track enabled status is controlled by the remote peer only. | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration public VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding | Improve this Doc View Source NativeHandle Retrieves an unsafe handle to video track. Currently for internal use by the Unity NativeVideoRenderer component. Declaration public IntPtr NativeHandle { get; } Property Value Type Description IntPtr Methods | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Overrides MediaTrack.ToString() Events | Improve this Doc View Source Argb32VideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event Argb32VideoFrameDelegate Argb32VideoFrameReady Event Type Type Description Argb32VideoFrameDelegate Remarks The event delivers to the handlers an ARGB32-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. | Improve this Doc View Source I420AVideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event I420AVideoFrameDelegate I420AVideoFrameReady Event Type Type Description I420AVideoFrameDelegate Remarks The event delivers to the handlers an I420-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. Implements IVideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.html",
    "title": "Class SctpNotNegotiatedException | MixedReality-WebRTC Documentation",
    "keywords": "Class SctpNotNegotiatedException Exception thrown when trying to add a data channel to a peer connection after a connection to a remote peer was established without an SCTP handshake. When using data channels, at least one data channel must be added to the peer connection before calling CreateOffer() to signal to the implementation the intent to use data channels and the need to perform a SCTP handshake during the connection. Inheritance Object SctpNotNegotiatedException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class SctpNotNegotiatedException : Exception Constructors | Improve this Doc View Source SctpNotNegotiatedException() Declaration public SctpNotNegotiatedException() | Improve this Doc View Source SctpNotNegotiatedException(String) Declaration public SctpNotNegotiatedException(string message) Parameters Type Name Description String message | Improve this Doc View Source SctpNotNegotiatedException(String, Exception) Declaration public SctpNotNegotiatedException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.SdpMessage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SdpMessage.html",
    "title": "Class SdpMessage | MixedReality-WebRTC Documentation",
    "keywords": "Class SdpMessage SDP message passed between the local and remote peers via the user's signaling solution. Inheritance Object SdpMessage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class SdpMessage : object Fields | Improve this Doc View Source Content The raw message content. Declaration public string Content Field Value Type Description String | Improve this Doc View Source Type The message type. Declaration public SdpMessageType Type Field Value Type Description SdpMessageType Methods | Improve this Doc View Source StringToType(String) Convert an internal string representation of an SDP message type back to its enumerated value. Declaration public static SdpMessageType StringToType(string type) Parameters Type Name Description String type The internal string representation of the SDP message Returns Type Description SdpMessageType The SDP message type associated with the string representation | Improve this Doc View Source TypeToString(SdpMessageType) Convert an SDP message type to its internal string representation. Declaration public static string TypeToString(SdpMessageType type) Parameters Type Name Description SdpMessageType type The SDP message type to convert Returns Type Description String The string representation of the SDP message type"
  },
  "api/Microsoft.MixedReality.WebRTC.SdpMessageType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SdpMessageType.html",
    "title": "Enum SdpMessageType | MixedReality-WebRTC Documentation",
    "keywords": "Enum SdpMessageType Type of SDP message. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum SdpMessageType : int Fields Name Description Answer Answer message used to accept a session offer. Offer Offer message used to initiate a new session."
  },
  "api/Microsoft.MixedReality.WebRTC.SdpSemantic.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SdpSemantic.html",
    "title": "Enum SdpSemantic | MixedReality-WebRTC Documentation",
    "keywords": "Enum SdpSemantic SDP semantic used for (re)negotiating a peer connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum SdpSemantic : int Fields Name Description PlanB Legacy Plan B, deprecated and soon removed. Only available for compatiblity with older implementations if needed. Do not use unless there is a problem with the Unified Plan. UnifiedPlan Unified plan, as standardized in the WebRTC 1.0 standard."
  },
  "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html",
    "title": "Class TaskExtensions | MixedReality-WebRTC Documentation",
    "keywords": "Class TaskExtensions Collection of extension methods for Task . Inheritance Object TaskExtensions Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public static class TaskExtensions : object Methods | Improve this Doc View Source AsTask(CancellationToken) A simple helper to enable \"awaiting\" a by creating a task wrapping it. Declaration public static Task AsTask(this CancellationToken cancellationToken) Parameters Type Name Description CancellationToken cancellationToken The to await. Returns Type Description Task The task that can be awaited. | Improve this Doc View Source IgnoreCancellation(Task) Prevents or from trickling up. Declaration public static Task IgnoreCancellation(this Task task) Parameters Type Name Description Task task The task to ignore exceptions for. Returns Type Description Task A wrapping task for the given task. | Improve this Doc View Source IgnoreCancellation<T>(Task<T>, T) Prevents or from trickling up. Declaration public static Task<T> IgnoreCancellation<T>(this Task<T> task, T defaultCancellationReturn = null) Parameters Type Name Description Task <T> task The task to ignore exceptions for. T defaultCancellationReturn The default value to return in case the task is cancelled. Returns Type Description Task <T> A wrapping task for the given task. Type Parameters Name Description T The result type of the Task. | Improve this Doc View Source Unless(Task, CancellationToken) The task will be awaited until the cancellation token is triggered. (await task unless cancelled). Declaration public static Task Unless(this Task task, CancellationToken cancellationToken) Parameters Type Name Description Task task The task to await. CancellationToken cancellationToken The cancellation token to stop awaiting. Returns Type Description Task The task that can be awaited unless the cancellation token is triggered. Remarks This is different from cancelling the task. The use case is to enable a calling method bow out of the await that it can't cancel, but doesn't require completion/cancellation in order to cancel it's own execution."
  },
  "api/Microsoft.MixedReality.WebRTC.Transceiver.Direction.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Transceiver.Direction.html",
    "title": "Enum Transceiver.Direction | MixedReality-WebRTC Documentation",
    "keywords": "Enum Transceiver.Direction Direction of the media flowing inside the transceiver. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum Direction : int Fields Name Description Inactive Transceiver is inactive, neither sending nor receiving any media data. ReceiveOnly Transceiver is receiving from the remote peer, but is not sending any media to the remote peer. SendOnly Transceiver is sending to the remote peer, but is not receiving any media from the remote peer. SendReceive Transceiver is both sending to and receiving from the remote peer connection."
  },
  "api/Microsoft.MixedReality.WebRTC.Transceiver.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Transceiver.html",
    "title": "Class Transceiver | MixedReality-WebRTC Documentation",
    "keywords": "Class Transceiver Transceiver of a peer connection. A transceiver is a media \"pipe\" connecting the local and remote peers, and used to transmit media data (audio or video) between the peers. The transceiver has a media flow direction indicating whether it is sending and/or receiving any media, or is inactive. When sending some media, the transceiver's local track is used as the source of that media. Conversely, when receiving some media, that media is delivered to the remote media track of the transceiver. As a convenience, the local track can be null if the local peer does not have anything to send. In that case some empty media is automatically sent instead (black frames for video, silence for audio) at very reduced rate. To completely stop sending, the media direction must be changed instead. Transceivers are owned by the peer connection which creates them, and cannot be destroyed nor removed from the peer connection. They become invalid when the peer connection is closed, and should not be used after that. Inheritance Object Transceiver Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class Transceiver : object Remarks This object corresponds roughly to the same-named notion in the WebRTC 1.0 standard when using the Unified Plan SDP semantic. For Plan B semantic, where RTP transceivers are not available, this wrapper tries to emulate the transceiver concept of the Unified Plan semantic, and is therefore providing an abstraction over the WebRTC concept of transceivers. Fields | Improve this Doc View Source _desiredDirection Backing field for DesiredDirection . Declaration protected Transceiver.Direction _desiredDirection Field Value Type Description Transceiver.Direction See Also DesiredDirection Properties | Improve this Doc View Source DesiredDirection Transceiver direction desired by the user. Once changed by the user, this value is the next direction that will be negotiated when calling CreateOffer() or CreateAnswer() . After the negotiation is completed, this is generally equal to NegotiatedDirection , unless the offer was partially rejected, for example if the local peer offered to send and receive some media but the remote peer only accepted to receive. Changing the value of this property triggers a RenegotiationNeeded event. Declaration public Transceiver.Direction DesiredDirection { get; set; } Property Value Type Description Transceiver.Direction See Also NegotiatedDirection | Improve this Doc View Source LocalAudioTrack Local audio track attached to the transceiver, if MediaKind is Audio . The property has two uses: as a convenience getter to retrieve LocalTrack already cast to a LocalAudioTrack type, or null if the transceiver media kind is Video . to attach a new local audio track if the transceiver is an audio transceiver; otherwise this throws a . Declaration public LocalAudioTrack LocalAudioTrack { get; set; } Property Value Type Description LocalAudioTrack | Improve this Doc View Source LocalTrack Local track attached to the transceiver, which is used to send data to the remote peer if NegotiatedDirection includes sending. This cannot be assigned directly; instead use LocalAudioTrack or LocalVideoTrack depending on the media kind of the transceiver. Declaration public LocalMediaTrack LocalTrack { get; } Property Value Type Description LocalMediaTrack See Also LocalAudioTrack LocalVideoTrack | Improve this Doc View Source LocalVideoTrack Local video track attached to the transceiver, if MediaKind is Video . The property has two uses: as a convenience getter to retrieve LocalTrack already cast to a LocalVideoTrack type, or null if the transceiver media kind is Audio . to attach a new local video track if the transceiver is a video transceiver; otherwise this throws a . Declaration public LocalVideoTrack LocalVideoTrack { get; set; } Property Value Type Description LocalVideoTrack | Improve this Doc View Source MediaKind Type of media carried by the transceiver, and by extension type of media of its tracks. Declaration public MediaKind MediaKind { get; } Property Value Type Description MediaKind | Improve this Doc View Source MlineIndex Index of the media line in the SDP protocol for this transceiver. If the transceiver is not yet associated with a media line, this index has a negative value (invalid). Transceivers are associated when an offer, local or remote, is applied to the local peer connection. Consequently, transceivers created as a result of applying a remote offer are created in an associated state, with a media line index already valid, while transceivers created locally by the peer connection have an invalid index until the next offer. Declaration public int MlineIndex { get; } Property Value Type Description Int32 Remarks For Plan B semantic ( PlanB ), the media line index is not present in the SDP protocol. Instead it is simulated by the implementation, which attempts to emulate the behavior of the Unified Plan semantic over an actual Plan B protocol. See Also Associated | Improve this Doc View Source Name A name for the transceiver, used for logging and debugging only. This can be set on construction if the transceiver is created by the local peer using AddTransceiver(MediaKind, TransceiverInitSettings) , or will be generated by the implementation otherwise. There is no guarantee of unicity; this name is only informational. Declaration public string Name { get; } Property Value Type Description String | Improve this Doc View Source NegotiatedDirection Last negotiated transceiver direction. This is constant when changing DesiredDirection , and is only udpated after an SDP session negotiation. This might be different from the desired direction if for example the local peer asked to receive but the remote peer refused. This is the actual direction the media is effectively transported in at any point in time. Declaration public Transceiver.Direction? NegotiatedDirection { get; protected set; } Property Value Type Description Nullable < Transceiver.Direction > See Also DesiredDirection | Improve this Doc View Source PeerConnection Peer connection this transceiver is part of. Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection See Also PeerConnection | Improve this Doc View Source RemoteAudioTrack Remote audio track attached to the transceiver, if MediaKind is Audio . This is equivalent to RemoteTrack for audio transceivers, and null otherwise. Declaration public RemoteAudioTrack RemoteAudioTrack { get; } Property Value Type Description RemoteAudioTrack | Improve this Doc View Source RemoteTrack Remote track attached to the transceiver, which is used to receive data from the remote peer if NegotiatedDirection includes receiving. This cannot be assigned. This is updated automatically when the remote track is created or destroyed as part of a renegotiation. Declaration public MediaTrack RemoteTrack { get; } Property Value Type Description MediaTrack See Also RemoteAudioTrack RemoteVideoTrack | Improve this Doc View Source RemoteVideoTrack Remote video track attached to the transceiver, if MediaKind is Video . This is equivalent to RemoteTrack for video transceivers, and null otherwise. Declaration public RemoteVideoTrack RemoteVideoTrack { get; } Property Value Type Description RemoteVideoTrack | Improve this Doc View Source StreamIDs List of stream IDs associated with the transceiver. Declaration public string[] StreamIDs { get; } Property Value Type Description String [] Methods | Improve this Doc View Source DirectionFromSendRecv(Boolean, Boolean) Compute a transceiver direction from some send/receive booleans. Declaration public static Transceiver.Direction DirectionFromSendRecv(bool hasSend, bool hasRecv) Parameters Type Name Description Boolean hasSend Does the direction includes sending? Boolean hasRecv Does the direction includes receiving? Returns Type Description Transceiver.Direction The computed transceiver direction. | Improve this Doc View Source HasRecv(Transceiver.Direction) Check whether the given direction includes receiving. Declaration public static bool HasRecv(Transceiver.Direction dir) Parameters Type Name Description Transceiver.Direction dir The direction to check. Returns Type Description Boolean true if direction is ReceiveOnly or SendReceive . | Improve this Doc View Source HasRecv(Nullable<Transceiver.Direction>) Check whether the given direction includes receiving. Declaration public static bool HasRecv(Transceiver.Direction? dir) Parameters Type Name Description Nullable < Transceiver.Direction > dir The direction to check. Returns Type Description Boolean true if direction is ReceiveOnly or SendReceive . | Improve this Doc View Source HasSend(Transceiver.Direction) Check whether the given direction includes sending. Declaration public static bool HasSend(Transceiver.Direction dir) Parameters Type Name Description Transceiver.Direction dir The direction to check. Returns Type Description Boolean true if direction is SendOnly or SendReceive . | Improve this Doc View Source HasSend(Nullable<Transceiver.Direction>) Check whether the given direction includes sending. Declaration public static bool HasSend(Transceiver.Direction? dir) Parameters Type Name Description Nullable < Transceiver.Direction > dir The direction to check. Returns Type Description Boolean true if direction is SendOnly or SendReceive . Events | Improve this Doc View Source Associated Event raised when the transceiver is associated with a media line, which therefore makes the MlineIndex property take a valid positive value. Declaration public event TransceiverAssociatedDelegate Associated Event Type Type Description TransceiverAssociatedDelegate Remarks The event is not raised if the transceiver is created in an associated state, that is if the transceiver is already associated when TransceiverAdded is raised to signal it was added. This happens when the transceiver is created as part of applying a remote offer. In short, this event is raised only for transceivers created locally with AddTransceiver(MediaKind, TransceiverInitSettings) . See Also MlineIndex | Improve this Doc View Source DirectionChanged Event raised when the NegotiatedDirection changed, which occurs after applying a local or remote description. This is a convenience event raised only when the direction effectively changed, to avoid having to parse all transceivers for change after each description was applied. Declaration public event TransceiverDirectionChangedDelegate DirectionChanged Event Type Type Description TransceiverDirectionChangedDelegate See Also AddTransceiver(MediaKind, TransceiverInitSettings) Close()"
  },
  "api/Microsoft.MixedReality.WebRTC.TransceiverAssociatedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TransceiverAssociatedDelegate.html",
    "title": "Delegate TransceiverAssociatedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate TransceiverAssociatedDelegate Delegate for the Associated event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void TransceiverAssociatedDelegate(Transceiver transceiver); Parameters Type Name Description Transceiver transceiver The transceiver becoming associated."
  },
  "api/Microsoft.MixedReality.WebRTC.TransceiverDirectionChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TransceiverDirectionChangedDelegate.html",
    "title": "Delegate TransceiverDirectionChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate TransceiverDirectionChangedDelegate Delegate for the DirectionChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void TransceiverDirectionChangedDelegate(Transceiver transceiver); Parameters Type Name Description Transceiver transceiver The transceiver whose NegotiatedDirection property changed."
  },
  "api/Microsoft.MixedReality.WebRTC.TransceiverInitSettings.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TransceiverInitSettings.html",
    "title": "Class TransceiverInitSettings | MixedReality-WebRTC Documentation",
    "keywords": "Class TransceiverInitSettings Settings to create a new transceiver wrapper. Inheritance Object TransceiverInitSettings Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class TransceiverInitSettings : object Fields | Improve this Doc View Source InitialDesiredDirection Initial value of DesiredDirection . Declaration public Transceiver.Direction InitialDesiredDirection Field Value Type Description Transceiver.Direction | Improve this Doc View Source Name Transceiver name, for logging and debugging. Declaration public string Name Field Value Type Description String | Improve this Doc View Source StreamIDs List of stream IDs to associate the transceiver with. Declaration public List<string> StreamIDs Field Value Type Description List < String >"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Android.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Android.html",
    "title": "Class Android | MixedReality-WebRTC Documentation",
    "keywords": "Class Android Inheritance Object Android Inherited Members Object.ToString() Object.Equals(Object) Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public static class Android Properties IsInitialized Check if the Android interop layer for Android is already initialized. Declaration public static bool IsInitialized { get; } Property Value Type Description Boolean Methods Initialize() Initialize the MixedReality-WebRTC library interop layer for Android. This is automatically called by the various library API functions, and can be safely called multiple times (no-op after first call). Declaration public static void Initialize()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AsyncInitHelper-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AsyncInitHelper-1.html",
    "title": "Class AsyncInitHelper<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class AsyncInitHelper<T> Utility for resources in Unity components that need asynchronous initialization, and don't match well the Unity synchronous enable/disable workflow. Inheritance Object AsyncInitHelper<T> Inherited Members Object.ToString() Object.Equals(Object) Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AsyncInitHelper<T> where T : class, IDisposable Type Parameters Name Description T Remarks This keeps track of an initialization task, allows callers to poll for the initialized object, and handles cancellation/cleanup if the initialization is aborted before it has completed. Properties Result Check if the initialization task has generated a result. Declaration public T Result { get; } Property Value Type Description T The result of the initialization task if there is one and it has just successfully completed; null if there is no task. Remarks If the initialization fails with an exception, this rethrows the exception. Methods AbortInitTask() Cancel the initialization task and dispose its result when it completes. Declaration public Task AbortInitTask() Returns Type Description Task A that will complete when the initialization task has ended and its result has been disposed. Remarks Any exceptions from the initialization task are silently dropped. TrackInitTask(Task<T>, CancellationTokenSource) Starts tracking an initialization task for a resource. Declaration public void TrackInitTask(Task<T> initTask, CancellationTokenSource cts = null) Parameters Type Name Description Task <T> initTask CancellationTokenSource cts This will be used to cancel the task if aborted before it has finished. Will be disposed at the end of the task."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioReceiver.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioReceiver.html",
    "title": "Class AudioReceiver | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioReceiver Endpoint for a WebRTC remote audio track. Inheritance Object MediaReceiver AudioReceiver Inherited Members MediaReceiver.IsLive MediaReceiver.Transceiver MediaReceiver.MediaLine MediaReceiver.OnAddedToMediaLine(MediaLine) MediaReceiver.OnRemovedFromMediaLine(MediaLine) MediaReceiver.OnDestroy() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AudioReceiver : MediaReceiver Remarks Setting this on an audio MediaLine will enable the corresponding transceiver to receive. A remote track will be exposed through AudioTrack once a connection is established. The audio track can optionally be played locally with an AudioRenderer . Fields AudioStreamStarted Event raised when the audio stream started. When this event is raised, the followings are true: The Track property is a valid remote audio track. The IsLive property is true . Declaration public AudioStreamStartedEvent AudioStreamStarted Field Value Type Description AudioStreamStartedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. AudioStreamStopped Event raised when the audio stream stopped. When this event is raised, the followings are true: The IsLive property is false . Declaration public AudioStreamStoppedEvent AudioStreamStopped Field Value Type Description AudioStreamStoppedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. Properties AudioTrack Remote audio track receiving data from the remote peer. Declaration public RemoteAudioTrack AudioTrack { get; } Property Value Type Description RemoteAudioTrack Remarks This is null until: MediaKind Media kind of the receiver. Declaration public override MediaKind MediaKind { get; } Property Value Type Description MediaKind Overrides MediaReceiver.MediaKind Track Remote track associated with this receiver. null if this object is not receiving at this time. Declaration public override MediaTrack Track { get; } Property Value Type Description MediaTrack Overrides MediaReceiver.Track Remarks This is always a or a Methods OnPaired(MediaTrack) Internal callback invoked when the receiver is paired with a media track. Declaration protected override void OnPaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver is paired with. Overrides MediaReceiver.OnPaired(MediaTrack) Remarks This will be called on the Unity update thread. OnUnpaired(MediaTrack) Internal callback invoked when the receiver is unpaired from a media track. Declaration protected override void OnUnpaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver was paired with. Overrides MediaReceiver.OnUnpaired(MediaTrack) Remarks This will be called on the Unity update thread."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioRenderer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioRenderer.html",
    "title": "Class AudioRenderer | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioRenderer Utility component used to play audio frames obtained from a WebRTC audio source. Inheritance Object AudioRenderer Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AudioRenderer : MonoBehaviour Remarks Calling StartRendering(IAudioSource) and StopRendering(IAudioSource) will start/stop playing the passed through a component on the same object, if there is one. The component will play only while enabled. Fields PadWithSine If true, pad buffer underruns with a sine wave. This will cause artifacts on underruns. Use for debugging. Declaration public bool PadWithSine Field Value Type Description Boolean Methods Awake() Declaration protected void Awake() OnAudioFilterRead(Single[], Int32) Declaration protected void OnAudioFilterRead(float[] data, int channels) Parameters Type Name Description Single [] data Int32 channels OnDestroy() Declaration protected void OnDestroy() OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable() StartRendering(IAudioSource) Start rendering the passed source. Declaration public void StartRendering(IAudioSource source) Parameters Type Name Description IAudioSource source Remarks Can be used to handle AudioStreamStarted . StopRendering(IAudioSource) Stop rendering the passed source. Must be called with the same source passed to StartRendering(IAudioSource) Declaration public void StopRendering(IAudioSource source) Parameters Type Name Description IAudioSource source Remarks Can be used to handle AudioStreamStopped . See Also AudioReceiver"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html",
    "title": "Class AudioStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. Inheritance Object AudioStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AudioStreamStartedEvent : UnityEvent<IAudioSource>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html",
    "title": "Class AudioStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. Inheritance Object AudioStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class AudioStreamStoppedEvent : UnityEvent<IAudioSource>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioTrackSource.html",
    "title": "Class AudioTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioTrackSource This component represents an audio track source generating audio frames for one or more audio tracks. Inheritance Object MediaTrackSource AudioTrackSource MicrophoneSource Inherited Members MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class AudioTrackSource : MediaTrackSource Properties IsLive Indicates if the source is currently producing frames. Declaration public override bool IsLive { get; } Property Value Type Description Boolean Overrides MediaTrackSource.IsLive MediaKind Media kind of the track source. Declaration public override MediaKind MediaKind { get; } Property Value Type Description MediaKind Overrides MediaTrackSource.MediaKind Source Audio track source object from the underlying C# library that this component encapsulates. The object is owned by this component, which will create it and dispose of it automatically. Declaration public AudioTrackSource Source { get; } Property Value Type Description AudioTrackSource Methods AttachSource(AudioTrackSource) Declaration protected void AttachSource(AudioTrackSource source) Parameters Type Name Description AudioTrackSource source DisposeSource() Declaration protected void DisposeSource() See Also MicrophoneSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.CaptureCameraAttribute.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.CaptureCameraAttribute.html",
    "title": "Class CaptureCameraAttribute | MixedReality-WebRTC Documentation",
    "keywords": "Class CaptureCameraAttribute Attribute for a property used by SceneVideoSource to capture the content of a framebuffer, and for which some constraints on stereoscopic rendering options need to be enforced (and errors can be reported in the Editor if they are not followed). Inheritance Object CaptureCameraAttribute Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class CaptureCameraAttribute : PropertyAttribute Methods Validate(Camera) Validate that a given instance can be used for framebuffer capture by SceneVideoSource based on the XR settings currently in effect. Declaration public static void Validate(Camera camera) Parameters Type Name Description Camera camera The camera instance to test the settings of. See Also SceneVideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html",
    "title": "Struct ConfigurableIceServer | MixedReality-WebRTC Documentation",
    "keywords": "Struct ConfigurableIceServer ICE server as a serializable data structure for the Unity inspector. Inherited Members ValueType.Equals(Object) ValueType.GetHashCode() Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetType() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public struct ConfigurableIceServer Fields Type The type of ICE server. Declaration public IceType Type Field Value Type Description IceType Uri The unqualified URI of the server. Declaration public string Uri Field Value Type Description String Remarks The URI must not have any stun: or turn: prefix. Methods ToString() Convert the server to the representation the underlying implementation use. Declaration public override string ToString() Returns Type Description String The stringified server information. Overrides ValueType.ToString()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource-1.html",
    "title": "Class CustomVideoSource<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class CustomVideoSource<T> Abstract base component for a custom video source delivering raw video frames directly to the WebRTC implementation. Inheritance Object MediaTrackSource VideoTrackSource CustomVideoSource<T> SceneVideoSource UniformColorVideoSource Inherited Members VideoTrackSource.Source VideoTrackSource.VideoStreamStarted VideoTrackSource.VideoStreamStopped VideoTrackSource.IsLive VideoTrackSource.MediaKind VideoTrackSource.AttachSource(VideoTrackSource) VideoTrackSource.DisposeSource() MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class CustomVideoSource<T> : VideoTrackSource where T : IVideoFrameStorage Type Parameters Name Description T Methods OnDisable() Declaration protected virtual void OnDisable() OnEnable() Declaration protected virtual void OnEnable() OnFrameRequested(in FrameRequest) Declaration protected abstract void OnFrameRequested(in FrameRequest request) Parameters Type Name Description FrameRequest request"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.CaptureCameraDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.CaptureCameraDrawer.html",
    "title": "Class CaptureCameraDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class CaptureCameraDrawer Property drawer for CaptureCameraAttribute , to report an error to the user if the associated property instance cannot be used for framebuffer capture by SceneVideoSource . Inheritance Object CaptureCameraDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class CaptureCameraDrawer : PropertyDrawer Methods GetPropertyHeight(SerializedProperty, GUIContent) Declaration public override float GetPropertyHeight(SerializedProperty property, GUIContent label) Parameters Type Name Description SerializedProperty property GUIContent label Returns Type Description Single OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect position, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect position SerializedProperty property GUIContent label Validate(Camera) Validate that a given instance can be used for framebuffer capture by SceneVideoSource based on the current settings of the Unity Player for the current build platform. Declaration public static void Validate(Camera camera) Parameters Type Name Description Camera camera The camera instance to test the settings of. See Also Validate(Camera)"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ConfigurableIceServerDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ConfigurableIceServerDrawer.html",
    "title": "Class ConfigurableIceServerDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class ConfigurableIceServerDrawer Property drawer for ConfigurableIceServer , to display servers on a single line with the kind first (fixed width) and the server address next (stretching). Inheritance Object ConfigurableIceServerDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class ConfigurableIceServerDrawer : PropertyDrawer Methods GetPropertyHeight(SerializedProperty, GUIContent) Declaration public override float GetPropertyHeight(SerializedProperty property, GUIContent label) Parameters Type Name Description SerializedProperty property GUIContent label Returns Type Description Single OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect rect, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect rect SerializedProperty property GUIContent label"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor Classes CaptureCameraDrawer Property drawer for CaptureCameraAttribute , to report an error to the user if the associated property instance cannot be used for framebuffer capture by SceneVideoSource . ConfigurableIceServerDrawer Property drawer for ConfigurableIceServer , to display servers on a single line with the kind first (fixed width) and the server address next (stretching). MicrophoneSourceEditor Inspector editor for MicrophoneSource . PeerConnectionEditor Custom editor for the PeerConnection component. SdpTokenDrawer Property drawer for SdpTokenAttribute , to validate the associated string property content and display an error message box if invalid characters are found. ToggleLeftAttribute Attribute to display a boolean field with a toggle on its left, prefixing the actual text of the field. ToggleLeftDrawer Property drawer for ToggleLeftAttribute . VideoRendererEditor Inspector editor for VideoRenderer . WebcamSourceEditor Inspector editor for WebcamSource ."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.MicrophoneSourceEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.MicrophoneSourceEditor.html",
    "title": "Class MicrophoneSourceEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class MicrophoneSourceEditor Inspector editor for MicrophoneSource . Inheritance Object MicrophoneSourceEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class MicrophoneSourceEditor : UnityEditor.Editor Methods OnInspectorGUI() Override implementation of Editor.OnInspectorGUI to draw the inspector GUI for the currently selected MicrophoneSource . Declaration public override void OnInspectorGUI()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.PeerConnectionEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.PeerConnectionEditor.html",
    "title": "Class PeerConnectionEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnectionEditor Custom editor for the PeerConnection component. Inheritance Object PeerConnectionEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class PeerConnectionEditor : UnityEditor.Editor Methods OnInspectorGUI() Declaration public override void OnInspectorGUI()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.SdpTokenDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.SdpTokenDrawer.html",
    "title": "Class SdpTokenDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class SdpTokenDrawer Property drawer for SdpTokenAttribute , to validate the associated string property content and display an error message box if invalid characters are found. Inheritance Object SdpTokenDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class SdpTokenDrawer : PropertyDrawer Methods GetPropertyHeight(SerializedProperty, GUIContent) Declaration public override float GetPropertyHeight(SerializedProperty property, GUIContent label) Parameters Type Name Description SerializedProperty property GUIContent label Returns Type Description Single OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect position, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect position SerializedProperty property GUIContent label"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ToggleLeftAttribute.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ToggleLeftAttribute.html",
    "title": "Class ToggleLeftAttribute | MixedReality-WebRTC Documentation",
    "keywords": "Class ToggleLeftAttribute Attribute to display a boolean field with a toggle on its left, prefixing the actual text of the field. Inheritance Object ToggleLeftAttribute Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class ToggleLeftAttribute : PropertyAttribute"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ToggleLeftDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.ToggleLeftDrawer.html",
    "title": "Class ToggleLeftDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class ToggleLeftDrawer Property drawer for ToggleLeftAttribute . Inheritance Object ToggleLeftDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class ToggleLeftDrawer : PropertyDrawer Methods OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect position, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect position SerializedProperty property GUIContent label"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.VideoRendererEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.VideoRendererEditor.html",
    "title": "Class VideoRendererEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoRendererEditor Inspector editor for VideoRenderer . Inheritance Object VideoRendererEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class VideoRendererEditor : UnityEditor.Editor Methods OnInspectorGUI() Override implementation of Editor.OnInspectorGUI to draw the inspector GUI for the currently selected MicrophoneSource . Declaration public override void OnInspectorGUI()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.WebcamSourceEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.WebcamSourceEditor.html",
    "title": "Class WebcamSourceEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class WebcamSourceEditor Inspector editor for WebcamSource . Inheritance Object WebcamSourceEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class WebcamSourceEditor : UnityEditor.Editor Methods EnumToInt<TValue>(TValue) Helper to convert an enum to its integer value. Declaration public static int EnumToInt<TValue>(TValue value) where TValue : Enum Parameters Type Name Description TValue value The enum value. Returns Type Description Int32 The integer value associated with value . Type Parameters Name Description TValue The enum type. IntToEnum<TValue>(Int32) Helper to convert an integer to its enum value. Declaration public static TValue IntToEnum<TValue>(int value) where TValue : Enum Parameters Type Name Description Int32 value The integer value. Returns Type Description TValue The enum value whose integer value is value . Type Parameters Name Description TValue The enum type. OnInspectorGUI() Override implementation of Editor.OnInspectorGUI to draw the inspector GUI for the currently selected WebcamSource . Declaration public override void OnInspectorGUI()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity Classes Android AsyncInitHelper<T> Utility for resources in Unity components that need asynchronous initialization, and don't match well the Unity synchronous enable/disable workflow. AudioReceiver Endpoint for a WebRTC remote audio track. AudioRenderer Utility component used to play audio frames obtained from a WebRTC audio source. AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. AudioTrackSource This component represents an audio track source generating audio frames for one or more audio tracks. CaptureCameraAttribute Attribute for a property used by SceneVideoSource to capture the content of a framebuffer, and for which some constraints on stereoscopic rendering options need to be enforced (and errors can be reported in the Editor if they are not followed). CustomVideoSource<T> Abstract base component for a custom video source delivering raw video frames directly to the WebRTC implementation. InvalidTransceiverMediaKindException Exception thrown when an invalid transceiver media kind was detected, generally when trying to pair a transceiver of one media kind with a media line of a different media kind. LocalOnlySignaler Simple signaler using two peer connections in the same process, and hard-coding their SDP message delivery to avoid the need for any kind of networking to deliver SDP messages. This component is designed to be used in demos where both peers are present in the same scene. MediaLine Media line abstraction for a peer connection. This container binds together a source component ( MediaTrackSource ) and/or a receiver component ( MediaReceiver ) on one side, with a transceiver on the other side. The media line is a declarative representation of this association, which is then turned into a binding by the implementation during an SDP negotiation. This forms the core of the algorithm allowing automatic transceiver pairing between the two peers based on the declaration of intent of the user. Assigning Unity components to the Source and Receiver properties serves as an indication of the user intent to send and/or receive media through the transceiver, and is used during the SDP exchange to derive the to negotiate. After the SDP negotiation is completed, the Transceiver property refers to the transceiver associated with this media line, and which the sender and receiver will use. Users typically interact with this class through the peer connection transceiver collection in the Unity inspector window, though direct manipulation via code is also possible. MediaReceiver Base class for media producers generating frames by receiving them from a remote peer. MediaTrackSource Base class for media track source components producing some media frames locally. MicrophoneSource This component represents a local audio source generating audio frames from a local audio capture device (microphone). The audio source can be used to create one or more audio tracks sharing the same audio content. NativeVideoRenderer This will render the video stream through native calls with DX11 or OpenGL, completely bypassing C# marshalling. This provides a considerable performance improvement compared to VideoRenderer . NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. SceneVideoSource Custom video source capturing the Unity scene content as rendered by a given camera, and sending it as a video track through the selected peer connection. SdpTokenAttribute Attribute for string properties representing an SDP token, which has constraints on the allowed characters it can contain, as defined in the SDP RFC. See https://tools.ietf.org/html/rfc4566#page-43 for details. Signaler Abstract base class to simplify implementing a WebRTC signaling solution in Unity. There is no requirement to use this class as a base class for a custom implementation, but it handles automatically registering the necessary PeerConnection event handlers, as well as dispatching free-threaded callbacks to the main Unity app thread for simplicity and safety, and leaves the implementation with instead with two sending methods SendMessageAsync(SdpMessage) and SendMessageAsync(IceCandidate) to implement, as well as handling received messages. TextureDesc UniformColorVideoSource A video source producing some colored frames generated programmatically. VideoReceiver Endpoint for a WebRTC remote video track. VideoRenderer Utility component used to play video frames obtained from a WebRTC video track. This can indiscriminately play video frames from a video track source on the local peer as well as video frames from a remote video receiver obtaining its frame from a remote WebRTC peer. VideoStreamStartedEvent Unity event corresponding to a new video stream being started. VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. VideoTrackSource This component represents a video track source, an entity which produces raw video frames for one or more tracks. The source can be added on a peer connection media line to be sent through that peer connection. It is a standalone object, independent of any peer connection, and can be shared with multiple of them. WebcamSource This component represents a local video sender generating video frames from a local video capture device (webcam). WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. WorkQueue Base class providing some utility work queue to dispatch free-threaded actions to the main Unity application thread, where the handler(s) can safely access Unity objects. Structs ConfigurableIceServer ICE server as a serializable data structure for the Unity inspector. VideoCaptureConstraints Additional optional constraints applied to the resolution and framerate when selecting a video capture format. Enums IceType Enumeration of the different types of ICE servers. LocalVideoSourceFormatMode Video capture format selection mode for a local video source. VideoKind Delegates LogCallback TextureSizeChangeCallback"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html",
    "title": "Enum IceType | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceType Enumeration of the different types of ICE servers. Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum IceType Fields Name Description None Indicates there is no ICE information Stun Indicates ICE information is of type STUN Turn Indicates ICE information is of type TURN"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.InvalidTransceiverMediaKindException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.InvalidTransceiverMediaKindException.html",
    "title": "Class InvalidTransceiverMediaKindException | MixedReality-WebRTC Documentation",
    "keywords": "Class InvalidTransceiverMediaKindException Exception thrown when an invalid transceiver media kind was detected, generally when trying to pair a transceiver of one media kind with a media line of a different media kind. Inheritance Object InvalidTransceiverMediaKindException Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class InvalidTransceiverMediaKindException : Exception Constructors InvalidTransceiverMediaKindException() Declaration public InvalidTransceiverMediaKindException() InvalidTransceiverMediaKindException(String) Declaration public InvalidTransceiverMediaKindException(string message) Parameters Type Name Description String message InvalidTransceiverMediaKindException(String, Exception) Declaration public InvalidTransceiverMediaKindException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalOnlySignaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalOnlySignaler.html",
    "title": "Class LocalOnlySignaler | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalOnlySignaler Simple signaler using two peer connections in the same process, and hard-coding their SDP message delivery to avoid the need for any kind of networking to deliver SDP messages. This component is designed to be used in demos where both peers are present in the same scene. Inheritance Object WorkQueue LocalOnlySignaler Inherited Members WorkQueue.IsMainAppThread WorkQueue.EnsureIsMainAppThread() WorkQueue.InvokeOnAppThread(Action) WorkQueue.Awake() WorkQueue.Update() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class LocalOnlySignaler : WorkQueue Fields Peer1 First peer to connect, which will generate an offer. Declaration public PeerConnection Peer1 Field Value Type Description PeerConnection Peer2 Second peer to connect, which will wait for an offer from the first peer. Declaration public PeerConnection Peer2 Field Value Type Description PeerConnection Properties IsConnected Check if the last connection attempt successfully completed. This is reset to false each time StartConnection() is called, and is updated after WaitForConnection(Int32) returned to indicate if the connection succeeded. Declaration public bool IsConnected { get; } Property Value Type Description Boolean Methods StartConnection() Initiate a connection by having Peer1 send an offer to Peer2 , and wait until the SDP exchange completed. To wait for completion, use WaitForConnection(Int32) then check the value of IsConnected after that to determine if WaitForConnection(Int32) terminated due to the connection being established or if it timed out. Declaration public bool StartConnection() Returns Type Description Boolean true if the exchange started successfully, or false otherwise. WaitForConnection(Int32) Wait for the connection being established. Declaration public IEnumerator WaitForConnection(int millisecondsTimeout) Parameters Type Name Description Int32 millisecondsTimeout Timeout in milliseconds to wait for the connection. Returns Type Description IEnumerator An enumerator used to yield while waiting. Examples Assert.IsTrue(signaler.StartConnection()); yield return signaler.WaitForConnection(millisecondsTimeout: 10000); Assert.IsTrue(signaler.IsConnected);"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSourceFormatMode.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSourceFormatMode.html",
    "title": "Enum LocalVideoSourceFormatMode | MixedReality-WebRTC Documentation",
    "keywords": "Enum LocalVideoSourceFormatMode Video capture format selection mode for a local video source. Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum LocalVideoSourceFormatMode Fields Name Description Automatic Automatically select a good resolution and framerate based on the runtime detection of the device the application is running on. This currently overwrites the default WebRTC selection only on HoloLens devices. Manual Manually specify a video profile unique ID and/or a kind of video profile to use, and additional optional constraints on the resolution and framerate of that profile."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LogCallback.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LogCallback.html",
    "title": "Delegate LogCallback | MixedReality-WebRTC Documentation",
    "keywords": "Delegate LogCallback Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public delegate void LogCallback(string str); Parameters Type Name Description String str"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MediaLine.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MediaLine.html",
    "title": "Class MediaLine | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaLine Media line abstraction for a peer connection. This container binds together a source component ( MediaTrackSource ) and/or a receiver component ( MediaReceiver ) on one side, with a transceiver on the other side. The media line is a declarative representation of this association, which is then turned into a binding by the implementation during an SDP negotiation. This forms the core of the algorithm allowing automatic transceiver pairing between the two peers based on the declaration of intent of the user. Assigning Unity components to the Source and Receiver properties serves as an indication of the user intent to send and/or receive media through the transceiver, and is used during the SDP exchange to derive the to negotiate. After the SDP negotiation is completed, the Transceiver property refers to the transceiver associated with this media line, and which the sender and receiver will use. Users typically interact with this class through the peer connection transceiver collection in the Unity inspector window, though direct manipulation via code is also possible. Inheritance Object MediaLine Inherited Members Object.ToString() Object.Equals(Object) Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class MediaLine Properties LocalTrack Local track created from a local source. Declaration public LocalMediaTrack LocalTrack { get; } Property Value Type Description LocalMediaTrack Remarks This is non- null when a live source is attached to the MediaLine , and the owning PeerConnection is connected. MediaKind Kind of media of the media line and its attached transceiver. This is assiged when the media line is created with AddMediaLine(MediaKind) and is immutable for the lifetime of the peer connection. Declaration public MediaKind MediaKind { get; } Property Value Type Description MediaKind Peer PeerConnection owning this MediaLine . Declaration public PeerConnection Peer { get; } Property Value Type Description PeerConnection Receiver Media receiver consuming the media received through the transceiver attached to this media line. Declaration public MediaReceiver Receiver { get; set; } Property Value Type Description MediaReceiver Remarks This must be an instance of a class derived from AudioReceiver or VideoReceiver depending on whether MediaKind is or , respectively. If this is non- null then the peer connection will negotiate receiving some media, otherwise it will signal the remote peer that it does not wish to receive (send-only or inactive). If Transceiver is valid, that is a first session negotiation has already been conducted, then changing this value raises a event on the peer connection of Transceiver . Must be changed on the main Unity app thread. SenderTrackName Name of the local media track this component will create when calling . If left empty, the implementation will generate a unique name for the track (generally a GUID). Declaration public string SenderTrackName { get; set; } Property Value Type Description String Remarks This value must comply with the 'msid' attribute rules as defined in https://tools.ietf.org/html/draft-ietf-mmusic-msid-05#section-2 , which in particular constraints the set of allowed characters to those allowed for a 'token' element as specified in https://tools.ietf.org/html/rfc4566#page-43 : Symbols [!#$%'*+-.^_`{|}~] and ampersand & Alphanumerical characters [A-Za-z0-9] Users can manually test if a string is a valid SDP token with the utility method Validate(String, Boolean) . The property setter will use this and throw an if the token is not a valid SDP token. The sender track name is taken into account each time the track is created. If this property is assigned after the track was created (already negotiated), the value will be used only for the next negotiation, and the current sender track will keep its current track name (either a previous value or a generated one). See Also Validate(String, Boolean) Source Media source producing the media to send through the transceiver attached to this media line. Declaration public MediaTrackSource Source { get; set; } Property Value Type Description MediaTrackSource Remarks This must be an instance of a class derived from AudioTrackSource or VideoTrackSource depending on whether MediaKind is or , respectively. Internally the peer connection will automatically create and manage a media track to bridge the media source with the transceiver. If this is non- null then the peer connection will negotiate sending some media, otherwise it will signal the remote peer that it does not wish to send (receive-only or inactive). If Transceiver is valid, that is a first session negotiation has already been completed, then changing this value raises a event on the peer connection of Transceiver . Must be changed on the main Unity app thread. Transceiver Transceiver attached with this media line. On the offering peer this changes during StartConnection() , while this is updated by when receiving an offer on the answering peer. Because transceivers cannot be destroyed, once this property is assigned a non- null value it keeps that value until the peer connection owning the media line is closed. Declaration public Transceiver Transceiver { get; } Property Value Type Description Transceiver"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MediaReceiver.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MediaReceiver.html",
    "title": "Class MediaReceiver | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaReceiver Base class for media producers generating frames by receiving them from a remote peer. Inheritance Object MediaReceiver AudioReceiver VideoReceiver Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class MediaReceiver : MonoBehaviour Properties IsLive Is the media source currently producing frames received from the remote peer? This is true while the remote media track exists, which is notified by events on the AudioReceiver or VideoReceiver . Declaration public bool IsLive { get; } Property Value Type Description Boolean MediaKind Media kind of the receiver. Declaration public abstract MediaKind MediaKind { get; } Property Value Type Description MediaKind MediaLine Media line this receiver is paired with, if any. Declaration public MediaLine MediaLine { get; } Property Value Type Description MediaLine Remarks Note that this is set to the connected MediaLine only if the owning PeerConnection is awake. This will be automatically reset if the PeerConnection owning the MediaLine is destroyed. Track Remote track associated with this receiver. null if this object is not receiving at this time. Declaration public abstract MediaTrack Track { get; } Property Value Type Description MediaTrack Remarks This is always a or a Transceiver Transceiver this receiver is paired with, if any. This is null until a remote description is applied which pairs the media line this receiver is associated with to a transceiver, or until the peer connection of this receiver's media line creates the receiver right before creating an SDP offer. Declaration public Transceiver Transceiver { get; } Property Value Type Description Transceiver Methods OnAddedToMediaLine(MediaLine) Internal callback invoked when the media receiver is assigned to a media line. Declaration protected virtual void OnAddedToMediaLine(MediaLine mediaLine) Parameters Type Name Description MediaLine mediaLine The new media line this receiver is assigned to. OnDestroy() Declaration protected void OnDestroy() OnPaired(MediaTrack) Internal callback invoked when the receiver is paired with a media track. Declaration protected virtual void OnPaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver is paired with. Remarks This will be called on the Unity update thread. OnRemovedFromMediaLine(MediaLine) Internal callback invoked when the media receiver is de-assigned from a media line. Declaration protected virtual void OnRemovedFromMediaLine(MediaLine mediaLine) Parameters Type Name Description MediaLine mediaLine The old media line this receiver was assigned to. OnUnpaired(MediaTrack) Internal callback invoked when the receiver is unpaired from a media track. Declaration protected virtual void OnUnpaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver was paired with. Remarks This will be called on the Unity update thread."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MediaTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MediaTrackSource.html",
    "title": "Class MediaTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaTrackSource Base class for media track source components producing some media frames locally. Inheritance Object MediaTrackSource AudioTrackSource VideoTrackSource Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class MediaTrackSource : MonoBehaviour Properties IsLive Indicates if the source is currently producing frames. Declaration public abstract bool IsLive { get; } Property Value Type Description Boolean MediaKind Media kind of the track source. Declaration public abstract MediaKind MediaKind { get; } Property Value Type Description MediaKind MediaLines List of audio media lines using this source. Declaration public IReadOnlyList<MediaLine> MediaLines { get; } Property Value Type Description IReadOnlyList < MediaLine > Remarks Note that a connected MediaLine will be added to this only if the owning PeerConnection is awake. A MediaLine will be automatically removed if the owning PeerConnection is destroyed. Methods AttachToMediaLines() Declaration protected void AttachToMediaLines() DetachFromMediaLines() Declaration protected void DetachFromMediaLines() See Also AudioTrackSource VideoTrackSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MicrophoneSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MicrophoneSource.html",
    "title": "Class MicrophoneSource | MixedReality-WebRTC Documentation",
    "keywords": "Class MicrophoneSource This component represents a local audio source generating audio frames from a local audio capture device (microphone). The audio source can be used to create one or more audio tracks sharing the same audio content. Inheritance Object MediaTrackSource AudioTrackSource MicrophoneSource Inherited Members AudioTrackSource.Source AudioTrackSource.MediaKind AudioTrackSource.IsLive AudioTrackSource.AttachSource(AudioTrackSource) AudioTrackSource.DisposeSource() MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class MicrophoneSource : AudioTrackSource Fields _autoGainControl Declaration protected bool _autoGainControl Field Value Type Description Boolean Properties AutoGainControl Declaration public bool AutoGainControl { get; } Property Value Type Description Boolean Methods OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable() Update() Declaration protected void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.NativeVideoRenderer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.NativeVideoRenderer.html",
    "title": "Class NativeVideoRenderer | MixedReality-WebRTC Documentation",
    "keywords": "Class NativeVideoRenderer This will render the video stream through native calls with DX11 or OpenGL, completely bypassing C# marshalling. This provides a considerable performance improvement compared to VideoRenderer . Inheritance Object NativeVideoRenderer Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class NativeVideoRenderer : MonoBehaviour Methods StartRendering(IVideoSource) Start rendering the passed source. Declaration public void StartRendering(IVideoSource source) Parameters Type Name Description IVideoSource source Remarks Can be used to handle VideoStreamStarted or VideoStreamStarted . StopRendering(IVideoSource) Stop rendering the passed source. Must be called with the same source passed to StartRendering(IVideoSource) Declaration public void StopRendering(IVideoSource _) Parameters Type Name Description IVideoSource _ Remarks Can be used to handle VideoStreamStopped or VideoStreamStopped ."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html",
    "title": "Class NodeDssSignaler | MixedReality-WebRTC Documentation",
    "keywords": "Class NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. Inheritance Object Signaler NodeDssSignaler Inherited Members Signaler.PeerConnection Signaler._nativePeer Signaler._mainThreadWorkQueue Signaler.OnPeerInitialized() Signaler.OnPeerUninitializing() Signaler.OnEnable() Signaler.OnDisable() Signaler.OnIceCandidateReadyToSend(IceCandidate) Signaler.OnSdpOfferReadyToSend(SdpMessage) Signaler.OnSdpAnswerReadyToSend(SdpMessage) Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class NodeDssSignaler : Signaler Fields AutoLogErrors Automatically log all errors to the Unity console. Declaration public bool AutoLogErrors Field Value Type Description Boolean HttpServerAddress The https://github.com/bengreenier/node-dss HTTP service address to connect to Declaration public string HttpServerAddress Field Value Type Description String LocalPeerId Unique identifier of the local peer. Declaration public string LocalPeerId Field Value Type Description String PollTimeMs The interval (in ms) that the server is polled at Declaration public float PollTimeMs Field Value Type Description Single RemotePeerId Unique identifier of the remote peer. Declaration public string RemotePeerId Field Value Type Description String Methods SendMessageAsync(IceCandidate) Asynchronously send an ICE candidate to the remote peer. Declaration public override Task SendMessageAsync(IceCandidate candidate) Parameters Type Name Description IceCandidate candidate The ICE candidate to send to the remote peer. Returns Type Description Task A object completed once the message has been sent, but not necessarily delivered. Overrides Signaler.SendMessageAsync(IceCandidate) SendMessageAsync(SdpMessage) Asynchronously send an SDP message to the remote peer. Declaration public override Task SendMessageAsync(SdpMessage message) Parameters Type Name Description SdpMessage message The SDP message to send to the remote peer. Returns Type Description Task A object completed once the message has been sent, but not necessarily delivered. Overrides Signaler.SendMessageAsync(SdpMessage) Update() Unity Engine Update() hook Declaration protected override void Update() Overrides Signaler.Update() Remarks https://docs.unity3d.com/ScriptReference/MonoBehaviour.Update.html"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. Inheritance Object WorkQueue PeerConnection Implements ISerializationCallbackReceiver Inherited Members WorkQueue.IsMainAppThread WorkQueue.EnsureIsMainAppThread() WorkQueue.InvokeOnAppThread(Action) Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class PeerConnection : WorkQueue Remarks The component initializes the underlying asynchronously when enabled, and closes it when disabled. The OnInitialized event is called when the connection object is ready to be used. Call StartConnection() to create an offer for a remote peer. Fields AutoCreateOfferOnRenegotiationNeeded Automatically create a new offer whenever a renegotiation needed event is received. Declaration public bool AutoCreateOfferOnRenegotiationNeeded Field Value Type Description Boolean Remarks Note that the renegotiation needed event may be dispatched asynchronously, so it is discourages to toggle this field ON and OFF. Instead, the user should choose an approach (manual or automatic) and stick to it. In particular, temporarily setting this to false during a batch of changes and setting it back to true right after the last change may or may not produce an automatic offer, depending on whether the negotiated event was dispatched while the property was still false or not. AutoLogErrorsToUnityConsole Flag to log all errors to the Unity console automatically. Declaration public bool AutoLogErrorsToUnityConsole Field Value Type Description Boolean IceCredential Optional credential for the ICE servers. Declaration public string IceCredential Field Value Type Description String IceServers Set of ICE servers the WebRTC library will use to try to establish a connection. Declaration public List<ConfigurableIceServer> IceServers Field Value Type Description List < ConfigurableIceServer > IceUsername Optional username for the ICE servers. Declaration public string IceUsername Field Value Type Description String OnError Event that occurs when a WebRTC error occurs Declaration public WebRTCErrorEvent OnError Field Value Type Description WebRTCErrorEvent OnInitialized Event fired after the peer connection is initialized and ready for use. Declaration public UnityEvent OnInitialized Field Value Type Description UnityEvent OnShutdown Event fired after the peer connection is shut down and cannot be used anymore. Declaration public UnityEvent OnShutdown Field Value Type Description UnityEvent Properties Peer Retrieves the underlying peer connection object once initialized. Declaration public PeerConnection Peer { get; } Property Value Type Description PeerConnection Remarks If OnInitialized has not fired, this will be null . Methods AddMediaLine(MediaKind) Add a new media line of the given kind. This method creates a media line, which expresses an intent from the user to get a transceiver. The actual object creation is delayed until a session negotiation is completed. Once the media line is created, the user can then assign its Source and Receiver properties to express their intent to send and/or receive some media through the transceiver that will be associated with that media line once a session is negotiated. This information is used in subsequent negotiations to derive a to negotiate. Therefore users should avoid modifying the property manually when using the Unity library, and instead modify the Source and Receiver properties. Declaration public MediaLine AddMediaLine(MediaKind kind) Parameters Type Name Description MediaKind kind The kind of media (audio or video) for the transceiver. Returns Type Description MediaLine A newly created media line, which will be associated with a transceiver once the next session is negotiated. Awake() Declaration protected override void Awake() Overrides WorkQueue.Awake() GetVideoCaptureDevicesAsync() Enumerate the video capture devices available as a WebRTC local video feed source. Declaration public static Task<IReadOnlyList<VideoCaptureDevice>> GetVideoCaptureDevicesAsync() Returns Type Description Task < IReadOnlyList < VideoCaptureDevice >> The list of local video capture devices available to WebRTC. HandleConnectionMessageAsync(SdpMessage) Pass the given SDP description received from the remote peer via signaling to the underlying WebRTC implementation, which will parse and use it. This must be called by the signaler when receiving a message. Once this operation has completed, it is safe to call . IMPORTANT This method is very similar to the SetRemoteDescriptionAsync() method available in the underlying C# library, and actually calls it. However it also performs additional work in order to pair the transceivers of the local and remote peer. Therefore Unity applications must call this method instead of the C# library one to ensure transceiver pairing works as intended. Declaration public async Task HandleConnectionMessageAsync(SdpMessage message) Parameters Type Name Description SdpMessage message The SDP message to handle. Returns Type Description Task A task which completes once the remote description has been applied and transceivers have been updated. Remarks This method can only be called from the main Unity application thread, where Unity objects can be safely accessed. OnAfterDeserialize() Declaration public void OnAfterDeserialize() OnBeforeSerialize() Declaration public void OnBeforeSerialize() StartConnection() Create a new connection offer, either for a first connection to the remote peer, or for renegotiating some new or removed transceivers. This method submits an internal task to create an SDP offer message. Once the message is created, the implementation raises the event to allow the user to send the message via the chosen signaling solution to the remote peer. IMPORTANT This method is very similar to the CreateOffer() method available in the underlying C# library, and actually calls it. However it also performs additional work in order to pair the transceivers of the local and remote peer. Therefore Unity applications must call this method instead of the C# library one to ensure transceiver pairing works as intended. Declaration public bool StartConnection() Returns Type Description Boolean true if the offer creation task was submitted successfully, and false otherwise. The offer SDP message is always created asynchronously. Remarks This method can only be called from the main Unity application thread, where Unity objects can be safely accessed. StartConnectionIgnoreError() Call StartConnection() and discard the result. Can be wired to a . Declaration public void StartConnectionIgnoreError() Update() Declaration protected override void Update() Overrides WorkQueue.Update() Implements ISerializationCallbackReceiver"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.SceneVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.SceneVideoSource.html",
    "title": "Class SceneVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class SceneVideoSource Custom video source capturing the Unity scene content as rendered by a given camera, and sending it as a video track through the selected peer connection. Inheritance Object MediaTrackSource VideoTrackSource CustomVideoSource < Argb32VideoFrameStorage > SceneVideoSource Inherited Members VideoTrackSource.Source VideoTrackSource.VideoStreamStarted VideoTrackSource.VideoStreamStopped VideoTrackSource.IsLive VideoTrackSource.MediaKind VideoTrackSource.AttachSource(VideoTrackSource) VideoTrackSource.DisposeSource() MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class SceneVideoSource : CustomVideoSource<Argb32VideoFrameStorage> Fields CameraEvent Camera event indicating the point in time during the Unity frame rendering when the camera rendering is to be captured. This defaults to , which is a reasonable default to capture the entire scene rendering, but can be customized to achieve other effects like capturing only a part of the scene. Declaration public CameraEvent CameraEvent Field Value Type Description CameraEvent SourceCamera Camera used to capture the scene content, whose rendering is used as video content for the track. Declaration public Camera SourceCamera Field Value Type Description Camera Remarks If the project uses Multi-Pass stereoscopic rendering, then this camera needs to render to a single eye to produce a single video frame. Generally this means that this needs to be a separate Unity camera from the one used for XR rendering, which is generally rendering to both eyes. If the project uses Single-Pass Instanced stereoscopic rendering, then Unity 2019.1+ is required to make this component work, due to the fact earlier versions of Unity are missing some command buffer API calls to be able to efficiently access the camera backbuffer in this mode. For Unity 2018.3 users who cannot upgrade, use Single-Pass (non-instanced) instead. Methods OnDisable() Declaration protected override void OnDisable() Overrides Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource<Argb32VideoFrameStorage>.OnDisable() OnEnable() Declaration protected override void OnEnable() Overrides Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource<Argb32VideoFrameStorage>.OnEnable() OnFrameRequested(in FrameRequest) Declaration protected override void OnFrameRequested(in FrameRequest request) Parameters Type Name Description FrameRequest request Overrides Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource<Argb32VideoFrameStorage>.OnFrameRequested(FrameRequest)"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.SdpTokenAttribute.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.SdpTokenAttribute.html",
    "title": "Class SdpTokenAttribute | MixedReality-WebRTC Documentation",
    "keywords": "Class SdpTokenAttribute Attribute for string properties representing an SDP token, which has constraints on the allowed characters it can contain, as defined in the SDP RFC. See https://tools.ietf.org/html/rfc4566#page-43 for details. Inheritance Object SdpTokenAttribute Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class SdpTokenAttribute : PropertyAttribute Constructors SdpTokenAttribute(Boolean) Declaration public SdpTokenAttribute(bool allowEmpty = true) Parameters Type Name Description Boolean allowEmpty Value of AllowEmpty . Fields InvalidCharacters Regular expression that matches all characters which can't be used in an SDP token. Declaration public const string InvalidCharacters = \"[^A-Za-z0-9!#$%&'*+-.^_`{|}~]\" Field Value Type Description String Properties AllowEmpty Allow empty tokens, that is a string property which is null or an empty string. This is not valid in the RFC, but can be allowed as a property value to represent a default value generated at runtime by the implementation instead of being provided by the user. This is typically used as an argument to Validate(String, Boolean) . Declaration public bool AllowEmpty { get; } Property Value Type Description Boolean true to allow the property to be null or empty. Methods Validate(String, Boolean) Validate an SDP token name against the list of allowed characters: Symbols [!#$%'*+-.^_`{|}~&] Alphanumerical characters [A-Za-z0-9] If the validation fails, the method throws an exception. Declaration public static void Validate(string name, bool allowEmpty = true) Parameters Type Name Description String name The token name to validate. Boolean allowEmpty true to allow the property to be null or empty without raising an exception. Remarks See https://tools.ietf.org/html/rfc4566#page-43 for 'token' reference."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html",
    "title": "Class Signaler | MixedReality-WebRTC Documentation",
    "keywords": "Class Signaler Abstract base class to simplify implementing a WebRTC signaling solution in Unity. There is no requirement to use this class as a base class for a custom implementation, but it handles automatically registering the necessary PeerConnection event handlers, as well as dispatching free-threaded callbacks to the main Unity app thread for simplicity and safety, and leaves the implementation with instead with two sending methods SendMessageAsync(SdpMessage) and SendMessageAsync(IceCandidate) to implement, as well as handling received messages. Inheritance Object Signaler NodeDssSignaler Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class Signaler : MonoBehaviour Fields _mainThreadWorkQueue Task queue used to defer actions to the main Unity app thread, which is the only thread with access to Unity objects. Declaration protected ConcurrentQueue<Action> _mainThreadWorkQueue Field Value Type Description ConcurrentQueue < Action > _nativePeer Native PeerConnection object from the underlying WebRTC C# library, available once the peer has been initialized. Declaration protected PeerConnection _nativePeer Field Value Type Description PeerConnection PeerConnection The PeerConnection this signaler needs to work for. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection Methods OnDisable() Declaration protected virtual void OnDisable() OnEnable() Declaration protected virtual void OnEnable() OnIceCandidateReadyToSend(IceCandidate) Callback invoked when an ICE candidate message has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected virtual void OnIceCandidateReadyToSend(IceCandidate candidate) Parameters Type Name Description IceCandidate candidate ICE candidate to send to the remote peer. OnPeerInitialized() Callback fired from the PeerConnection when it finished initializing, to subscribe to signaling-related events. Declaration public void OnPeerInitialized() OnPeerUninitializing() Callback fired from the PeerConnection before it starts uninitializing itself and disposing of the underlying implementation object. Declaration public void OnPeerUninitializing() OnSdpAnswerReadyToSend(SdpMessage) Callback invoked when a local SDP answer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected virtual void OnSdpAnswerReadyToSend(SdpMessage answer) Parameters Type Name Description SdpMessage answer The SDP answer message to send. OnSdpOfferReadyToSend(SdpMessage) Callback invoked when a local SDP offer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected virtual void OnSdpOfferReadyToSend(SdpMessage offer) Parameters Type Name Description SdpMessage offer The SDP offer message to send. SendMessageAsync(IceCandidate) Asynchronously send an ICE candidate to the remote peer. Declaration public abstract Task SendMessageAsync(IceCandidate candidate) Parameters Type Name Description IceCandidate candidate The ICE candidate to send to the remote peer. Returns Type Description Task A object completed once the message has been sent, but not necessarily delivered. SendMessageAsync(SdpMessage) Asynchronously send an SDP message to the remote peer. Declaration public abstract Task SendMessageAsync(SdpMessage message) Parameters Type Name Description SdpMessage message The SDP message to send to the remote peer. Returns Type Description Task A object completed once the message has been sent, but not necessarily delivered. Update() Unity Engine Update() hook Declaration protected virtual void Update() Remarks https://docs.unity3d.com/ScriptReference/MonoBehaviour.Update.html"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.TextureDesc.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.TextureDesc.html",
    "title": "Class TextureDesc | MixedReality-WebRTC Documentation",
    "keywords": "Class TextureDesc Inheritance Object TextureDesc Inherited Members Object.ToString() Object.Equals(Object) Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class TextureDesc Fields height Declaration public int height Field Value Type Description Int32 texture Declaration public IntPtr texture Field Value Type Description IntPtr width Declaration public int width Field Value Type Description Int32"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.TextureSizeChangeCallback.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.TextureSizeChangeCallback.html",
    "title": "Delegate TextureSizeChangeCallback | MixedReality-WebRTC Documentation",
    "keywords": "Delegate TextureSizeChangeCallback Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public delegate void TextureSizeChangeCallback(int width, int height, IntPtr videoHandle); Parameters Type Name Description Int32 width Int32 height IntPtr videoHandle"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.UniformColorVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.UniformColorVideoSource.html",
    "title": "Class UniformColorVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class UniformColorVideoSource A video source producing some colored frames generated programmatically. Inheritance Object MediaTrackSource VideoTrackSource CustomVideoSource < Argb32VideoFrameStorage > UniformColorVideoSource Inherited Members CustomVideoSource<Argb32VideoFrameStorage>.OnEnable() CustomVideoSource<Argb32VideoFrameStorage>.OnDisable() VideoTrackSource.Source VideoTrackSource.VideoStreamStarted VideoTrackSource.VideoStreamStopped VideoTrackSource.IsLive VideoTrackSource.MediaKind VideoTrackSource.AttachSource(VideoTrackSource) VideoTrackSource.DisposeSource() MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class UniformColorVideoSource : CustomVideoSource<Argb32VideoFrameStorage> Fields Colors List of colors to cycle through. Declaration public List<Color32> Colors Field Value Type Description List < Color32 > Speed Color cycling speed, in change per second. Declaration public float Speed Field Value Type Description Single Methods OnFrameRequested(in FrameRequest) Declaration protected override void OnFrameRequested(in FrameRequest request) Parameters Type Name Description FrameRequest request Overrides Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource<Argb32VideoFrameStorage>.OnFrameRequested(FrameRequest) Start() Declaration protected void Start() Update() Declaration protected void Update() UpdateBuffer() Declaration protected void UpdateBuffer()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoCaptureConstraints.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoCaptureConstraints.html",
    "title": "Struct VideoCaptureConstraints | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureConstraints Additional optional constraints applied to the resolution and framerate when selecting a video capture format. Inherited Members ValueType.Equals(Object) ValueType.GetHashCode() ValueType.ToString() Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetType() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public struct VideoCaptureConstraints Fields framerate Desired framerate, in frame-per-second, or zero for unconstrained. Note: the comparison is exact, and floating point imprecision may prevent finding a matching format. Use with caution. Declaration public double framerate Field Value Type Description Double height Desired resolution height, in pixels, or zero for unconstrained. Declaration public int height Field Value Type Description Int32 width Desired resolution width, in pixels, or zero for unconstrained. Declaration public int width Field Value Type Description Int32"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoKind.html",
    "title": "Enum VideoKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum VideoKind Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum VideoKind Fields Name Description ARGB I420 None"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoReceiver.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoReceiver.html",
    "title": "Class VideoReceiver | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoReceiver Endpoint for a WebRTC remote video track. Inheritance Object MediaReceiver VideoReceiver Inherited Members MediaReceiver.IsLive MediaReceiver.Transceiver MediaReceiver.MediaLine MediaReceiver.OnAddedToMediaLine(MediaLine) MediaReceiver.OnRemovedFromMediaLine(MediaLine) MediaReceiver.OnDestroy() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class VideoReceiver : MediaReceiver Remarks Setting this on a video MediaLine will enable the corresponding transceiver to receive. A remote track will be exposed through VideoTrack once a connection is established. The video track can optionally be displayed locally with a VideoRenderer . Fields VideoStreamStarted Event raised when the video stream started. When this event is raised, the followings are true: The Track property is a valid remote video track. The IsLive property is true . Declaration public VideoStreamStartedEvent VideoStreamStarted Field Value Type Description VideoStreamStartedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. VideoStreamStopped Event raised when the video stream stopped. When this event is raised, the followings are true: The Track property is null . The IsLive property is false . Declaration public VideoStreamStoppedEvent VideoStreamStopped Field Value Type Description VideoStreamStoppedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. Properties MediaKind Media kind of the receiver. Declaration public override MediaKind MediaKind { get; } Property Value Type Description MediaKind Overrides MediaReceiver.MediaKind Track Remote track associated with this receiver. null if this object is not receiving at this time. Declaration public override MediaTrack Track { get; } Property Value Type Description MediaTrack Overrides MediaReceiver.Track Remarks This is always a or a VideoTrack Remote video track receiving data from the remote peer. This is null until Transceiver is set to a non-null value and a remote track is added to that transceiver. Declaration public RemoteVideoTrack VideoTrack { get; } Property Value Type Description RemoteVideoTrack Methods OnPaired(MediaTrack) Internal callback invoked when the receiver is paired with a media track. Declaration protected override void OnPaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver is paired with. Overrides MediaReceiver.OnPaired(MediaTrack) Remarks This will be called on the Unity update thread. OnUnpaired(MediaTrack) Internal callback invoked when the receiver is unpaired from a media track. Declaration protected override void OnUnpaired(MediaTrack track) Parameters Type Name Description MediaTrack track The media track this receiver was paired with. Overrides MediaReceiver.OnUnpaired(MediaTrack) Remarks This will be called on the Unity update thread."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoRenderer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoRenderer.html",
    "title": "Class VideoRenderer | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoRenderer Utility component used to play video frames obtained from a WebRTC video track. This can indiscriminately play video frames from a video track source on the local peer as well as video frames from a remote video receiver obtaining its frame from a remote WebRTC peer. Inheritance Object VideoRenderer Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class VideoRenderer : MonoBehaviour Remarks This component writes to the attached Material , via the attached Renderer . Fields EnableStatistics Declaration public bool EnableStatistics Field Value Type Description Boolean FrameLoadStatHolder A textmesh onto which frame load stat data will be written Declaration public TextMesh FrameLoadStatHolder Field Value Type Description TextMesh Remarks This is how fast the frames are given from the underlying implementation FramePresentStatHolder A textmesh onto which frame present stat data will be written Declaration public TextMesh FramePresentStatHolder Field Value Type Description TextMesh Remarks This is how fast we render frames to the display FrameSkipStatHolder A textmesh into which frame skip stat dta will be written Declaration public TextMesh FrameSkipStatHolder Field Value Type Description TextMesh Remarks This is how often we skip presenting an underlying frame MaxFramerate Declaration public float MaxFramerate Field Value Type Description Single Methods Argb32VideoFrameReady(Argb32VideoFrame) Declaration protected void Argb32VideoFrameReady(Argb32VideoFrame frame) Parameters Type Name Description Argb32VideoFrame frame I420AVideoFrameReady(I420AVideoFrame) Declaration protected void I420AVideoFrameReady(I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame OnDisable() Declaration protected void OnDisable() StartRendering(IVideoSource) Start rendering the passed source. Declaration public void StartRendering(IVideoSource source) Parameters Type Name Description IVideoSource source Remarks Can be used to handle VideoStreamStarted or VideoStreamStarted . StopRendering(IVideoSource) Stop rendering the passed source. Must be called with the same source passed to StartRendering(IVideoSource) Declaration public void StopRendering(IVideoSource _) Parameters Type Name Description IVideoSource _ Remarks Can be used to handle VideoStreamStopped or VideoStreamStopped ."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html",
    "title": "Class VideoStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStartedEvent Unity event corresponding to a new video stream being started. Inheritance Object VideoStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class VideoStreamStartedEvent : UnityEvent<IVideoSource>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html",
    "title": "Class VideoStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. Inheritance Object VideoStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class VideoStreamStoppedEvent : UnityEvent<IVideoSource>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoTrackSource.html",
    "title": "Class VideoTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoTrackSource This component represents a video track source, an entity which produces raw video frames for one or more tracks. The source can be added on a peer connection media line to be sent through that peer connection. It is a standalone object, independent of any peer connection, and can be shared with multiple of them. Inheritance Object MediaTrackSource VideoTrackSource CustomVideoSource <T> WebcamSource Inherited Members MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class VideoTrackSource : MediaTrackSource Fields VideoStreamStarted Event raised when the video stream started. When this event is raised, the followings are true: The property is a valid local video track. The will become true just after the event is raised, by design. Declaration public VideoStreamStartedEvent VideoStreamStarted Field Value Type Description VideoStreamStartedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. VideoStreamStopped Event raised when the video stream stopped. When this event is raised, the followings are true: The property is null . The has just become false right before the event was raised, by design. Declaration public VideoStreamStoppedEvent VideoStreamStopped Field Value Type Description VideoStreamStoppedEvent Remarks This event is raised from the main Unity thread to allow Unity object access. Properties IsLive Indicates if the source is currently producing frames. Declaration public override bool IsLive { get; } Property Value Type Description Boolean Overrides MediaTrackSource.IsLive MediaKind Media kind of the track source. Declaration public override MediaKind MediaKind { get; } Property Value Type Description MediaKind Overrides MediaTrackSource.MediaKind Source Video track source object from the underlying C# library that this component encapsulates. The object is owned by this component, which will create it and dispose of it automatically. Declaration public VideoTrackSource Source { get; } Property Value Type Description VideoTrackSource Methods AttachSource(VideoTrackSource) Declaration protected void AttachSource(VideoTrackSource source) Parameters Type Name Description VideoTrackSource source DisposeSource() Declaration protected void DisposeSource() See Also WebcamSource CustomVideoSource <T> SceneVideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.WebcamSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.WebcamSource.html",
    "title": "Class WebcamSource | MixedReality-WebRTC Documentation",
    "keywords": "Class WebcamSource This component represents a local video sender generating video frames from a local video capture device (webcam). Inheritance Object MediaTrackSource VideoTrackSource WebcamSource Inherited Members VideoTrackSource.Source VideoTrackSource.VideoStreamStarted VideoTrackSource.VideoStreamStopped VideoTrackSource.IsLive VideoTrackSource.MediaKind VideoTrackSource.AttachSource(VideoTrackSource) VideoTrackSource.DisposeSource() MediaTrackSource.MediaLines MediaTrackSource.AttachToMediaLines() MediaTrackSource.DetachFromMediaLines() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class WebcamSource : VideoTrackSource Fields Constraints For manual FormatMode , optional constraints on the resolution and framerate of the capture format. These constraints are additive, meaning a matching format must satisfy all of them at once, in addition of being restricted to the formats supported by the selected video profile or kind of profile. Any negative or zero value means no constraint. Declaration public VideoCaptureConstraints Constraints Field Value Type Description VideoCaptureConstraints Remarks Video capture formats for HoloLens 1 and HoloLens 2 are available here: https://docs.microsoft.com/en-us/windows/mixed-reality/locatable-camera EnableMixedRealityCapture Enable Mixed Reality Capture (MRC) if available on the local device. This option has no effect on devices not supporting MRC, and is silently ignored. Declaration public bool EnableMixedRealityCapture Field Value Type Description Boolean EnableMRCRecordingIndicator Enable the on-screen recording indicator when Mixed Reality Capture (MRC) is available and enabled. This option has no effect on devices not supporting MRC, or if MRC is not enabled. Declaration public bool EnableMRCRecordingIndicator Field Value Type Description Boolean FormatMode Selection mode for the video capture format. Declaration public LocalVideoSourceFormatMode FormatMode Field Value Type Description LocalVideoSourceFormatMode VideoProfileId For manual FormatMode , unique identifier of the video profile to use, or an empty string to leave unconstrained. Declaration public string VideoProfileId Field Value Type Description String VideoProfileKind For manual FormatMode , kind of video profile to use among a list of predefined ones, or an empty string to leave unconstrained. Declaration public VideoProfileKind VideoProfileKind Field Value Type Description VideoProfileKind WebcamDevice Optional identifier of the webcam to use. Setting this value forces using the given webcam, and will fail opening any other webcam. Valid values are obtained by calling GetVideoCaptureDevicesAsync() . Declaration public VideoCaptureDevice WebcamDevice Field Value Type Description VideoCaptureDevice Remarks This property is purposely not shown in the Unity inspector window, as there is very little reason to hard-code a value for it, which would only work on a specific device with a given immutable hardware. It is still serialized on the off-chance that there is a valid use case for hard-coding it. See Also GetVideoCaptureDevicesAsync() Methods OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable() Update() Declaration protected void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html",
    "title": "Class WebRTCErrorEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. Inheritance Object WebRTCErrorEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class WebRTCErrorEvent : UnityEvent<string>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.WorkQueue.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.WorkQueue.html",
    "title": "Class WorkQueue | MixedReality-WebRTC Documentation",
    "keywords": "Class WorkQueue Base class providing some utility work queue to dispatch free-threaded actions to the main Unity application thread, where the handler(s) can safely access Unity objects. Inheritance Object WorkQueue LocalOnlySignaler PeerConnection Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class WorkQueue : MonoBehaviour Properties IsMainAppThread Check if the current thread is the main Unity application thread where it is safe to access Unity objects. Declaration public bool IsMainAppThread { get; } Property Value Type Description Boolean Remarks Should be only called once the object is awake. Methods Awake() Declaration protected virtual void Awake() EnsureIsMainAppThread() Ensure the current method is running on the main Unity application thread. Declaration public void EnsureIsMainAppThread() Remarks Should be only called once the object is awake. InvokeOnAppThread(Action) Invoke the specified action on the main Unity app thread. Declaration public void InvokeOnAppThread(Action action) Parameters Type Name Description Action action The action to execute. Remarks If this object is awake, and this method is called from the main Unity app thread, action will be executed synchronously. Otherwise, action will be called during the next call to this object's Update() . Update() Implementation of MonoBehaviour.Update to execute from the main Unity app thread any background work enqueued from free-threaded callbacks. Declaration protected virtual void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoCaptureDevice.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoCaptureDevice.html",
    "title": "Struct VideoCaptureDevice | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureDevice Identifier for a video capture device. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoCaptureDevice Fields | Improve this Doc View Source id Unique device identifier. Declaration public string id Field Value Type Description String | Improve this Doc View Source name Friendly device name. Declaration public string name Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoCaptureFormat.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoCaptureFormat.html",
    "title": "Struct VideoCaptureFormat | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureFormat Capture format for a video track. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoCaptureFormat Fields | Improve this Doc View Source fourcc FOURCC identifier of the video encoding. Declaration public uint fourcc Field Value Type Description UInt32 | Improve this Doc View Source framerate Capture framerate, in frames per second. Declaration public double framerate Field Value Type Description Double | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoEncoding.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoEncoding.html",
    "title": "Enum VideoEncoding | MixedReality-WebRTC Documentation",
    "keywords": "Enum VideoEncoding Enumeration of video encodings. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum VideoEncoding : int Fields Name Description Argb32 32-bit ARGB32 video encoding with 8-bit per component, encoded as uint32 little-endian 0xAARRGGBB value, or equivalently (B,G,R,A) in byte order. I420A I420A video encoding with chroma (UV) halved in both directions (4:2:0), and optional Alpha plane."
  },
  "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html",
    "title": "Class VideoFrameQueue<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. Inheritance Object VideoFrameQueue<T> Implements IVideoFrameQueue Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class VideoFrameQueue<T> : object, IVideoFrameQueue where T : class, IVideoFrameStorage, new() Type Parameters Name Description T The type of video frame storage Constructors | Improve this Doc View Source VideoFrameQueue(Int32) Create a new queue with a maximum frame length. Declaration public VideoFrameQueue(int maxQueueLength) Parameters Type Name Description Int32 maxQueueLength Maxmimum number of frames to enqueue before starting to drop incoming frames Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration public float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration public float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration public float QueuedFramesPerSecond { get; } Property Value Type Description Single Methods | Improve this Doc View Source Clear() Clear the queue and drop all frames currently pending. Declaration public void Clear() | Improve this Doc View Source Enqueue(Argb32VideoFrame) Try to enqueue a new video frame encoded in raw ARGB format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(Argb32VideoFrame frame) Parameters Type Name Description Argb32VideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source Enqueue(I420AVideoFrame) Enqueue a new video frame encoded in I420+Alpha format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source RecycleStorage(T) Recycle a frame storage, putting it back into the internal pool for later reuse. This prevents deallocation and reallocation of a frame, and decreases pressure on the garbage collector. Declaration public void RecycleStorage(T frame) Parameters Type Name Description T frame The unused frame storage to recycle for a later new frame | Improve this Doc View Source TrackLateFrame() Track statistics for a late frame, which short-circuits the queue and is delivered as soon as it is received. Declaration public void TrackLateFrame() | Improve this Doc View Source TryDequeue(out T) Try to dequeue a video frame, usually to be consumed by a video sink (video player). Declaration public bool TryDequeue(out T frame) Parameters Type Name Description T frame On success, returns the dequeued frame. Returns Type Description Boolean Return true on success or false if the queue is empty. Implements IVideoFrameQueue"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoProfile.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoProfile.html",
    "title": "Class VideoProfile | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoProfile Video profile. Inheritance Object VideoProfile Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class VideoProfile : object Fields | Improve this Doc View Source uniqueId Unique identifier of the video profile. Declaration public string uniqueId Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoProfileKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoProfileKind.html",
    "title": "Enum VideoProfileKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum VideoProfileKind Kind of video profile. This corresponds to the enum of the API. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum VideoProfileKind : int Fields Name Description BalancedVideoAndPhoto Balanced video profile to capture both videos and photos. HdrWithWcgPhoto Video profile for capturing photos with High Dynamic Range (HDR) and Wide Color Gamut (WCG). HdrWithWcgVideo Video profile for capturing videos with High Dynamic Range (HDR) and Wide Color Gamut (WCG). HighFrameRate Video profile containing high framerate capture formats. HighQualityPhoto Video profile for high quality photo capture. PhotoSequence Video profile for capturing a sequence of photos. Unspecified Unspecified video profile kind. Used to remove any constraint on the video profile kind. VariablePhotoSequence Video profile for capturing a variable sequence of photos. VideoConferencing Video profile for video conferencing, often of lower power consumption and lower latency by deprioritizing higher resolutions. This is the recommended profile for most WebRTC applications, if supported. VideoHdr8 Video profile for capturing videos with High Dynamic Range (HDR). VideoRecording Video profile for video recording, often of higher quality and framerate at the expense of power consumption and latency. See Also https://docs.microsoft.com/en-us/uwp/api/windows.media.capture.knownvideoprofile"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoTrackSource.html",
    "title": "Class VideoTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoTrackSource Video source for WebRTC video tracks. The video source is not bound to any peer connection, and can therefore be shared by multiple video tracks from different peer connections. This is especially useful to share local video capture devices (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology (one connection per pair of peers). The user owns the video track source, and is in charge of keeping it alive until after all tracks using it are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still using it is undefined. The Tracks property contains the list of tracks currently using the source. Inheritance Object VideoTrackSource DeviceVideoTrackSource ExternalVideoTrackSource Implements IVideoSource IDisposable Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public abstract class VideoTrackSource : object, IVideoSource Properties | Improve this Doc View Source Enabled Enabled status of the source. True until the object is disposed. Declaration public bool Enabled { get; } Property Value Type Description Boolean | Improve this Doc View Source FrameEncoding Video encoding indicating the kind of frames the source is producing. Declaration public abstract VideoEncoding FrameEncoding { get; } Property Value Type Description VideoEncoding | Improve this Doc View Source Name A name for the video track source, used for logging and debugging. Declaration public string Name { get; set; } Property Value Type Description String | Improve this Doc View Source Tracks List of local video tracks this source is providing raw video frames to. Declaration public IReadOnlyList<LocalVideoTrack> Tracks { get; } Property Value Type Description IReadOnlyList < LocalVideoTrack > Methods | Improve this Doc View Source Dispose() Declaration public virtual void Dispose() | Improve this Doc View Source ToString() Declaration public override string ToString() Returns Type Description String Events | Improve this Doc View Source Argb32VideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event Argb32VideoFrameDelegate Argb32VideoFrameReady Event Type Type Description Argb32VideoFrameDelegate Remarks The event delivers to the handlers an ARGB32-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. | Improve this Doc View Source I420AVideoFrameReady Event that occurs when a new video frame is available from the source, either because the source produced it locally ( VideoTrackSource , LocalVideoTrack ) or because it received it from the remote peer ( RemoteVideoTrack ). Declaration public event I420AVideoFrameDelegate I420AVideoFrameReady Event Type Type Description I420AVideoFrameDelegate Remarks The event delivers to the handlers an I420-encoded video frame. This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely while the event is invoked, but access to any resource used by its handlers must be synchronized manually. Note that a handler might be invoked (at most once) after it has been removed from the event. Implements IVideoSource IDisposable See Also DeviceVideoTrackSource ExternalVideoTrackSource LocalVideoTrack"
  },
  "index.html": {
    "href": "index.html",
    "title": "Index | MixedReality-WebRTC Documentation",
    "keywords": "MixedReality-WebRTC documentation (latest) This is the MixedReality-WebRTC documentation for the master branch , which contains the latest features and API changes. In general this latest API is incompatible with the NuGet packages . For the documentation corresponding to other branches, including the release/1.0 branch from which the NuGet 1.x packages are built, use the drop-down selection at the top right of this page. User Manual Introduction Getting started Download Installation Migration Guide Building from sources C# tutorial (Desktop) C# tutorial (UWP) Unity tutorial Glossary C# library Feature Overview Tutorial (Desktop) Tutorial (UWP) Peer Connection Signaling Unity integration Feature Overview Tutorial Peer Connection Signaler Audio MicrophoneSource AudioReceiver Video WebcamSource VideoReceiver VideoRenderer Advanced topics Building the Core dependencies from sources API reference C# Library PeerConnection The PeerConnection object is the API entry point to establish a remote connection. Transceiver The transceiver is the \"pipe\" which transports some audio or video between the two peers. DataChannel Encapsulates a single data channel for transmitting raw blobs of bytes. Unity Library PeerConnection The PeerConnection component builds on the same-named library class to expose a remote peer connection. WebcamSource The WebcamSource component provides access to the local webcam for local rendering and remote streaming. MicrophoneSource The MicrophoneSource component provides access to the local microphone for audio streaming to the remote peer."
  },
  "manual/android/building-android.html": {
    "href": "manual/android/building-android.html",
    "title": "Building from sources (Android) | MixedReality-WebRTC Documentation",
    "keywords": "Building from sources (Android) The MixedReality-WebRTC library for Android takes the form of an Android archive ( .aar ). This archive is built using the scripts located in tools/build/libwebrtc and tools/build/android , in that order. TODO - More detailed steps. See various README.md in those folders in the meantime."
  },
  "manual/building.html": {
    "href": "manual/building.html",
    "title": "Building from sources | MixedReality-WebRTC Documentation",
    "keywords": "Building from sources Important Building MixedReality-WebRTC from sources, and in particular the native libwebrtc implementation, is fairly involved, and is not required to use the various MixedReality-WebRTC libraries. Prebuilt binaries are available for C/C++ and C# project (NuGet packages) and Unity (UPM packages), which provide a much easier way to obtain and consume those libraries. See Installation for details. Building the C/C++ and C# libraries of MixedReality-WebRTC from sources involves two main steps: Building the native C/C++ library for the specific platform(s) of interest: Building for Windows platforms (Desktop and UWP) Building for Android (Unity) Building the platform-independent C# library . On Windows, if you want to start right away with MixedReality-WebRTC, the recommended approach is to consume the precompiled binaries distributed as NuGet packages instead. See Download for details. There is currently no prebuilt Android binaries."
  },
  "manual/building-core.html": {
    "href": "manual/building-core.html",
    "title": "Building the Core dependencies from sources | MixedReality-WebRTC Documentation",
    "keywords": "Building the Core dependencies from sources This document describes how to build the entire project from sources, including the so-called Core dependencies: The low-level WebRTC C++ implementation webrtc.lib from Google. The UWP WinRT wrapper Org.WebRtc.winmd from the Microsoft WebRTC UWP team. The dependencies require some heavy setup and take time to compile, therefore it is strongly recommended to use the prebuilt binaries shipped as NuGet packages instead of trying to build those from source. Windows Desktop These packages contain webrtc.lib built for Windows Desktop ( Win32 ) for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x64.Release Windows UWP These packages contain webrtc.lib built for Windows UWP for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.UWP.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.UWP.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.x64.Release Microsoft.MixedReality.WebRTC.Native.Core.UWP.ARM.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.ARM.Release In addition, the Microsoft.MixedReality.WebRTC.Native.Core.UWP package (which should have been named .WinRT for consistency) references all the architecture-dependent ones for UWP for convenience, and also contains the platform-independent generated C++/WinRT headers necessary to consume the libraries. WinRT binding These packages contain the WinRT binding ( Org.WebRtc.dll , Org.WebRtc.winmd , Org.WebRtc.WrapperGlue.lib ) built for Windows UWP for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x64.Release Microsoft.MixedReality.WebRTC.Native.Core.WinRT.ARM.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.ARM.Release In general most users will want to follow the steps in the readme instead of the ones below if there is no need to modify the input dependencies. Prerequisites General Python 2.7 must be installed as the default interpreter. That is, python --version must return a Python version equal to 2.7. It is strongly recommended to get a patch version >= 15, that is Python 2.7.15+ , as some users reported to the WebRTC UWP team some spurious failures with earlier versions. Python 3.x does not work and should not be the default interpreter. A recent version of Perl is needed for some builds. On Windows you can install for example Strawberry Perl , or any other equivalent distribution you want. Core WebRTC Core WebRTC refers to the C++ implementation of WebRTC maintained by Google and used by this project, whose source repository is https://webrtc.googlesource.com/src . Visual Studio 2017 is required to compile the core WebRTC implementation from Google. Having the MSVC v141 toolchain installed inside another version of Visual Studio is unfortunately not enough (see this issue ), the actual IDE needs to be installed for the detection script to work. Selecting the C++ Workload alone is enough. If compiling for ARM or ARM64 architecture though, check the Visual C++ compilers and libraries for ARM(64) optional individual component. The Windows SDK 10.0.17134 (also called 1803, or April 2018) is required to compile the Google WebRTC core implementation ( archive download ). As mentioned on the README of WebRTC UWP, the Debugging Tools for Windows are required: When installing the SDK, include the feature Debugging Tools for Windows which is required by the preparation scripts. Note that the SDK installed as part of Visual Studio does not include this feature. If the SDK is already installed, this optional feature can be added with Add or Remove Programs > Windows Software Development Kit - Windows 10.0.x > Modify > Select Change then Next button > Check Debugging Tools for Windows . Core WebRTC UWP wrappers The UWP wrappers refer to the set of wrappers and other UWP-specific additional code made available by the WebRTC UWP team (Microsoft) on top of the core implementation, to allow access to the core WebRTC API. The Windows SDK 10.0.17763 (also called 1809, or October 2018) is required to compile the UWP wrappers provided by the WebRTC UWP team ( archive download ), with the Debugging Tools for Windows as above. The UWP wrappers also require the v141 platform toolset for UWP, either from the Universal Windows Platform development workload in Visual Studio 2017, or from the optional component C++ (v141) Universal Windows Platform tools in Visual Studio 2019 . The UWP wrappers use C++/WinRT, so the C++/WinRT Visual Studio extension must be installed from the marketplace. MixedReality-WebRTC C++ library The MSVC v142 - VS 2019 C++ x64/x86 build tools toolchain is required to build the C++17 library of MixedReality-WebRTC. This is installed by default with the Desktop development with C++ workload on Visual Studio 2019. Note - Currently due to CI limitations some projects are downgraded to VS 2017, but will be reverted to VS 2019 eventually (see #14). Unity integration The Unity integration has been tested on Unity 2018.3.x and 2019.1.x . Versions earlier than 2018.3.x may work but are not officially supported. Build steps Check out the repository and its dependencies git clone --recursive https://github.com/microsoft/MixedReality-WebRTC.git Note that this may take some time (> 5 minutes) due to the large number of submodules in the WebRTC UWP SDK repository this repository depends on. Build the WebRTC UWP SDK libraries Using the build script In order to simplify building, a PowerShell build script is available. The prerequisites still need to be installed manually before running it. To use the script, simply run for example: cd tools/build/ build.ps1 -BuildConfig Debug -BuildArch x64 -BuildPlatform Win32 Note - Currently the build script assumes it runs from tools/build/ only. It will fail if invoked from another directory. Valid parameter values are: BuildConfig : Debug | Release BuildArch : x86 | x64 | ARM | ARM64 BuildPlatform : Win32 | UWP Note - ARM and ARM64 are only valid for the UWP platform. Note - ARM64 is not yet available (see #13 ). The manual steps are details below and can be skipped if running the build script. Manually The WebRTC UWP project has specific requirements . In particular it needs Python 2.7.15+ installed as default , that is calling python from a shell without specifying a path launches that Python 2.7.15+ version. Note - Currently the Azure hosted agents with VS 2017 have Python 2.7.14 installed, but this is discouraged by the WebRTC UWP team as some spurious build errors might occur. The new VS 2019 build agents have Python 2.7.16 installed. Note - Currently the libyuv external dependency is incorrectly compiled with Clang instead of MSVC on ARM builds. This was an attempt to benefit from inline assembly, but this produces link errors (see this issue ). Until this is fixed, a patch is available under tools\\patches\\libyuv_win_msvc_157.patch which is applied by build.ps1 but needs to be applied manually if build.ps1 is not used. More generally all patches under tools\\patches need to be manually applied . For Windows 10 Desktop support (also called \"Win32\"): Open the WebRtc.Win32.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For the Unity editor, the x64 binaries are required. Build the WebRtc.Win32.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build . The other projects are samples and are not needed. For UWP support: Open the WebRtc.Universal.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For HoloLens, the x86 binaries are required. For HoloLens 2, the ARM binaries are required (ARM64 is not supported yet, see #13 ). Build first the WebRtc.UWP.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build Next build the Org.WebRtc and Org.WebRtc.WrapperGlue projects. The other projects samples and are not needed. Build the MixedReality-WebRTC libraries Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the repository. Build the solution with F7 or Build > Build Solution On successful build, the binaries will be generated in a sub-directory under bin/ , and the relevant DLLs will be copied by a post-build script to libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Plugins\\ for Unity to consume them. The Microsoft.MixedReality.WebRTC.sln Visual Studio solution contains several projects: The native C++ library, which can be compiled: for Windows Desktop with the Microsoft.MixedReality.WebRTC.Native.Win32 project for UWP with the Microsoft.MixedReality.WebRTC.Native.UWP project The C# library project Microsoft.MixedReality.WebRTC A C# unit tests project Microsoft.MixedReality.WebRTC.Tests A UWP C# sample app project Microsoft.MixedReality.WebRTC.TestAppUWP based on WPF and XAML Optionally test the installation Test the install by e.g. opening the Unity project at libs\\Microsoft.MixedReality.WebRTC.Unity , loading the Assets\\Microsoft.MixedReality.WebRTC.Unity.Examples\\VideoChatDemo scene and pressing Play . After a few seconds (depending on the machine) the left media player should display the video feed from the local webcam. The Unity console should also display a message about the WebRTC library being initialized successfully. See the Hello, Unity World! tutorial for more details."
  },
  "manual/building-cslib.html": {
    "href": "manual/building-cslib.html",
    "title": "Building the C# library | MixedReality-WebRTC Documentation",
    "keywords": "Building the C# library The MixedReality-WebRTC C# library is a platform-independent .NET Standard 2.0 library which relies on a platform-specific version of the C library to provide its WebRTC implementation. The C# library is built on Windows from the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the git repository. Building from a non-Windows environment is not supported. This documentation assumes that the user has already built the native C/C++ library on Windows, and therefore has installed its prerequisites and already cloned the git repository of MixedReality-WebRTC. If you only built the Android archive mrwebrtc.aar on a different Linux machine then you should install those prerequisites first on the current Windows machine. Note If you have already built the entire solution for the native C/C++ library on Windows, then the C# library is already built. The steps below are mainly for Android-only users, although it is strongly recommended to build the Windows Desktop x64 configuration anyway to be able to run MixedReality-WebRTC in Play Mode in the Unity editor. Building the library Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the cloned git repository. Select a build configuration (Debug or Release) with the Solution Configuration drop-down widget under the menu bar. The architecture selection is irrelevant here; it is only relevant when building the native C library. Build the Microsoft.MixedReality.WebRTC C# project by right-clicking on it and selecting the Build menu entry. On successful build, the C# assembly will be generated in a sub-folder of bin/netstandard2.0 , and the DLL will also be copied by a post-build script to libs\\unity\\library\\Runtime\\Plugins\\ for the Unity library to consume it. Important Be sure to build the solution before opening any Unity library project. As part of the build, the library is copied to the Plugins folder of the Unity library. There are already some associated .meta files, which have been committed to the git repository, to inform Unity of the platform of each DLL and how to deploy it. If the Unity project is opened first, before the DLLs are present, Unity will assume those .meta files are stale and will delete them, and then later will recreate some with a different default config once the DLLs are copied. This leads to errors about modules with duplicate names. See the Building from Sources (Windows) page for more details. Testing the build Test the newly built C# library by e.g. using the TestAppUWP sample application: Build the Microsoft.MixedReality.WebRTC.TestAppUWP C# project from the same Visual Studio solution. Run it by right-clicking on the project and selecting Debug > Start New Instance (or F5 if the project is configured as the Startup Project). See the Hello, C# World! (UWP) tutorial for more details. Installing into an existing C# project The C# library requires the C library, which contains the core WebRTC implementation. The setup is summarized in the following table: Source DLLs How to add bin\\netstandard2.0\\Release\\Microsoft.MixedReality.WebRTC.dll Include in \"References\" of your VS project bin\\<platform>\\<arch>\\Release\\mrwebrtc.dll Add as \"Content\" to the project, so that the Deploy step copies the DLL to the AppX folder alongside the application executable. See the TestAppUWP project for an example, noting how it uses the $(Platform) and $(Configuration) Visual Studio variables to automatically copy the right DLL corresponding to the currently selected project configuration. where: <platform> is either Win32 for a Desktop app, or UWP for a UWP app. <arch> is one of [ x86 , x64 , ARM ]. Note that ARM is only available on UWP."
  },
  "manual/building-windows.html": {
    "href": "manual/building-windows.html",
    "title": "Building from sources (Windows) | MixedReality-WebRTC Documentation",
    "keywords": "Building from sources (Windows) The MixedReality-WebRTC libraries for Windows platforms (Desktop and UWP) are built from the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the git repository. Prerequisites Environment and tooling The low-level WebRTC implementation from Google ( webrtc.lib ) requires in total approximately 20 GB of disk space when built for all supported Windows variants. Due to Windows paths length limit, it is recommended to clone the source repository close to the root of the filesystem, e.g. C:\\mr-webrtc\\ or similar, as the recursive external dependencies create a deep hierarchy which may otherwise produce paths beyond the OS limit and result in build failure. The solution uses Visual Studio 2019 with the following features: The MSVC v141 - VS 2017 C++ x64/x86 build tools toolchain from Visual Studio 2017 is required to build the C++17 library of MixedReality-WebRTC. This will eventually be replaced with the Visual Studio 2019 compiler (v142 toolchain) once the project is upgraded to a Google milestone supporting Visual Studio 2019 (see details on issue #14 ) For ARM support, the MSVC v141 - VS 2017 C++ ARM build tools toolchain is also required. The C# library requires a .NET Standard 2.0 compiler, like the Roslyn compiler available as part of Visual Studio when installing the .NET desktop development workload. The UWP libraries and projects require UWP support from the compiler, available as part of Visual Studio when installing the Universal Windows Platform development workload. The Unity library is officially supported for Unity version 2018.4.x (LTS) and 2019.4.x (LTS). However, we do our best to keep things working on other versions too where possible. Versions earlier than 2018.4.x may work but are not tested at all, and no support will be provided for those. Cloning the repository The official repository containing the source code of MixedReality-WebRTC is hosted on GitHub . The latest developments are done on the master branch, while the latest stable release is a release/* branch. Clone the choosen branch of the repository and its dependencies recursively , preferably close to the root of the filesystem (see prerequisites): git clone --recursive https://github.com/microsoft/MixedReality-WebRTC.git -b <branch_name> C:\\mr-webrtc Note that this may take some time (> 5 minutes) due to the large number of submodules in the WebRTC UWP SDK repository this repository depends on. Building the core WebRTC implementation The so-called Core implementation is constituted of: webrtc.lib : A static library containing the Google implementation of the WebRTC standard. Org.WebRtc.winmd : A set of WinRT wrappers for accessing the WebRTC API from UWP. Those libraries require several extra prerequisites. They are also complex and time-consuming to build. Therefore to save time and avoid headache a build script is provided in tools\\build\\build.ps1 which checks for most prerequisites and build a single variant of webrtc.lib . cd tools\\build .\\build.ps1 -BuildConfig Release -BuildArch x64 -BuildPlatform Win32 # Desktop x64, e.g. Unity Editor .\\build.ps1 -BuildConfig Release -BuildArch ARM -BuildPlatform UWP # UWP ARM, e.g. HoloLens 2 [...] Not all build variants need to be built; you can build only the one(s) necessary for your target device(s). Building the MixedReality-WebRTC libraries Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the freshly cloned repository. Select the build variant with the Solution Configuration selector at the top of the window under the main menu. Choose the build configuration ( Debug or Release ); generally it is recommended to always use the Release configuration, as the Debug configuration of the underlying webrtc.lib implementation used by the native C library can be notably slow in Debug . Choose the build architecture corresponding to the variant needed: x86 for the 32-bit Windows Desktop and UWP variants x64 for the 64-bit Windows Desktop and UWP variants; this includes the x64 Desktop variant required by the Unity editor ARM for the 32-bit Windows UWP ARM variant Build the solution with Build > Build Solution . This builds only 2 build variants (Desktop and UWP) for the selected architecture and build configuration. If you need other variants, repeat from step 2. and select them. On successful build, the binaries will be generated in a sub-directory under bin/ , and the relevant DLLs will be copied by a post-build script to libs\\unity\\library\\Runtime\\Plugins\\ for the Unity library to consume them. Copying the binaries Note: This section describes manual copying in case the Visual Studio solution is not used or the automated copying does not work for any reason. If the solution built successfully, the binaries should already have been copied (see above). The C# library Microsoft.MixedReality.WebRTC.dll is a .NET Standard 2.0 library. This means it is compatible with all CPU architectures. The C# library is available from bin\\netstandard2.0\\Debug or bin\\netstandard2.0\\Release depending on the build configuration which was compiled. In doubt you should use the Release configuration, which can provide better performance. This module needs to be copied somehwere into the libs\\unity\\library\\Runtime\\Plugins\\ folder of the git repository checkout. This is done automatically by the build process of the Visual Studio solution, but can also be done via the command line with xcopy , assuming that the git repository of MixedReality-WebRTC was cloned in D:\\mr-webrtc : cd /D D:\\testproj xcopy D:/mr-webrtc/bin/netstandard2.0/Release/Microsoft.MixedReality.WebRTC.dll libs/unity/library/Runtime/Plugins/Win32/x86_64/ Note : By convention, and because of past experiences of getting errors when doing otherwise, the C# library is copied to the Win32\\x86_64 sub-folder where the native C library for the Unity editor is also located. It is unclear if this is best practice, as the Unity documentation is outdated on this topic. For the native C library mrwebrtc.dll things are a bit more complex. The DLL is compiled for a particular platform and architecture, in addition of the Debug or Release build configurations, and the correct variant needs to be used. On Windows, the Unity Editor needs a 64-bit Desktop variant ( Debug or Release ); it is available from the bin\\Win32\\x64\\Release folder ( Win32 is an alias for Desktop ), and should be copied to the libs\\unity\\library\\Runtime\\Plugins\\Win32\\x86_64\\ folder. Again this is done automatically by the Visual Studio build process, but can be done manually with the following commands: cd /D D:\\testproj xcopy D:/mr-webrtc/bin/Win32/x64/Release/mrwebrtc.dll libs/unity/library/Runtime/Plugins/Win32/x86_64/ Importing the library Once the necessary binaries have been added to libs\\unity\\library\\Runtime\\Plugins , the Unity library UPM package can be imported into any number of Unity projects. Warning It is critical to setup the binaries first, before importing into a project and opening the Unity editor, otherwise the import settings might get overwritten by Unity. See Configuring the import settings below for details. The Unity library already has the proper layout on disk of a UPM package, and a package.json file is provided. This means the package can be imported simply by following the official Unity instructions for installing a local package . Configuring the import settings Note: This step is normally not necessary, as the .meta files are already checked in the git repository and properly configured. It can happen that Unity deletes and recreates those files, therefore the configuration process is explained in details here anyway. Important Be sure to build the binaries before importing the Unity library package into any Unity project. As part of the build, the libraries are copied to the Plugins folder of the Unity library. There are already some associated .meta files, which have been committed to the git repository, to inform Unity of the platform of each DLL and how to deploy it. If the Unity project is opened first, before the DLLs are present, Unity will assume those .meta files are stale and will delete them, and then later will recreate some with a different default config once the DLLs are copied. This leads to errors about modules with duplicate names. If done correctly then none of the below is necessary. The git repository comes with pre-configured .meta files already setup. However we detail below the manual steps to configure those .meta files for the sake of clarity, and in case Unity deleted those files because the binaries have not been built yet (although using git reset is a faster and safer way to recover them), or if users want to get rid of the UPM package and incorporate a custom copy of the MixedReality-WebRTC into their Unity project and modify it. When building a Unity application for a given platform, another variant of the MixedReality-WebRTC native C/C++ library than the one the Unity editor uses may be required to deploy on that platform. In order for the C# library to be truly platform-independent, the name of all native C/C++ library variants is the same: mrwebrtc . This allows the C# code to reference that DLL with the same DllImport attribute path . But this also means that multiple files with the same name exist in different folders, and Unity needs to know which one is associated with which build variant, to be able to deploy the correct one only. This is done by configuring the platform associated with a DLL in the import settings in the Unity inspector. When importing a UPM package into a Unity project, the content of the UPM package can be accessed from the Packages group of the Project window. To configure the import settings: In the Project window, select one of the mrwebrtc.dll files from the Plugins folder. The Inspector window now shows the import settings for the selected file, which contains option to configure the deploy platform(s) and architecture(s). For example, by selecting in the Inspector window: Include Platforms: WSAPlayer , the DLL will be used by Unity on UWP platforms. WSAPlayer is the name Unity uses for its UWP standalone player. CPU equal to x86_64 , Unity will only deploy that DLL when deploying on a 64-bit Intel architecture. This way, multiple variants of the same-named mrwebrtc.dll can co-exist in different sub-folders of Assets/Plugins/ and Unity will deploy and use the correct variant on each platform. Note If Unity complains about \" Multiple plugins with the same name 'mrwebrtc' \", then the configurations of the various mrwebrtc.dll files are not exclusive, and 2 or more files have been allowed for deploying on the same platform. Check the configuration of all variants again. For Windows Desktop , the native implementation DLL variants are: Path Include Platforms Settings Example use Plugins/Win32/x86 Standalone x86 32-bit Windows Desktop application Plugins/Win32/x86_64 Editor CPU = x86_64, OS = Windows Unity Editor on Windows Standalone x86_64 64-bit Windows Desktop application For Windows UWP , the native implementation DLL variants are: Path Include Platforms SDK CPU Example use Plugins/UWP/x86 WSAPlayer UWP X86 Microsoft HoloLens (1st gen) Plugins/UWP/x86_64 WSAPlayer UWP X64 64-bit UWP Desktop app on Windows Plugins/UWP/ARM WSAPlayer UWP ARM HoloLens 2 (compatibility) Note UWP ARM64 is not supported. The ARM (32-bit) architecture variant can be used as a fallback on HoloLens 2 and other devices which support running 32-bit ARM applications on ARM64 hardware. For Android , the native implementation archive mrwebrc.aar is configured as: Path Include Platforms Example use Plugins/Android/arm64-v8a Android Android phone In all cases the \"Any Platform\" setting must be unchecked. If all variants are installed, the resulting hierarchy should look like this in the Project window: Plugins  Android |  arm64-v8a |  mrwebrtc.aar  Win32 |  x86 | |  mrwebrtc.dll |  x86_64 |  mrwebrtc.dll  UWP  x86 |  mrwebrtc.dll  x86_64 |  mrwebrtc.dll  ARM  mrwebrtc.dll Note that the .meta files where the import settings are saved do not appear in the Project window. Testing the build Test the newly built libraries by e.g. running some of the Windows Desktop tests: Build the mrwebrtc-win32-tests after selecting a Solution Configuration including the x86 or x64 architecture. Run it by right-clicking on the project and selecting Debug > Start New Instance (or F5 if the project is configured as the Startup Project). Alternatively, the test program uses Google Test and integrates with the Visual Studio Test Explorer, so tests can be run from that panel too. Next : Building the C# library"
  },
  "manual/cs/cs.html": {
    "href": "manual/cs/cs.html",
    "title": "C# library overview | MixedReality-WebRTC Documentation",
    "keywords": "C# library overview The C# library Microsoft.MixedReality.WebRTC provides a wrapper over the native C library of MixedReality-WebRTC, offering a more C# oriented API with familiar constructs such as Task and the async / await keywords. There are currently two different tutorials: The .NET Core 3.0 tutorial for the Windows Desktop platform builds a console application which records audio and video. The UWP tutorial for the Windows Universal Platform builds a graphical application with a XAML-based UI."
  },
  "manual/cs/cs-peerconnection.html": {
    "href": "manual/cs/cs-peerconnection.html",
    "title": "C# PeerConnection class | MixedReality-WebRTC Documentation",
    "keywords": "C# PeerConnection class The PeerConnection class is the entry point to using MixedReality-WebRTC. It encapsulates a connection between a local peer on the local physical device, and a remote peer on the same or, more generally, another physical device. Note The Unity integration also has a PeerConnection component, which is based on this class. Initialization A PeerConnection instance can be created with the default constructor, but initially most of its properties, methods, and events, cannot be used until it is initialized with a call to InitializeAsync() . Once intialized, the Initialized property returns true and it is safe to use the peer connection. InitializeAsync() takes a PeerConnectionConfiguration and a CancellationToken . The former allows configuring the peer connection about to be established, while the later allows cancelling that task while it is being processed. PeerConnectionConfiguration contains several fields, but the most important are: IceServers contains an optional collection of STUN and/or TURN servers used by the Interactive Connectivity Establishment (ICE) to establish a connection with the remote peer through routing devices (NATs). Without these, only direct connections to the remote peer can be established, which can be enough if the application knows in advance the network topology surrounding the two peers, but is generally recommended otherwise to increase the chances of establishing a connection over the Internet. SdpSemantic describes the semantic used by the Session Description Protocol (SDP) while trying to establish a connection. This is a compatibility feature, which allows connecting with older peers supporting only the deprecated Plan B semantic. New code should always use the default Unified Plan , which is the only one accepted by the WebRTC 1.0 standard. The other fields are for advanced use, and can be left initially with their default values. Once the connection is initialized, two categories of actions can be performed, in any order: Attempt to establish a connection with the remote peer. Add some media tracks and/or data channels. Media (audio and/or video) tracks and data channels added before the connection is established will be available immediately on connection. Other tracks and channels can be added later during the lifetime of the peer connection, while it remains open. Events A PeerConnection exposes several categories of events: Signaling events are related to establishing a connection to the remote peer. Media and data channel events provide informational notifications about adding and removing transceivers, media tracks, and data channels. In general it is strongly recommended to subscribe to these events before starting to establish a connection to the remote peer. Signaling events Event Status* Description LocalSdpReadytoSend Mandatory Raised when the local peer finished crafting an SDP message, to request the user to send that message to the remote peer via its chosen signaling solution. Failing to handle this event always prevents the peer-to-peer connection from establishing. IceCandidateReadytoSend Mandatory Raised when the local peer finished crafting an ICE candidate message, to request the user to send that message to the remote peer via its chosen signaling solution. Failing to handle this event generally prevents the peer-to-peer connection from establishing. Connected Recommended Raised when the connection with the remote peer is established. This indicates that tracks and channel are ready for use (transports are writable), although ICE can continue its discovery in the background. This does not indicate that the session negotiation is completed yet; this event is raised on the answering peer as soon as it creates an answer and applies it, before it is sent to the remote peer, while it is only raised later on the offering peer when that answer is actually received and applied. RenegotiationNeeded Recommended Raised when the peer connection detected that the current session is obsolete, and needs to be renegotiated. This is generally the result of some media tracks or data channels being added or removed, and should be handled to make those changes available to the remote peer. However, it is perfectly acceptable to ignore some of those events, or delay their processing, if several changes are expected in a short period of time, to avoid triggering multiple unnecessary renegotiations. A renegotiation eventually needs to happen though for the newly added tracks and channel to become open. IceStateChanged Optional Raised when the state of the local ICE connection changed. IceGatheringStateChanged Optional Raised when the state of the ICE candidate gathering process changed. * Status indicates the recommended subscription status for a working peer connection: Mandatory means those events must be handled, otherwise the connection cannot be established. Recommended means those events are typically handled, although not mandatory. Optional means those events are informational only, and it is entirely optional to subscribe to them. Media and data channel events Event Status* Description TransceiverAdded Optional Raised when a transceiver was added to the peer connection, either manually by calling AddTransceiver() , or automatically by the implementation when a remote session description is applied with SetRemoteDescriptionAsync() and the remote peer added a new transceiver. AudioTrackAdded Optional Raised when an audio track was added on the remote peer, and received on the local peer after a session negotiation. This only concerns remotely-created tracks. Tracks whose creation was initied locally with e.g. LocalAudioTrack.CreateFromSource() do not raise the AudioTrackAdded event. AudioTrackRemoved Optional Invoked when an audio track was removed on the remote peer, and a remove message was received on the local peer after a session negotiation. This only concerns remote tracks. Local tracks do not raise the AudioTrackRemoved event. VideoTrackAdded Optional Invoked when a video track was added on the remote peer, and received on the local peer after a session negotiation. This only concerns remotely-created tracks. Tracks whose creation was initied locally with e.g. LocalVideoTrack.CreateFromSource() do not raise the VideoTrackAdded event. VideoTrackRemoved Optional Invoked when a video track was removed on the remote peer, and a remove message was received on the local peer after a session negotiation. This only concerns remote tracks. Local tracks do not raise the VideoTrackRemoved event. DataChannelAdded Optional Invoked when a data channel was added to the local peer connection. This is always fired, irrelevant of how the data channel was initially created (in-band or out-of-band). DataChannelRemoved Optional Invoked when a data channel was removed from the local peer connection. This is always fired, irrelevant of how the data channel was initially created (in-band or out-of-band). * Status - See above."
  },
  "manual/cs/cs-signaling.html": {
    "href": "manual/cs/cs-signaling.html",
    "title": "C# signaling | MixedReality-WebRTC Documentation",
    "keywords": "C# signaling The C# library does not have a dedicated class for signaling. Instead, the PeerConnection class provides some events and methods to build upon in order to build a signaling solution. A custom signaling solution needs to handle sending locally-prepared messages to the remote peer, for example via a separate TCP/IP connection, and dispatching messages received from the remote peer down to the local peer connection. Neither the WebRTC standard nor the MixedReality-WebRTC library specify the way those messages must be transmitted. Local to remote (send) Local messages are generated under several circumstances by the local peer. The signaling solution must listen to the LocalSdpReadytoSend and IceCandidateReadytoSend events, and dispatch those messages to the remote peer by whatever way it choses. peerConnection.LocalSdpReadyToSend += (SdpMessage message) => { MyCustomSignaling_SendSdp(message); }; peerConnection.IceCandidateReadytoSend += (IceCandidate candidate) => { MyCustomSignaling_SendIce(candidate); }; Remote to local (receive) Upon receiving the above messages, the signaling solution must: For SDP messages originating from a LocalSdpReadytoSend event raised on the remote peer and sent to the local peer, call the PeerConnection.SetRemoteDescriptionAsync() method to inform the local peer connection of the newly received session description. public void OnSdpMessageReceived(SdpMessage message) { await peerConnection.SetRemoteDescriptionAsync(message); // Optionally, and only after SetRemoteDescriptionAsync() completed: if (type == \"offer\") { peerConnection.CreateAnswer(); } } For ICE messages originating from an IceCandidateReadytoSend event raised on the remote peer and sent to the local peer, call the PeerConnection.AddIceCandidate() method to inform the local peer connection of the newly received ICE candidate. public void OnIceMessageReceived(IceCandidate candidate) { peerConnection.AddIceCandidate(candidate); }"
  },
  "manual/cs/helloworld-cs-connection-core3.html": {
    "href": "manual/cs/helloworld-cs-connection-core3.html",
    "title": "Establishing a WebRTC connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a WebRTC connection Now that the signaling solution is in place, the final step is to establish a peer connection. Continue editing the Program.cs file and append the following: For debugging purpose, and to understand what is going on with the connection, connect the Connected and IceStateChanged events to handlers printing messages to console. pc.Connected += () => { Console.WriteLine(\"PeerConnection: connected.\"); }; pc.IceStateChanged += (IceConnectionState newState) => { Console.WriteLine($\"ICE state: {newState}\"); }; The Connected event is invoked when the peer connection is established. The IceStateChanged is invoked each time the ICE status changes. Note that the Connected event can be invoked before the ICE status reaches its IceConnectionState.Connected state. In order to verify that the remote video is received, we also subscribe to the I420AVideoFrameReady event. Since this event is invoked frequently, we only print a message every 60 frames. int numFrames = 0; pc.VideoTrackAdded += (RemoteVideoTrack track) => { track.I420AVideoFrameReady += (I420AVideoFrame frame) => { ++numFrames; if (numFrames % 60 == 0) { Console.WriteLine($\"Received video frames: {numFrames}\"); } }; }; To establish a WebRTC connection, one peer (the caller ) has to call CreateOffer() , but not both (the callee just waits). Since the signaler implementation NamedPipeSignaler already provides a way to distinguish between the two peers, we use that information to select which peer will automatically initiate the call. if (signaler.IsClient) { Console.WriteLine(\"Connecting to remote peer...\"); pc.CreateOffer(); } else { Console.WriteLine(\"Waiting for offer from remote peer...\"); } In this state, the application is working but will terminate immediately once the peer connection is established. To prevent that, simply wait until the user press a key, and then close the signaler. Console.WriteLine(\"Press a key to terminate the application...\"); Console.ReadKey(true); signaler.Stop(); Console.WriteLine(\"Program termined.\"); Run the two instances of the application again. This time both terminals print a large quantity of messages related to SDP and ICE message exchanges, and eventually establish a WebRTC peer connection. If launched with the audio or video capture flags, the capturer instance records those media and send them via the network to the other instance, which invokes the remote frame callback and print a message every 60 frames. After that, you can press any key to stop each instance. The signaler and peer connection will close and the program will terminate. Note: Starting from v2.0 and the introduction of explicit transceiver objects and support for multiple tracks, users have to be careful to add transceivers on the offering side alone (they will be created on the answering side automatically), as there is no automated pairing of existing transceivers in WebRTC. Therefore in this tutorial the offering peer, which is the instance started last (client/caller), needs to be the one which will send audio and/or video ( -a / --audio and -v / --video flags). Otherwise the caller will not add any transceiver, and the callee (server; the instance started first) will not be able to send anything. This is only an issue if the callee wants to send some media but the caller doesn't. If they both send, then the caller will add the transceiver and the callee will use it."
  },
  "manual/cs/helloworld-cs-connection-uwp.html": {
    "href": "manual/cs/helloworld-cs-connection-uwp.html",
    "title": "Establishing a WebRTC connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a WebRTC connection Now that the signaling solution is in place, the final step is to establish a peer connection. Continue editing the OnLoaded() method and append after the InitializeAsync() call: For debugging purpose, and to understand what is going on with the connection, connect the Connected and IceStateChanged events to handlers printing messages to the debugger. These messages are visible in the Output window of Visual Studio. _peerConnection.Connected += () => { Debugger.Log(0, \"\", \"PeerConnection: connected.\\n\"); }; _peerConnection.IceStateChanged += (IceConnectionState newState) => { Debugger.Log(0, \"\", $\"ICE state: {newState}\\n\"); }; The Connected event is raised when the peer connection is established, that is when the underlying media transports are writable. There is a subtlety here, in that this does not necessarily correspond to an offer/answer pair exchange being completed; indeed on the callee (answering peer) once the answer is created everything is done and the transports are writable, but the answer has not been sent yet, so the caller (offering peer) needs to wait until it received and applied that answer for its transports to be writable, and therefore its Connected event will be raised much later than the one of the callee (answering peer). The IceStateChanged is raised each time the ICE status changes. Note that the Connected event can be raised before the ICE status reaches its IceConnectionState.Connected state, since the two processes occur in parallel and are partly independent. In order to render the remote video, we also subscribe to the RemoteVideoTrack.I420AVideoFrameReady event. This requires accessing the remote video track, which is only available once the connection is established and the track created during the session negotiation (more precisely: when an offer or answer is applied with SetRemoteDescriptionAsync() . _peerConnection.VideoTrackAdded += (RemoteVideoTrack track) => { _remoteVideoTrack = track; _remoteVideoTrack.I420AVideoFrameReady += RemoteVideo_I420AFrameReady; }; That event handler is similar to the one for the local video, using another video bridge. Create a new set of fields for the remote video: private object _remoteVideoLock = new object(); private bool _remoteVideoPlaying = false; private MediaStreamSource _remoteVideoSource; private VideoBridge _remoteVideoBridge = new VideoBridge(5); This time we increase the video buffer queue to 5 frames to avoid starving the video rendering pipeline as the remote video is more prone to delays due to network latency. This potentially introduces more latency in the overall video streaming process, but optimizing for latency is a complex topic well outside of the scope of this tutorial. Modify the OnMediaStreamSourceRequested() event handler to dispatch either to the local or to the remote bridge: if (sender == _localVideoSource) videoBridge = _localVideoBridge; else if (sender == _remoteVideoSource) videoBridge = _remoteVideoBridge; else return; Implement the handler with the newly created members: private void RemoteVideo_I420AFrameReady(I420AVideoFrame frame) { lock (_remoteVideoLock) { if (!_remoteVideoPlaying) { _remoteVideoPlaying = true; uint width = frame.width; uint height = frame.height; RunOnMainThread(() => { // Bridge the remote video track with the remote media player UI int framerate = 30; // assumed, for lack of an actual value _remoteVideoSource = CreateI420VideoStreamSource(width, height, framerate); var remoteVideoPlayer = new MediaPlayer(); remoteVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource( _remoteVideoSource); remoteVideoPlayerElement.SetMediaPlayer(remoteVideoPlayer); remoteVideoPlayer.Play(); }); } } _remoteVideoBridge.HandleIncomingVideoFrame(frame); } In the App_Suspending() event handler, add a line to also clear the media player of the remote element. remoteVideoPlayerElement.SetMediaPlayer(null); Finally, we need to change the UI to add the new remoteVideoPlayerElement XAML control displaying the remote video track. Open MainPage.xaml in the visual editor and edit it: Add a new <MediaPlayerElement> tag for the remote video. <MediaPlayerElement x:Name=\"remoteVideoPlayerElement\" /> Warning Be sure to put the remote tag first, before the local one, so that the remote video is rendered first in the background, and the local one second on top of it. Otherwise the local video will be hidden by the remote one. Change the tag for the local video to reduce its size and position it in the lower right corner of the window, like is typical for local video preview in a video chat application. <MediaPlayerElement x:Name=\"localVideoPlayerElement\" Width=\"320\" Height=\"240\" HorizontalAlignment=\"Right\" VerticalAlignment=\"Bottom\" Margin=\"0,0,20,20\" /> Here we fix the size to 320x240 pixels, align the control to the lower right corner of the window, and add a 20px margin. At this point, the sample application is functional, although there is no mechanism to initiate a call. You can either add a button or similar to call CreateOffer() , or test this sample with the TestAppUWP available in the MixedReality-WebRTC repository in examples/TestAppUWP , which implements a \"Create offer\" button. Be sure to set the correct local and remote peer ID on both peers and have a node-dss server running before hitting the \"Create offer\" button, otherwise signaling will not work. In either cases however, be sure to create transceivers only on the caller: If adding a CreateOffer() button to the application, then move the calls to AddTransceiver() into the button handler, so that they are manually created on the caller only. On the callee, they will be created automatically in the same order when applying the offer received from the caller. If using the TestAppUWP to make a connection with, the TestAppUWP is the caller so the transceivers should be created inside that app, and the calls to AddTransceiver() deleted from App1 ."
  },
  "manual/cs/helloworld-cs-core3.html": {
    "href": "manual/cs/helloworld-cs-core3.html",
    "title": "Hello, C# world! (Desktop) | MixedReality-WebRTC Documentation",
    "keywords": "Hello, C# world! (Desktop) In this tutorial we will create a simple .NET Core 3.0 application based on the MixedReality-WebRTC C# library . Creating a project Creating a peer connection Adding local media tracks A custom signaling solution Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-mediatracks-core3.html": {
    "href": "manual/cs/helloworld-cs-mediatracks-core3.html",
    "title": "Adding local media tracks | MixedReality-WebRTC Documentation",
    "keywords": "Adding local media tracks Now that the peer connection is initialized, there are two possible paths depending on the usage scenario: Immediately adding local audio and/or video tracks to the peer connection, so that they are available right away when the connection will be established with the remote peer. Waiting for the connection to be established, and add the local media tracks after that. The first case is benefical in the sense that media tracks will be immediately negotiated during the connection establishing, without the need for an extra negotiation specific to the tracks. However it requires knowing in advance that the tracks are used. Conversely, the latter case corresponds to a scenario like late joining, where the user or the application can control when to add or remove local tracks, at the expense of requiring an extra network negotiation each time the list of tracks changes. In this tutorial, we add the local media tracks right away for simplicity. Continue editing the Program.cs file and append the following: Create some variables we need at the start of the Main method, before the try block: AudioTrackSource microphoneSource = null; VideoTrackSource webcamSource = null; Transceiver audioTransceiver = null; Transceiver videoTransceiver = null; LocalAudioTrack localAudioTrack = null; LocalVideoTrack localVideoTrack = null; Use the DeviceVideoTrackSource.CreateAsync() method to create a new video track source obtaining its frames from a local video capture device (webcam). webcamSource = await DeviceVideoTrackSource.CreateAsync(); This method optionally takes a LocalVideoDeviceInitConfig object to configure the video capture. In this tutorial, we leave that object out and use the default settings, which will open the first available webcam with its default resolution and framerate. This is generally acceptable, although on mobile devices like HoloLens developers probably want to limit the capture resolution and framerate to reduce the power consumption and save on battery. The video track source is a standalone object, which can be used by multiple tracks, including from different peer connections. This allows sharing a local webcam among multiple conections. From this source, create a local video track which will send those captured frames to the remote peer. var videoTrackConfig = new LocalVideoTrackInitConfig { trackName = \"webcam_track\" }; localVideoTrack = LocalVideoTrack.CreateFromSource(webcamSource, videoTrackConfig); Note that the local video track created is not associated with the peer connection yet; the LocalVideoTrack.CreateFromSource() is a static method which does not reference any peer connection. The local video track will be bound to one specific peer connection later when added to a video transceiver. After that, it will stay implicitly bound to that peer connection, even if detached from its transceiver, and cannot be reused with another peer connection. Use the DeviceAudioTrackSource.CreateAsync() method to create an audio track source obtaining its audio frames from a local audio capture device (microphone). microphoneSource = await DeviceAudioTrackSource.CreateAsync(); Again, the method optionally takes LocalAudioDeviceInitConfig object to configure the audio capture, but we can ignore it to get the default settings. Use the LocalAudioTrack.CreateFromSource() method to create an audio track sending to the remote peer those audio frames. var audioTrackConfig = new LocalAudioTrackInitConfig { trackName = \"microphone_track\" }; localAudioTrack = LocalAudioTrack.CreateFromSource(microphoneSource, audioTrackConfig); Use the PeerConnection.AddTransceiver() method to add to the peer connection some audio and video transceivers, which are the transports through which the audio and video tracks are sent to the remote peer. videoTransceiver = pc.AddTransceiver(MediaKind.Video); videoTransceiver.LocalVideoTrack = localVideoTrack; videoTransceiver.DesiredDirection = Transceiver.Direction.SendReceive; audioTransceiver = pc.AddTransceiver(MediaKind.Audio); audioTransceiver.LocalAudioTrack = localAudioTrack; audioTransceiver.DesiredDirection = Transceiver.Direction.SendReceive; This binds the local media tracks to the peer connection forever, and instructs the native implementation to use those tracks while they are attached to a transceiver. This also ask to negotiate a bidirectional transport (send + receive) for each transceiver in order to both send the audio and video captured locally, and receive those the remote peer will send. At the very end of the program, outside of the try / catch block, make sure to dispose of the audio and video tracks and sources after the peer connection has been closed. The audio and video tracks and sources are owned by the user, and must be disposed of manually. The media sources also must outlive the local tracks using them, so make sure to dispose of them last. localAudioTrack?.Dispose(); localVideoTrack?.Dispose(); microphoneSource?.Dispose(); webcamSource?.Dispose(); Run the application again. This time, there is no visible difference in the terminal window, except some extra delay to open the audio and video devices; this delay varies greatly depending on the number of capture devices on the host machine, but is generally within a few seconds too. Additionally, if the webcam or microphone have a LED indicating recording, it should briefly turn ON when the capture device starts recording, and immediately stop when the program reaches its end and the peer connection and its tracks are automatically shut down. Next : A custom signaling solution"
  },
  "manual/cs/helloworld-cs-mediatracks-uwp.html": {
    "href": "manual/cs/helloworld-cs-mediatracks-uwp.html",
    "title": "Add local media tracks | MixedReality-WebRTC Documentation",
    "keywords": "Add local media tracks Now that the peer connection is initialized, there are two possible paths, which can be both used: Immediately adding local audio and/or video tracks to the peer connection, so that they are available right away when the connection will be established with the remote peer. Waiting for the connection to be established, and add the local media tracks after that. The first case is benefical in the sense that media tracks will be immediately negotiated during the connection establishing, without the need for an extra negotiation specific for the tracks. However it requires knowing in advance that the tracks are used. Conversely, the latter case corresponds to a scenario like late joining, where the user or the application can control when to add or remove local tracks, at the expense of requiring an extra network negotiation each time the list of tracks is changed. In this tutorial, we add the local media tracks right away for simplicity. Creating the tracks Media tracks are slim objects which bridge a media track source , that is a media frame producer, on one hand and a transceiver , that is a \"media pipe\" to transport that media to the remote peer, on the other hand. Media track sources can be of various types, but the most common ones are device-based, meaning they produce some audio and video frames by capturing them from a device. The main classes for this are the DeviceAudioTrackSource and the DeviceVideoTrackSource , which respectively open an audio or video capture device and use it to capture audio or video frames to provide them to one or more tracks. Continue editing the MainPage.xaml.cs file and append in the OnLoaded method the following: Create new private variables needed to store the tracks and their sources. DeviceAudioTrackSource _microphoneSource; DeviceVideoTrackSource _webcamSource; LocalAudioTrack _localAudioTrack; LocalVideoTrack _localVideoTrack; Use the DeviceVideoTrackSource.CreateAsync() method to create a new video track source obtaining its frames from a local video capture device (webcam). _webcamSource = await DeviceVideoTrackSource.CreateAsync(); This method optionally takes a LocalVideoDeviceInitConfig object to configure the video capture. In this tutorial, we leave that object out and use the default settings, which will open the first available webcam with its default resolution and framerate. This is generally acceptable, although on mobile devices like HoloLens developers probably want to limit the capture resolution and framerate to reduce the power consumption and save on battery. The video track source is a standalone object, which can be used by multiple tracks, including from different peer connections. This allows sharing a local webcam among multiple conections. From this source, create a local video track which will send those captured frames to the remote peer. var videoTrackConfig = new LocalVideoTrackInitConfig { trackName = \"webcam_track\" }; _localVideoTrack = LocalVideoTrack.CreateFromSource(_webcamSource, videoTrackConfig); Note that the local video track created is not associated with the peer connection yet; the LocalVideoTrack.CreateFromSource() is a static method which does not reference any peer connection. The local video track will be bound to one specific peer connection later when added to a video transceiver. After that, it will stay implicitly bound to that peer connection, even if detached from its transceiver, and cannot be reused with another peer connection. Use the DeviceAudioTrackSource.CreateAsync() method to create an audio track source obtaining its audio frames from a local audio capture device (microphone). _microphoneSource = await DeviceAudioTrackSource.CreateAsync(); Again, the method optionally takes LocalAudioDeviceInitConfig object to configure the audio capture, but we can ignore it to get the default settings. Use the LocalAudioTrack.CreateFromSource() method to create an audio track sending to the remote peer those audio frames. var audioTrackConfig = new LocalAudioTrackInitConfig { trackName = \"microphone_track\" }; _localAudioTrack = LocalAudioTrack.CreateFromSource(_microphoneSource, audioTrackConfig); Adding the transceivers Continue editing the MainPage.xaml.cs file and append in the OnLoaded method the following: Create 2 new Transceiver variables, one for the audio feed and one for the video feed. Transceiver _audioTransceiver; Transceiver _videoTransceiver; Use the AddTransceiver() method to create the audio and video transceivers on the peer connection. _audioTransceiver = _peerConnection.AddTransceiver(MediaKind.Audio); _videoTransceiver = _peerConnection.AddTransceiver(MediaKind.Video); Note that the order of the calls to AddTransceiver() matters. Here we create first an audio transceiver associated with the media line index #0, and then a video transceiver associated with the media line index #1. On the remote peer, when it receives the SDP offer from this local peer and applies it by calling SetRemoteDescriptionAsync() , the transceivers will be created in that same order. Important As is, the code will not produce the intended effect, because transceivers must be added only on the offering (also known as caller ) peer, and then are automatically created on the answering (also known as callee ) peer, so adding transceivers on both peers will result in twice the amount intended. The calls to AddTransceiver() above need to be conditional to the caller, but this requires a mechanism that we will add only later in this tutorial when looking at establishing a connection. For now we leave this code as is, and will revisit it in the last chapter. By default the transceivers have no track attached to them, and will send some empty media data (black frame for video, silence for audio). Attach the local tracks created prevoously to the transceivers, so that the WebRTC implementation uses them instead and send their media data to the remote peer. _audioTransceiver.LocalAudioTrack = _localAudioTrack; _videoTransceiver.LocalVideoTrack = _localVideoTrack; At this point, if you run the application again, there is no visible difference, except some extra delay to open the audio and video devices; this delay varies greatly depending on the number of capture devices on the host machine, but is generally within a few seconds too, sometimes much less. Additionally, if the webcam or microphone have a LED indicating recording, it should turn ON when the capture device starts recording. But the captured audio and video are not visible. This is because the audio and video tracks are capturing frames from the webcam and microphone, to be sent later to the remote peer once connected to it, but there is no local rendering by default. Importing the VideoBridge utility In order to display the local webcam feed as a feedback for the user, we need to collect the video frames captured by WebRTC via the webcam, and display them locally in the app using whichever technology we choose. In this tutorial for simplicity we use the MediaPlayerElement XAML control, which is based on the Media Foundation framework. And to keep things simple, we also use the TestAppUwp.Video.VideoBridge helper class from the TestAppUWP sample application provided by MixedReality-WebRTC, which bridges a raw source of frames (WebRTC) with the MediaPlayerElement , taking care of the interoperability details for us. The TestAppUwp.Video.VideoBridge helper class makes use of the StreamSamplePool class, which also needs to be imported. For the sake of simplicity in this tutorial we simply copy those two files. Download the VideoBridge.cs and StreamSamplePool.cs from the MixedReality-WebRTC repository, or copy them from a local clone of the repository, and paste them into the current tutorial project. Add a reference to the App1.csproj project by right-clicking on the project in the Solution Explorer panel and selecting Add > Existing Item... (or using Shift+Alt+A), pointing the add dialog to the two newly copied files VideoBridge.cs and StreamSamplePool.cs . Editing the local video UI Double-click on the MainPage.xaml (or right-click > Open ) to bring up the XAML visual editor of the MainPage page. The page is currently blank, similar to what is displayed when launching the application. We will add a video player to it. The XAML visual editor allows editing the XAML user interface both via the visual top panel and via the XAML code in the bottom panel. Which approach is best is generally up to personal preference, although for discoverability it is easier to drag and drop controls from the Toolbox panel. In this tutorial however we make use of the MediaPlayerElement control which is not available from the toolbox. Edit the MainPage.xaml code from the XAML panel of the editor, and inside the <Grid> element add a <MediaPlayerElement> node: <Grid> <MediaPlayerElement x:Name=\"localVideoPlayerElement\" /> </Grid> This will create a media player which covers the entire surface of the application window. Bridging the track and its rendering The key link between a raw source of video frames and the Media Foundation pipeline is the MediaStreamSource class, which wraps an external video source to deliver raw frames directly to the media playback pipeline. Get back to the associated MainPage.xaml.cs and continue editing: At the top of the file, import the following extra modules: using TestAppUWP.Video; using Windows.Media.Core; using Windows.Media.Playback; using Windows.Media.MediaProperties; Create a new utility method CreateI420VideoStreamSource() which builds a MediaStreamSource instance to encaspulate a given video stream encoded in I420 format, which is the encoding in which WebRTC provides its raw video frames. This method will be reused later for the remote video too. private MediaStreamSource CreateI420VideoStreamSource( uint width, uint height, int framerate) { if (width == 0) { throw new ArgumentException(\"Invalid zero width for video.\", \"width\"); } if (height == 0) { throw new ArgumentException(\"Invalid zero height for video.\", \"height\"); } // Note: IYUV and I420 have same memory layout (though different FOURCC) // https://docs.microsoft.com/en-us/windows/desktop/medfound/video-subtype-guids var videoProperties = VideoEncodingProperties.CreateUncompressed( MediaEncodingSubtypes.Iyuv, width, height); var videoStreamDesc = new VideoStreamDescriptor(videoProperties); videoStreamDesc.EncodingProperties.FrameRate.Numerator = (uint)framerate; videoStreamDesc.EncodingProperties.FrameRate.Denominator = 1; // Bitrate in bits per second : framerate * frame pixel size * I420=12bpp videoStreamDesc.EncodingProperties.Bitrate = ((uint)framerate * width * height * 12); var videoStreamSource = new MediaStreamSource(videoStreamDesc); videoStreamSource.BufferTime = TimeSpan.Zero; videoStreamSource.SampleRequested += OnMediaStreamSourceRequested; videoStreamSource.IsLive = true; // Enables optimizations for live sources videoStreamSource.CanSeek = false; // Cannot seek live WebRTC video stream return videoStreamSource; } The CreateI420VideoStreamSource() method references the SampleRequested event, which is invoked by the Media Foundation playback pipeline when it needs a new frame. We use the VideoBridge helper class to serve those frames. At the top of the MainPage class, define two new variables: a MediaStreamSource for wrapping the local video stream and exposing it to the Media Foundation playback pipeline, and a VideoBridge for managing the delivery of the video frames. The video bridge is initialized with a queue capacity of 3 frames, which is generally enough for local video as it is not affected by network latency. private MediaStreamSource _localVideoSource; private VideoBridge _localVideoBridge = new VideoBridge(3); Implement the OnMediaStreamSourceRequested() callback using the video bridge. As we plan to reuse that callback for the remote video, the code finds the suitable video bridge based on the source which invoked the event. private void OnMediaStreamSourceRequested(MediaStreamSource sender, MediaStreamSourceSampleRequestedEventArgs args) { VideoBridge videoBridge; if (sender == _localVideoSource) videoBridge = _localVideoBridge; else return; videoBridge.TryServeVideoFrame(args); } In the OnLoaded() method where the video track source was created, subscribe to the I420AVideoFrameReady event. _webcamSource = await DeviceVideoTrackSource.CreateAsync(); _webcamSource.I420AVideoFrameReady += LocalI420AFrameReady; Implement the event handler by enqueueing the newly captured video frames into the bridge, which will later deliver them when the Media Foundation playback pipeline requests them. private void LocalI420AFrameReady(I420AVideoFrame frame) { _localVideoBridge.HandleIncomingVideoFrame(frame); } Starting the media playback The last part is to actually start the playback pipeline when video frames start to be received from WebRTC. This is done lazily for two reasons: to avoid starving the Media Foundation playback pipeline if the WebRTC video track source takes some time to start delivering frames, which it generally does compared to the expectation of the playback pipeline. to get access to the frame resolution, which is not otherwise available from WebRTC. Unfortunately at this time the capture framerate is not available, so we assume a framerate of 30 frames per second (FPS). At the top of the MainPage class, add a boolean field to indicate whether the local video is playing. This is protected by a lock, because the I420AVideoFrameReady and the SampleRequested events can be fired in parallel from multiple threads. private bool _localVideoPlaying = false; private object _localVideoLock = new object(); Modify the LocalI420AFrameReady() event handler to start the media player when the first WebRTC frame arrives. private void LocalI420AFrameReady(I420AVideoFrame frame) { lock (_localVideoLock) { if (!_localVideoPlaying) { _localVideoPlaying = true; // Capture the resolution into local variable useable from the lambda below uint width = frame.width; uint height = frame.height; // Defer UI-related work to the main UI thread RunOnMainThread(() => { // Bridge the local video track with the local media player UI int framerate = 30; // assumed, for lack of an actual value _localVideoSource = CreateI420VideoStreamSource( width, height, framerate); var localVideoPlayer = new MediaPlayer(); localVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource( _localVideoSource); localVideoPlayerElement.SetMediaPlayer(localVideoPlayer); localVideoPlayer.Play(); }); } } // Enqueue the incoming frame into the video bridge; the media player will // later dequeue it as soon as it's ready. _localVideoBridge.HandleIncomingVideoFrame(frame); } Some of the work cannot be carried during the execution of this event handler, which is invoked from an unspecified worker thread, because access to XAML UI elements must be done exclusively on the main UWP UI thread. Therefore we use a helper method which schedule this work for execution on that thread. Note The use of _localVideoSource from the OnMediaStreamSourceRequested() event handler is not protected by the _localVideoLock lock. This is because the event cannot be fired until well after the _localVideoSource has been assigned a new value, so there is no race condition concern here. And since _localVideoSource is not further modified, we avoid acquiring that lock in the OnMediaStreamSourceRequested() to reduce the chances of contention. The lock is actually not needed at all at this point, since _localVideoPlaying is also only modified in the current LocalI420AFrameReady() event handler. But a typical application will provide some UI like a button to start and stop the local video, and therefore needs to synchronize access to _localVideoPlaying and _localVideoSource , at which point OnMediaStreamSourceRequested() will also need to acquire this lock. Implement the RunOnMainThread() helper using the Dispatcher of the current window. private void RunOnMainThread(Windows.UI.Core.DispatchedHandler handler) { if (Dispatcher.HasThreadAccess) { handler.Invoke(); } else { // Note: use a discard \"_\" to silence CS4014 warning _ = Dispatcher.RunAsync(Windows.UI.Core.CoreDispatcherPriority.Normal, handler); } } In the App_Suspending() event handler, clear the media player of the localVideoPlayerElement control so that it repaint itself and avoids keeping the last video frame when the video is turned off. localVideoPlayerElement.SetMediaPlayer(null); At this point the MediaPlayerElement control and the WebRTC local video track are connected together. Launch the application again; the local webcam starts capturing video frame, which are displayed in the main window of the application. Next : A custom signaling solution"
  },
  "manual/cs/helloworld-cs-peerconnection-core3.html": {
    "href": "manual/cs/helloworld-cs-peerconnection-core3.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection Next, we create a PeerConnection object which encapsulates the connection to the remote peer. Continue editing the Program.cs file and append the following: Create the peer connection object. Note that the PeerConnection class is marked as disposable, so must be disposed to clean-up the native resources. Failing to do so generally lead to crashes or hangs, as the internal WebRTC threads are not stopped and therefore the native DLL cannot be unloaded. using var pc = new PeerConnection(); This construct with the using keyword is a shorthand that will automatically call Dispose() when that variable pc gets out of scope, in our case at the end of the Main function. The PeerConnection object is initally created in an idle state where it cannot be used until initialized with a call to InitializeAsync() . This method takes a PeerConnectionConfiguration object which allows specifying some options to configure the connection. In this tutorial, most default options are suitable, but we want to specify a STUN server to make sure that the peer connection can connect to the remote peer even if behind a NAT . var config = new PeerConnectionConfiguration { IceServers = new List<IceServer> { new IceServer{ Urls = { \"stun:stun.l.google.com:19302\" } } } }; await pc.InitializeAsync(config); In this example we use a free STUN server courtesy of Google. Note that this is fine for testing, but must not be used for production . Also, the ICE server list uses the List<> generic class, so we need to import the System.Collections.Generic module with a using directive at the top of the file. using System.Collections.Generic; Print a simple message to console to notify the user that the peer connection is initialized. This is optional, but is always good practice to inform the user after important steps completed, whether successfully or not. Console.WriteLine(\"Peer connection initialized.\"); Run the application again; the printed message should appear after some time. It generally takes up to a few seconds to initialize the peer connection, depending on the device. Next : Adding local media tracks"
  },
  "manual/cs/helloworld-cs-peerconnection-uwp.html": {
    "href": "manual/cs/helloworld-cs-peerconnection-uwp.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection Next, we create a PeerConnection object which encapsulates the connection to the remote peer. The PeerConnection class is marked as disposable, so must be disposed to clean-up the native resources. Failing to do so generally lead to crashes or hangs, as the internal WebRTC threads are not stopped and therefore the native DLL cannot be unloaded. Unlike in the .NET Core tutorial where we build a console application however, here with the XAML framework we cannot easily use the using var construct to rely on the C# compiler to call the Dispose() method for us, because currently the only available method is the OnLoaded() method, and it will terminate and release all its local variable once loading is finished, and before the end of the application. Instead, we need to keep a reference to the PeerConnection instance and call Dispose() explicitly when done with it. Continue editing the MainPage.xaml.cs file and append the following: At the top of the MainPage class, declare a private variable of type PeerConnection . private PeerConnection _peerConnection; Continue to append to the OnLoaded() method. First, instantiate the peer connection. _peerConnection = new PeerConnection(); The PeerConnection object is initally created in an idle state where it cannot be used until initialized with a call to InitializeAsync() . This method takes a PeerConnectionConfiguration object which allows specifying some options to configure the connection. In this tutorial, most default options are suitable, but we want to specify a STUN server to make sure that the peer connection can connect to the remote peer even if behind a NAT . var config = new PeerConnectionConfiguration { IceServers = new List<IceServer> { new IceServer{ Urls = { \"stun:stun.l.google.com:19302\" } } } }; await _peerConnection.InitializeAsync(config); In this example we use a free STUN server courtesy of Google. Note that this is fine for testing, but must not be used for production . Also, the ICE server list uses the List<> generic class, so we need to import the System.Collections.Generic module with a using directive at the top of the file. using System.Collections.Generic; Print a simple message to the debugger to confirm that the peer connection wass initialized. In a real-world application, properly notifying the user of failures is critical, but here for the sake of this tutorial we simply rely on a any exception interrupting the application before the message is printed if an error occur. Debugger.Log(0, \"\", \"Peer connection initialized successfully.\\n\"); In the App_Suspending() event handler, add some code to dispose of the peer connection. private void App_Suspending(object sender, SuspendingEventArgs e) { if (_peerConnection != null) { _peerConnection.Close(); _peerConnection.Dispose(); _peerConnection = null; } } Run the application again; the printed message should appear after some time in the Visual Studio Output window under the Debug section. It can take up to a few seconds to initialize the peer connection, depending on the device. Next : Adding local media tracks"
  },
  "manual/cs/helloworld-cs-setup-core3.html": {
    "href": "manual/cs/helloworld-cs-setup-core3.html",
    "title": "Creating a project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a project In this tutorial we use .NET Core 3.1 to create a C# Desktop application. Because there is currently no simple solution to display raw video frames from a .NET Core application, like there is with UWP, we will limit this tutorial to creating a simple console application without graphics. Think of it as a recording tool to broadcast audio and/or video to a remote device. Note This tutorial assumes that the host device where the app will be running during the tutorial has access to: a webcam, or any other video capture device recognized by WebRTC a microphone, or any other audio capture device recognized by WebRTC Install .NET Core 3.1 Download the latest .NET Core 3.1 SDK (and not Runtime) from its download page and install it. Warning Visual Studio v16.3 or later is required, otherwise the console application might not run correctly inside Visual Studio. Generate the project Open a terminal and use the dotnet command to create a new project from the console template. We will name this tutorial project TestNetCoreConsole . dotnet new console --name TestNetCoreConsole This generates a folder named TestNetCoreConsole which contains the following notable files: TestNetCoreConsole.csproj : The C# project Program.cs : The C# source code for the application Note Starting Visual Studio v16.3, this project can also be generated from the Visual Studio wizard by creating a new project and selecting Console App (.NET Core) , which will default to .NET Core 3.1. Open the .NET Core project in Visual Studio 2019 Open the C# project generated earlier ( TestNetCoreConsole.csproj ), then build and run it, either by pressing F5 or selecting in the menu Debug > Start Debugging . After the project built successfully, a terminal window should appear. Note The project can alternatively be built on the command line with dotnet build , and launched with dotnet run (which implies building). Executing a dotnet run returns immediately, since the empty generated program does not currently output anything. Add a dependency to MixedReality-WebRTC In order to use the MixedReality-WebRTC project in this new TestNetCoreConsole app, we will add a dependency to its C# NuGet package hosted on nuget.org . This is by far the easiest way, although a locally-built copy of the Microsoft.MixedReality.WebRTC.dll assembly could also be alternatively used (but this is out of the scope of this tutorial). There are again multiple ways to add a reference to this NuGet package, in particular via the Visual Studio NuGet package manager for the project, or via the dotnet command line. For simplicity, we show here how to do so the dotnet way, which simply involves typing a single command from within the project folder. dotnet add TestNetCoreConsole.csproj package Microsoft.MixedReality.WebRTC This will download from nuget.org and install the Microsoft.MixedReality.WebRTC.nupkg NuGet package, which contains the same-named assembly, as well as its native dependencies (x86 and x64) for the Windows Desktop platform. After that, TestNetCoreConsole.csproj should contain a reference to the package, with a version corresponding to the latest stable version found on nuget.org , or the one you specified with the --version option. <ItemGroup> <PackageReference Include=\"Microsoft.MixedReality.WebRTC\" Version=\"...\" /> </ItemGroup> Test the reference In order to ensure everything works fine and the Microsoft.MixedReality.WebRTC assembly can be used, we will use one of its functions to list the video capture devices, as a test. This makes uses of the static method DeviceVideoTrackSource.GetCaptureDevicesAsync() . This is more simple than creating objects, as there is no clean-up needed after use. Edit the Program.cs file: At the top of the file, add some using statement to import the Microsoft.MixedReality.WebRTC assembly. Also import the System.Threading.Tasks module, as we will use the async / await construct and the Task object. using System.Threading.Tasks; using Microsoft.MixedReality.WebRTC; Modify the signature of the Main function to make it asynchronous, by changing its return type from void to Task and adding the async keyword. This allows using the await keyword inside its body, which greatly simplifies the writing for asynchronous code. static async Task Main(string[] args) Change the body of the Main function to call GetCaptureDevicesAsync() and display the list of devices found on the standard output. static async Task Main(string[] args) { try { // Asynchronously retrieve a list of available video capture devices (webcams). var deviceList = await DeviceVideoTrackSource.GetCaptureDevicesAsync(); // For example, print them to the standard output foreach (var device in deviceList) { Console.WriteLine($\"Found webcam {device.name} (id: {device.id})\"); } } catch (Exception e) { Console.WriteLine(e.Message); } } Launch the application again. This time the terminal window shows a list of devices. This list depends on the actual host device (computer) where the application is running, but looks something like: Found webcam <some webcam name> (id: <some long ID>) Note that there might be multiple lines if multiple capture devices are available, which is unusual but can happen e.g. if you plug a USB webcam into a laptop which already has an integrated webcam. In general the first capture device listed will be the default one used by WebRTC, although it is possible to explicitly select a device (see DeviceVideoTrackSource.CreateAsync() for more details). Next : Creating a peer connection"
  },
  "manual/cs/helloworld-cs-setup-uwp.html": {
    "href": "manual/cs/helloworld-cs-setup-uwp.html",
    "title": "Creating a project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a project In this tutorial we create a C# UWP application with a simple XAML-based UI to render video. Note At this time there is no solution to render raw video frames from a .NET Core 3.0 application that is simple and short enough to be used in a turorial. Instead, we use the MediaPlayerElement XAML control from UWP which provides the necessary API. Its WPF equivalent MediaElement unfortunately does not allow specifying a custom video source other than an URI-based source like a file on disk. Note This tutorial assumes that the host device where the app will be running during the tutorial has access to: a webcam, or any other video capture device recognized by WebRTC a microphone, or any other audio capture device recognized by WebRTC Generate the project Open Visual Studio 2019 and select Create a new project . Filter projects by Language = C# and Platform = UWP to find the Blank App (Universal Window) project template. When prompted to select a platform version, chose a version of at least Windows 10, version 1803 (10.0; Build 17134) or higher, as older versions are not officially supported by MixedReality-WebRTC. For HoloLens 2 development, it is strongly recommended to select at least Windows 10, version 1903 (10.0; Build 18362) to be able to use the latest OS APIs, although those are not used in this tutorial. Visual Studio 2019 generates a C# project ( .csproj ) and solution ( .sln ). In this tutorial we use the default name App1 , therefore we get the following hierarchy: - App1/ - Assets/ - App.xaml - App.xaml.cs - App1.csproj - MainPage.xaml - MainPage.xaml.cs - App1.sln At this point the project is already in a working state, although not yet functional. Press F5 or select in the menu Debug > Start Debugging to build and launch the application. A blank window titles \"App1\" should appear. Note : At the top of the window, the XAML debug bar allows access to debugging feature for the UI. This bar only appears while debugging, not when running the app outside Visual Studio 2019. It can be ignored. Add a dependency to MixedReality-WebRTC In order to use the MixedReality-WebRTC project in this new App1 application, we will add a dependency to its C# NuGet package hosted on nuget.org . This is by far the easiest way, although a locally-built copy of the Microsoft.MixedReality.WebRTC.dll assembly could also be alternatively used (but this is out of the scope of this tutorial). In the Solution Explorer , right-click on the App1 (Universal Windows) C# project and select the Manage NuGet Packages... menu entry. This opens a new tab NuGet: App1 which allows configuring the NuGet dependencies for this project alone. The Installed tab contains the list of NuGet dependencies already installed, and should contain the Microsoft.NETCore.UniversalWindowsPlatform which was already installed by Visual Studio when creating the project. Select the Browse tab and, after making sure that the Package source is set to nuget.org , select the Microsoft.MixedReality.WebRTC.UWP NuGet package and click Install . Note : If you cannot find the package, make sure that Include prerelease is checked, which disables filtering out preview packages (those packages with a version containing a suffix like \"-preview\" after the X.Y.Z version number). This will download from nuget.org and install the Microsoft.MixedReality.WebRTC.UWP.nupkg NuGet package, which contains the Microsoft.MixedReality.WebRTC.dll assembly, as well as its native dependencies (x86, x64, ARM) for the UWP platform. After that, the App1 project should contain a reference to the package. Test the reference In order to ensure everything works fine and the Microsoft.MixedReality.WebRTC assembly can be used, we will use one of its functions to list the video capture devices, as a test. This makes uses of the static method DeviceVideoTrackSource.GetCaptureDevicesAsync() . This is more simple than creating objects, as there is no clean-up needed after use. First, because this sample application is a UWP application, it needs to declare some capabilities to access the microphone and webcam on the host device. In the Solution Explorer of Visual Studio, double-click on the Package.appxmanifest to open the AppX manifest of the app and select the Capabilities tab. Check Microphone and Webcam , and confirm that Internet (Client) is already checked. Warning Currently the Microphone capability is mandatory, even if not using audio. This is due to a limitation of the underlying native implementation which attempts to access the microphone while initializing the audio module (see #360 ). The Webcam capability however is only needed if using video. Next, edit MainPage.xaml.cs : At the top of the file, add some using statement to import the Microsoft.MixedReality.WebRTC assembly. Also import the System.Diagnostics module, as we will be using the Debugger class to print debug information to the Visual Studio output window. Finally, import the Windows.Media.Capture module to be able to request access to the microphone and webcam, and the Windows.ApplicationModel module to handle resource clean-up. using Microsoft.MixedReality.WebRTC; using System.Diagnostics; using Windows.Media.Capture; using Windows.ApplicationModel; In the MainPage constructor, register a handler for the Loaded event, which will be fired once the XAML user interface finished loading. For now it is not required to wait on the UI to call Microsoft.MixedReality.WebRTC methods. But later when accessing the UI to interact with its controls, either to get user inputs or display results, this will be required. So as a best practice we start doing so right away instead of invoking some code directly in the MainPage constructor. Also register a handler for the Application.Suspending event to clean-up resources on exit. public MainPage() { this.InitializeComponent(); this.Loaded += OnLoaded; Application.Current.Suspending += App_Suspending; } Create the event handler OnLoaded() and use it to request access from the user to the microphone and camera, and enumerate the video capture devices. The MediaCapture.InitializeAsync() call will prompt the user with a dialog to authorize access to the microphone and webcam. The latter be must authorized before calling DeviceVideoTrackSource.GetCaptureFormatsAsync() , while the former will be needed in the following of the tutorial for calls like PeerConnection.AddLocalAudioTrackAsync() . private async void OnLoaded(object sender, RoutedEventArgs e) { // Request access to microphone and camera var settings = new MediaCaptureInitializationSettings(); settings.StreamingCaptureMode = StreamingCaptureMode.AudioAndVideo; var capture = new MediaCapture(); await capture.InitializeAsync(settings); // Retrieve a list of available video capture devices (webcams). IReadOnlyList<VideoCaptureDevice> deviceList = await DeviceVideoTrackSource.GetCaptureDevicesAsync(); // Get the device list and, for example, print them to the debugger console foreach (var device in deviceList) { // This message will show up in the Output window of Visual Studio Debugger.Log(0, \"\", $\"Webcam {device.name} (id: {device.id})\\n\"); } } Create the event handler App_Suspending() . For now there is nothing to do from it. private void App_Suspending(object sender, SuspendingEventArgs e) { } Launch the app again. The main window is still empty, but the Output window of Visual Studio 2019 ( View > Output , or Alt + 2 ) should show a list of devices. This list depends on the actual host device running the app, but looks something like: Webcam <some device name> (id: <some device ID>) Note that there might be multiple lines if multiple capture devices are available. In general the first one listed will be the default used by WebRTC, although it is possible to explicitly select a device (see DeviceVideoTrackSource.CreateAsync ). If this is the first time that MediaCapture.InitializeAsync() is requesting access to the webcam and microhpone, Windows displays a prompt asking the user for confirmation. You must click Yes , otherwise access to the microphone and webcam will be denied, and WebRTC will not be able to use them. This is part of the standard UWP capability mechanism for security and privacy. If you clicked No by mistake, the prompt will not appear again and access will be silently denied on next runs. To change this access setting again, go to the Windows Settings > Privacy > Microphone , find the App1 application and toggle its access On . Do the same for the webcam from the Settings > Privacy > Camera page. Next : Creating a peer connection"
  },
  "manual/cs/helloworld-cs-signaling-core3.html": {
    "href": "manual/cs/helloworld-cs-signaling-core3.html",
    "title": "A custom signaling solution | MixedReality-WebRTC Documentation",
    "keywords": "A custom signaling solution Signaling is the process of communicating with a remote endpoint with the intent of establishing a peer-to-peer connection. The WebRTC standard does not enforce any specific protocol or solution for WebRTC signaling; instead it simply states that some opaque messages must be transported between the remote peers by whatever mean the developer choses, its signaling solution . In general, the signaling solution involves a third-party server in addition of the two peers trying to connect to each other. Using a third-party server may seem counter-intuitive at first when dealing with peer-to-peer connection, but in general that third-party server is an easy-to-reach server (public IP) which acts as a relay and enables WebRTC to discover a direct route between the two peers even in complex network scenarios (one or both peers behind a NAT) where it would otherwise be impossible for the two peers to directly discover each other. The service provided by the signaling server is also sometimes referred to as some discovery service or identity service (because it makes the identity of each peer available to the other). NamedPipeSignaler In this tutorial we use the NamedPipeSignaler found in examples/TestNetCoreConsole/NamedPipeSignaler in the GitHub repository. This is a simple signaling solution based as the name implies on named pipes, which allows local peer discovery and connection out of the box on a local host without any configuration. This is not a production-ready solution, but for this tutorial it has the benefit of being very simple, sidestepping any networking configuration and potential issue. Install The easiest way to consume the NamedPipeSignaler class in the TestNetCoreConsole sample app is to copy the examples/TestNetCoreConsole/NamedPipeSignaler.cs file alongside the TestNetCoreConsole.csproj project. This avoids the need for any reference setup in the project, or any other kind of project configuration. Pipe creation There is no need to understand how the NamedPipeSignaler class works for this tutorial. But for the sake of curiosity, this is how the connection is established (the reader can skip to the Setup the signaler section below if not interested): Try to create a pipe server. If that succeeds, then this peer is the first peer and will act as server . If that fails, then another peer already created that pipe server, so this peer will act as client . If acting as server: Wait for the remote peer to connect its client pipe to this server. Create a reverse pipe client and connect to the reverse pipe server of the remote peer. If acting as client: Connect to the pipe server created by the other peer. Create a reverse pipe server, and wait for the server to connect back with its reverse pipe client. At this point, both peer have a client pipe for sending data and a server pipe for receiving data, and can communicate. Start a background task to read incoming messages from the remote peer, and wait. We note here that despite WebRTC relying on peer-to-peer connection, the two peers are not strictly equal. This is not only due to the fact that this particular signaling solution is assymetric, but also to the assymetric nature of establishing a WebRTC connection. In general we refer to the peer initiating the connection as the caller and the other peer as the callee . Setup the signaler Continue editing the Program.cs file and append the following: Create a signaler associated with the existing peer connection. var signaler = new NamedPipeSignaler.NamedPipeSignaler(pc, \"testpipe\"); Connect handlers to the signaler's messages, and forward them to the peer connection. signaler.SdpMessageReceived += async (SdpMessage message) => { // Note: we use 'await' to ensure the remote description is applied // before calling CreateAnswer(). Failing to do so will prevent the // answer from being generated, and the connection from establishing. await pc.SetRemoteDescriptionAsync(message); if (message.Type == SdpMessageType.Offer) { pc.CreateAnswer(); } }; signaler.IceCandidateReceived += (IceCandidate candidate) => { pc.AddIceCandidate(candidate); }; In addition of forwarding the messages to the peer connection, we also automatically call PeerConnection.CreateAnswer() on the callee peer as soon as the remote offer received from the caller has been applied. This ensures the minimum amount of latency, but also means the callee automatically accepts any incoming call. Alternatively, a typical application would display some user feedback and wait for confirmation to accept the incoming call. Start the signaler and connect it to the remote peer's signaler. await signaler.StartAsync(); This last call will block until the two signalers are connected with each other. At this point the signaler is functional. However as pointed above it will wait for a second instance of the TestNetCoreConsole app to connect. Currently unless the local machine has at least 2 webcams and 2 microphones then this cannot work because both instances will attempt to capture the webcam and microphone, and one of them will fail to do so and terminate before the program even reach the point where the signaler starts. Optional audio and video capture In order to test the signaler with 2 instances of TestNetCoreConsole and a single microphone and webcam, we need one of those instances not to attempt to open the audio and video capture devices. For this, we had some command-line arguments to control the audio and video capture. Continue editing the Program.cs file: At the top of the Main function, check if the audio and video capture arguments are present on the command-line arguments provided by the user. We name those arguments -v / --video to enable video capture, and -a / --audio to enable audio capture. bool needVideo = Array.Exists(args, arg => (arg == \"-v\") || (arg == \"--video\")); bool needAudio = Array.Exists(args, arg => (arg == \"-a\") || (arg == \"--audio\")); Wrap the calls to AddLocal(Audio|Video)TrackAsync into if blocks using the boolean just defined. We also print some console message, so that the user can confirm whether the flags were indeed taken into account. This is useful to avoid mistakes since we will be running 2 instances of the app, one with the flags and one without. We also move the code for the transeivers inside that block. // Record video from local webcam, and send to remote peer if (needVideo) { Console.WriteLine(\"Opening local webcam...\"); localVideoTrack = await LocalVideoTrack.CreateFromDeviceAsync(); videoTransceiver = pc.AddTransceiver(MediaKind.Video); videoTransceiver.DesiredDirection = Transceiver.Direction.SendReceive; videoTransceiver.LocalVideoTrack = localVideoTrack; } // Record audio from local microphone, and send to remote peer if (needAudio) { Console.WriteLine(\"Opening local microphone...\"); localAudioTrack = await LocalAudioTrack.CreateFromDeviceAsync(); audioTransceiver = pc.AddTransceiver(MediaKind.Audio); audioTransceiver.DesiredDirection = Transceiver.Direction.SendReceive; audioTransceiver.LocalAudioTrack = localAudioTrack; } Establishing a signaler connection At this point the sample app is ready to establish a signaler connection. That is, 2 instances of the TestNetCoreConsole app can be launched, and their NamedPipeSignaler instances will connect to each other. Note however that we are not done yet with the peers, so the WebRTC peer-to-peer connection itself will not be established yet. Start 2 instances of the sample app: one with the audio/video flags, the capturer one without any flag, the receiver Terminal #1 (capturer) dotnet run TestNetCoreConsole -- --audio --video Terminal #2 (receiver) dotnet run TestNetCoreConsole The two terminals should print some messages and eventually indicate that the signaler connection was successful: Signaler connection established. Next : Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-signaling-uwp.html": {
    "href": "manual/cs/helloworld-cs-signaling-uwp.html",
    "title": "A custom signaling solution | MixedReality-WebRTC Documentation",
    "keywords": "A custom signaling solution Signaling is the process of communicating with a remote endpoint with the intent of establishing a peer-to-peer connection. The WebRTC standard does not enforce any specific protocol or solution for WebRTC signaling; instead it simply states that some opaque messages must be transported between the remote peers by whatever mean the developer choses, its signaling solution . The .NET Core Desktop tutorial introduces the NamedPipeSignaler , a simple solution for local discovery based on named pipes. Unfortunately named pipes are not available on UWP, so this solution cannot be used. Instead, and also as an example of an alternate solution, we rely in this tutorial on the existing NodeDssSignaler already used by the TestAppUWP sample app and the Unity integration. This requires a little bit more setup, described in details in the Unity tutorial. Note The NodeDssSignaler found in the TestAppUWP and the one found in the Unity integration use the same protocol and are compatible, but the code is different, the latter being based on Unity's MonoBehaviour component class. Here we use the former, which is written in pure C# and is independent of Unity. Install Like for the VideoBridge helper class, the easiest way to consume the NodeDssSignaler class in the App1 sample app for UWP is to copy the NodeDssSignaler.cs file alongside the App1.csproj project of the current tutorial, and add a reference to it in the project using right-click > Add > Existing Item... (or Shift+Alt+A). The NodeDssSignaler class makes use of the Newtonsoft.Json package for JSON data serialization and deserialization. This module is available as a NuGet package. In the Solution Explorer panel, right click on the References item and select Manage NuGet Packages... . In the Browse panel, search for the Newtonsoft.Json package, select the latest stable version, and click the Install button. Warning Do not forget to start the node-dss signaling server. See the detailed explanations in the Unity turorial about installing and starting it with NodeJS. In this tutorial we choose the local peer ID to be the string \" App1 \", and the remote peer ID is the value we will assign below to the NodeDssSignaler.RemotePeerId field. Setup Continue editing the MainPage.xaml.cs file. Import the TestAppUwp module. At the top of the MainPage.xaml.cs file add: using TestAppUwp; At the top of the MainPage class, create a NodeDssSignaler field. private NodeDssSignaler _signaler; In the OnLoaded() method, add some subscriptions to the signaling events. _peerConnection.LocalSdpReadytoSend += Peer_LocalSdpReadytoSend; _peerConnection.IceCandidateReadytoSend += Peer_IceCandidateReadytoSend; The LocalSdpReadytoSend event is triggered after a call to CreateOffer or CreateAnswer when WebRTC prepared the corresponding SDP message and signals the application the message is ready to be sent. The IceCandidateReadytoSend event similarly corresponds to ICE candidate messages generated by WebRTC, which the application needs to deliver to the remote peer. Implement the event handlers, which simply format the SDP message for the signaler using the utility methods provided by the NodeDssSignaler class. private void Peer_LocalSdpReadytoSend(SdpMessage message) { var msg = NodeDssSignaler.FromSdpMessage(message); _signaler.SendMessageAsync(msg); } private void Peer_IceCandidateReadytoSend(IceCandidate iceCandidate) { var msg = NodeDssSignaler.FromIceCandidate(iceCandidate); _signaler.SendMessageAsync(msg); } Continue appending to the OnLoaded() method to initialize and start the signaler. // Initialize the signaler _signaler = new NodeDssSignaler() { HttpServerAddress = \"http://127.0.0.1:3000/\", LocalPeerId = \"App1\", RemotePeerId = \"<input the remote peer ID here>\", }; _signaler.OnMessage += async (NodeDssSignaler.Message msg) => { switch (msg.MessageType) { case NodeDssSignaler.Message.WireMessageType.Offer: // Wait for the offer to be applied await _peerConnection.SetRemoteDescriptionAsync(msg.ToSdpMessage()); // Once applied, create an answer _peerConnection.CreateAnswer(); break; case NodeDssSignaler.Message.WireMessageType.Answer: // No need to await this call; we have nothing to do after it _peerConnection.SetRemoteDescriptionAsync(msg.ToSdpMessage()); break; case NodeDssSignaler.Message.WireMessageType.Ice: _peerConnection.AddIceCandidate(msg.ToIceCandidate()); break; } }; _signaler.StartPollingAsync(); Warning Take care to set the value of the RemotePeerId field of the NodeDssSignaler to the remote peer's ID, otherwise the signaling will not work. Similarly, this tutorial arbitrarily uses App1 as the local peer ID for the application; this value needs to be set as the remote peer's ID when configuring the remote peer signaler. See the explanations in the Unity turorial for more details. In the App_Suspending() event handler, stop the signaler and clean-up the resources. if (_signaler != null) { _signaler.StopPollingAsync(); _signaler = null; } At this point the sample app is ready to establish a connection. Next : Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-uwp.html": {
    "href": "manual/cs/helloworld-cs-uwp.html",
    "title": "Hello, C# world! (UWP) | MixedReality-WebRTC Documentation",
    "keywords": "Hello, C# world! (UWP) In this tutorial we will create a simple UWP application based on the MixedReality-WebRTC C# library , with a XAML-based UI. Creating a project Creating a peer connection Add local media tracks A custom signaling solution Establishing a WebRTC connection"
  },
  "manual/download.html": {
    "href": "manual/download.html",
    "title": "Downloading MixedReality-WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Downloading MixedReality-WebRTC The Unity library and its optional samples are distributed as UPM packages. The library package contains all binaries for Windows Desktop, UWP, and Android. The C/C++ library and C# library are distributed as NuGet packages for Windows Desktop and UWP. Unity library UPM packages are distributed in two ways: Using the Mixed Reality Feature Tool , a free Microsoft utility to manage Mixed Reality packages for Unity. This is the recommended way, which takes care of installing any required dependency and automatically download and install the package(s) into an existing Unity project. Via direct download on the GitHub Releases page for manual installation. See the Unity documentation Installing a package from a local tarball file . Note If an existing Unity project manifest already contains a Microsoft Mixed Reality entry in the scopedRegistries section, is is recommended that it be removed. The library package contains prebuilt binaries for all supported Unity platforms: Windows Desktop (x86, x64) Windows UWP (x86, x64, ARM) Android (ARM64) See Installation for more details on how to install and use those packages in an existing Unity project, whether via the Mixed Reality Feature Tool or manually via direct download on the GitHub Releases page. NuGet packages Platform C# library C/C++ library Windows Desktop (x86, x64) Microsoft.MixedReality.WebRTC mrwebrtc UWP (x86, x64, ARM) Microsoft.MixedReality.WebRTC.UWP mrwebrtc_uwp See Installation for details on how to use NuGet packages in a Visual Studio project. Legacy v1 packages Warning The legacy v1 native packages are now deprecated and unsupported , and should not be used. In v1 the native packages Microsoft.MixedReality.WebRTC.Native.Desktop and Microsoft.MixedReality.WebRTC.Native.UWP contain a native library exposing a C++ interface. This caused several issues: Compatibility with various C++ compilers and CRT versions, which is a general problem when working with DLLs on Windows platforms. Architecture defect : some library headers were including some internal headers from the Google's WebRTC implementation, which were not shipped with those NuGet packages. See bug #123 for details. To avoid those issues, starting from 2.0.0 the native library exposes a pure C interface. To highlight this change and the fact the packages are native, some new package names are used."
  },
  "manual/gettingstarted.html": {
    "href": "manual/gettingstarted.html",
    "title": "Getting started | MixedReality-WebRTC Documentation",
    "keywords": "Getting started The MixedReality-WebRTC project is comprised of several entities: A C library to integrate into a native C/C++ application A C# library to integrate into a C# application A Unity library to help integrate into an existing Unity application Not all libraries are required for all use cases, but each library builds upon the previous one. This means that for use in a C++ application only the C library needs to be installed. But the Unity library requires installing also the C# and C libraries. In this chapter we discuss: Download : Downloading precompiled binary packages of MixedReality-WebRTC. Installation : How to install the various libraries for use in your poject. Building from sources : Building the MixedReality-WebRTC from sources. Hello, C# world! (Desktop) : Your first C# .NET Core 3.0 application based on the C# library. Hello, C# world! (UWP) : Your first C# UWP application based on the C# library. Hello, Unity world! : Your first Unity application based on the Unity library. Users migrating from a previous version of the libraries can consult the Migration Guide ."
  },
  "manual/glossary.html": {
    "href": "manual/glossary.html",
    "title": "Glossary | MixedReality-WebRTC Documentation",
    "keywords": "Glossary D Data channel A data channel is a \"pipe\" to send and receive random blobs of bytes. L Local media track A local media track is a media track whose source is local to the peer connection, that is which has a local frame-producing media track source . In other words the track is a sender track, which produce frames to be sent to the remote peer. See also media track remote media track M Media The term media refers to either or both of audio and video . It does not include data channels . Media track A media track is a slim entity which bridges a media track source with a transceiver . See also media media track source local media track remote media track Media track source A media track source is an entity producing some media frames. Those frames are made available to one or more media tracks . The source itself is a standalone object not associated with any particular peer connection, and therefore can be used with multiple tracks from multiple peer connections at the same time. See also media media track source P Peer connection The peer connection is the main entity of WebRTC. It manages a connection to a single remote peer. The peer connection contains a collection of transceivers , which describe which media is sent to and received from the remote peer, as well as a collection of data channels , both of which can be empty. See also transceiver data channel R Remote media track A remote media track is a media track whose source is remote to the peer connection, that is which receives its frame from the remote peer; this is a receiver track. See also media track local media track T Transceiver A transceiver is a \"pipe\" for transporting some media between two peers. Each transceiver is owned by a specific peer connection , and describes how this media is encoded (audio or video codec type and options) and transported (transport options, like bandwidth). A transceiver always has exactly one local media track (sender) and one remote media track (receiver), both of which can be null . If null , the transceiver acts as if a dummy track existed which draws its media frames from a null source (black frames for video, silence for audio)."
  },
  "manual/installation.html": {
    "href": "manual/installation.html",
    "title": "Installation | MixedReality-WebRTC Documentation",
    "keywords": "Installation C# library The C# library is consumed as a NuGet package by adding a dependency to that package in your C# project. In Visual Studio 2019: Right-click on the C# project > Manage NuGet Packages... to open the NuGet package manager window. In the search bar, type \"Microsoft.MixedReality.WebRTC\". You may need to check the Include prerelease option. Select the NuGet package to install: For a C# Desktop project, choose Microsoft.MixedReality.WebRTC . For a C# UWP project, choose Microsoft.MixedReality.WebRTC.UWP . In the right panel, choose a version and click the Install button. This will add a dependency to the currently selected C# project. If multiple projects are using the MixedReality-WebRTC library, this process must be repeated for each project. C/C++ library The C/C++ library is consumed as a NuGet package by adding a dependency to that package in your C/C++ project. The C/C++ library is often referred to as the native library or implementation library. In Visual Studio 2019: Right-click on the C++ project > Manage NuGet Packages... to open the NuGet package manager window. Select the Browse tab. In the search bar, type \"Microsoft.MixedReality.WebRTC.Native\". You may need to check the Include prerelease option. Select the NuGet package to install: For a C++ Desktop project, choose mrwebrtc . For a C++ UWP project, choose mrwebrtc_uwp . In the right panel, choose a version and click the Install button. This will add a dependency to the currently selected C++ project. If multiple projects are using the MixedReality-WebRTC library, this process must be repeated for each project. Unity library Starting from v2.0.0, a package named com.microsoft.mixedreality.webrtc is the main distribution method for the Unity library. An optional companion package com.microsoft.mixedreality.webrtc.samples is also available which contains some samples to show how to use the Unity library. This samples package depends on the main library package, and should not be used in production. The library package contains prebuilt binaries for all supported Unity platforms: Windows Desktop (x86, x64) Windows UWP (x86, x64, ARM) Android (ARM64) These packages are currently distributed in two ways: Using the Mixed Reality Feature Tool , a free Microsoft utility to manage Mixed Reality packages for Unity. This is the recommended way, which takes care of installing any required dependency and automatically download and install the package(s) into an existing Unity project. The tool can be downloaded from https://aka.ms/MRFeatureTool . As on-disk packages (local packages) downloaded from the GitHub Releases page . Follow the official Unity instructions to import the package into a Unity project via the Unity Package Manager (UPM) window. Note If an existing Unity project manifest already contains a Microsoft Mixed Reality entry in the scopedRegistries section, is is recommended that it be removed."
  },
  "manual/introduction.html": {
    "href": "manual/introduction.html",
    "title": "Introduction | MixedReality-WebRTC Documentation",
    "keywords": "Introduction The MixedReality-WebRTC project is a collection of components to help mixed reality app developers to integrate peer-to-peer audio, video, and data real-time communication into their application and improve their collaborative experience. These components are based on the WebRTC protocol for Real-Time Communication (RTC), which is supported by most modern web browsers. Project scope The MixedReality-WebRTC project focuses on features which actively contribute to enhance collaborative experiences in mixed reality apps. Although the WebRTC technology can be used for many different video and audio streaming applications -- and in fact the MixedReality-WebRTC project itself can be used in any applications, even non-mixed-reality ones -- the project scope limits the features the maintaining team will spend its development resources on. This does not mean that other features are not welcome; as an open-source project, we welcome any contribution. However we will dedicate more of our own resources to those contributions within the project scope."
  },
  "manual/migration-guide.html": {
    "href": "manual/migration-guide.html",
    "title": "Migration Guide | MixedReality-WebRTC Documentation",
    "keywords": "Migration Guide This guide details the main steps to migrate between major release versions. Migrate from 1.x to 2.x The 2.0 release introduces some significant changes in the API compared to the 1.0 release in order to include support for multiple media tracks per peer connection. Old C++ library Previously in the 1.x release the Microsoft.MixedReality.WebRTC.Native.dll module acted both as an implementation DLL for the C# library as well as a C++ library for direct use in end-user C++ applications. This double usage had diverging constraints which were making the internal implementation unnecessarily complex. Starting from the 2.0 release, this module now exposes a pure C API providing the core implementation of MixedReality-WebRTC. This library can be used from C/C++ programs with ease, as the use of a C API: allows use from C programs, which was not previously possible; sidesteps C++ complications with DLLs (destructor, template and inlining, etc. ). The library has been renamed to mrwebrtc.dll ( libmrwebrtc.so on Android) to emphasize this change and the fact there is no C++ library anymore, only a C library. C# library The C# library exposes a transceiver API very similar to the one found in the WebRTC 1.0 standard, and therefore familiarity with that standard helps understanding the API model. The API is not guaranteed to exactly follow the standard, but generally stays pretty close to it. Standalone audio and video sources Audio and video sources (webcam, microphone, external callback-based source) are now standalone objects not tied to any peer connection. Those objects, called track sources , can be reused by many audio and video tracks, which allows usage scenario such as sharing a single webcam or microphone device among multiple peer connections. Users must create the audio and video track source objects explicitly, independently of the peer connection. Device-based (microphone) audio track sources are created with the class method DeviceAudioTrackSource.CreateAsync() . Video track sources are created with class methods such as DeviceVideoTrackSource.CreateAsync() for device-based sources (webcam) or ExternalVideoTrackSource.CreateFromI420ACallback() for custom callback-based sources. Users are owning those track source objects and must ensure they stay alive while in use by any audio or video track, and are properly disposed after use ( IDisposable ). Standalone local track objects Audio and video tracks are now standalone objects, not owned by a peer connection, and not initially tied to any peer connection but bound to one on first use. Users must create the audio and video track objects explicitly, independently of the peer connection. Those tracks are created from a track source (see above). Audio tracks are created with LocalAudioTrack.CreateFromSource() . Video tracks are created with LocalVideoTrack.CreateFromSource() . Users are owning those track objects and must ensure they stay alive while in use by the peer connection they are bound to on first use (when first assigned to a transceiver; see below), and are properly disposed after use ( IDisposable ). Note that remote tracks remain owned by the peer connection which created them in response to an SDP offer or answer being received and applied, like in the previous 1.0 API. Transceivers Previously in the 1.0 API the peer connection was based on an API similar to the track-based API of the pre-standard WebRTC specification. The 2.0 release introduces a different transceiver -based API for manipulating audio and video tracks, which is more closely based on the WebRTC 1.0 standard. A transceiver is a \"media pipe\" in charge of the encoding and transport of some audio or video tracks. Each transceiver has a media kind (audio or video), and a sender track slot and a receiver track slot. Audio tracks can be attached to audio transceivers (transceivers with a Transceiver.MediaKind property equal to MediaKind.Audio ). Conversely, video tracks can be attached to video transceivers ( MediaKind.Video ). An empty sender track slot on a transceiver makes it send (if its direction include sending) empty data, that is black frames for video or silence for audio. An empty receiver track slot on a transceiver means the received media data, if any (depends on direction), is discarded by the implementation. A peer connection owns an ordered collection of audio and video transceivers. Users must create a transceiver with PeerConnection.AddTransceiver() . Transceivers cannot be removed; they stay attached to the peer connection until that peer connection is destroyed. Transceivers have a media direction which indicates if they are currently sending and/or receiving media from the remote peer. This direction can be set by the user by changing the Transceiver.DesiredDirection property. Changing a transceiver direction requires an SDP session renegotiation, and therefore changing the value of the Transceiver.DesiredDirection property raises a PeerConnection.RenegotiationNeeded event. After the session has been renegotiated, the negotiated direction can be read from the Transceiver.NegotiatedDirection read-only property. Media tracks are attached to and removed from transceivers. Unlike in 1.0, this does not require any session negotiation . Tracks can be transparently (from the point of view of the session) attached to a transceiver, detached from it, attached to a different transceiver, etc. without any of these raising a PeerConnection.RenegotiationNeeded event. A typical workflow with the transceiver API is as follow: The offering peer creates some transceivers and create an SDP offer. The offer is sent to the answering peer . The answering peer accepts the offer; this automatically creates the transceivers present in the offer that the offering peer created in step 1. The answering peer optionally add more transceivers beyond the ones already existing. The answering peer creates an SDP answer. The answer is sent back to the offering peer . The offering peer accepts the answer; this automatically creates any additional transceiver that the answering peer added in step 4. Migrating from the 1.x release, users typically: On the offering peer , replace calls to PeerConnection.AddLocalAudioTrack() with: a call to DeviceAudioTrackSource.CreateAsync() to create the device-based (microphone) audio track source. a call to LocalAudioTrack.CreateFromSource() to create the audio track that will bind the microphone audio of that source to the transceiver sending it to the remote peer. a call to PeerConnection.AddTransceiver() to add an audio transceiver. assigning the Transceiver.LocalAudioTrack property to the audio track. setting the Transceiver.DesiredDirection to Direction.SendReceive or Direction.SendOnly depending on whether they expect to also receive an audio track from the remote peer. On the answering peer , replace calls to PeerConnection.AddLocalAudioTrack() in the same way as on the offering peer, creating a track source and a track. However, do not immediately call PeerConnection.AddTransceiver() , but instead wait for the offer to create the transceiver. This requires some coordination, either implicit (pre-established transceiver order) or explicit (user communication between the 2 peers, for example using data channels), to determine on each peer which transceiver to use for which track. Note that the Unity library uses implicit coordination via media lines (declarative model). Proceed similarly for video tracks. Signaling SDP messages are now encapsulated in a data structure for clarity ( SdpMessage and IceCandidate ). Unity integration Unlike the C# library, which stays close to the WebRTC 1.0 in terms of transceiver behavior, the Unity integration takes a step back and build some convenience features on top of the transceiver API of the C# library. For the user, this avoids having to deal manually with pairing transceivers and tracks, and relying instead on a much simpler declarative model. Users are encouraged to follow the updated Unity tutorial to understand how to setup a peer connection with this new API. Peer connection The PeerConnection component now holds a collection of media lines , which can be described as a sort of \"transceiver intent\". These describe the final result the user intend to produce after an SDP session is fully negotiated in terms of transceivers and their attached tracks. The component internally manages adding transceivers when needed to match the user's media line description, as well as creating and destroying local sender tracks when a source is assigned by the user to the media line or removed from it. Signaler The PeerConnection.Signaler property has been removed; the Signaler component is now only a helper to creating custom signaling solution, but is not required anymore. As a result, the Signaler component now has a Signaler.PeerConnection property which must be set up by the user. This is true in particular for the NodeDssSignaler component which derives from the Signaler abstract base component. Video The LocalVideoSource component, which was previously using a webcam to capture video frames, has been renamed into the more explicit WebcamSource component. The WebcamSource component derives from the abstract VideoTrackSource component, which is also the base class for the callback-based sources like the SceneVideoSource component (previously: SceneVideoSender ) which captures its video frame from the rendering of any Unity Camera component. The RemoteVideoSource component has been renamed into the VideoReceiver component. The VideoSource component has been split into VideoTrackSource for local sources and VideoReceiver for remote sources. The MediaPlayer component has been renamed into the VideoRenderer component for clarity, has it only deal with video rendering and not audio. This also prevent a name collision with the Unity built-in component. This component can render any video source exposing some method taking an IVideoSource , and most notably VideoTrackSource (local video) and VideoReceiver (remote video). Audio The LocalAudioSource component, which was previously using a microphone to capture audio frames, as been renamed into the more explicit MicrophoneSource component. The MicrophoneSource component derives from the abstract AudioTrackSource component to enable future customizing usages and for symmetry with video. The RemoteAudioSource component has been renamed into the AudioReceiver component. This component now forwards its audio to a local Unity.AudioSource component on the same GameObject , which allows the audio to be injected into the Unity DSP pipeline and mixed with other audio sources from the scene. This in particular enables usage scenarios such as spatial audio (3D localization of audio sources for increased audio immersion). The AudioSource component has been split into AudioTrackSource for local sources and AudioReceiver for remote sources."
  },
  "manual/unity/helloworld-unity.html": {
    "href": "manual/unity/helloworld-unity.html",
    "title": "Hello, Unity world! | MixedReality-WebRTC Documentation",
    "keywords": "Hello, Unity world! In this tutorial we will create a simple Unity application based on the MixedReality-WebRTC Unity integration . Creating a new Unity project Importing MixedReality-WebRTC Creating a peer connection Creating a signaler Adding local video Adding remote video Establishing a connection"
  },
  "manual/unity/helloworld-unity-connection.html": {
    "href": "manual/unity/helloworld-unity-connection.html",
    "title": "Establishing a connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a connection Now that we have both media senders and receivers, we can attempt to establish a connection with a remote peer. There are a few requirements for this: We need 2 instances of the application running at the same time. Unfortunately the Unity Editor cannot be opened twice with the same project. As a workaround, we can build and deploy a standalone app on a device, even locally on the developer machine. Alternatively, we can use a second computer running another instance of the Unity Editor with an exact copy of this Unity project. The later is easier because we can still modify the project. The NodeDssSignaler component needs to be configured to know which remote peer to expect. This is due to the fact that this is a simple, easy, and not production-ready solution which does not offer any functionality to discover and select a remote peer. Instead it uses strings to identify the two peers. We can chose any two different strings. Warning When deploying to multiple devices, remember to change the IP address of the node-dss server to the IP address of the host machine in the NodeDssSignaler component, instead of the default 127.0.0.1 . Configuring the NodeDssSignaler The NodeDssSignaler has a Remote Peer Id property which contains the string identifying the remote peer to connect with. This should be filled with the identifier of the remote peer. The easiest way to obtain this identifier is to press Play and wait for the local signaler to start polling our node-dss server. If the server was started with the DEBUG=*dss environment variable set, it will output for each web request a message containing the identifier of the peer. Download and install Node.js from the official website . Clone the node-dss repository : git clone https://github.com/bengreenier/node-dss.git Configure and run it: cd node-dss set DEBUG=dss* npm install npm start The node-dss server should start and wait for incoming connections. At this point we can press Play in the Unity Editor to start polling the node-dss server, and retrieve from the shell terminal its identifier string, which corresponds to the string generated by the implementation since we left the NodeDssSignaler.LocalPeerId property empty: This string needs to be pasted into the Remote Peer Id property of the remote peer on the other machine. Repeat the process on the remote machine and paste the result on the Remote Peer Id property of the local machine. Warning This step is critical, and there is no safeguard. If any of the two signalers doesn't have the correct value for the identifier of the remote peer then the peer connection will not be established. Example: Peer LocalPeerId RemotePeerId Peer #1 PC1 MyMachine Peer #2 MyMachine PC1 Starting the WebRTC connection Now that both peers are connected to the node-dss signaling server and can exchange some SDP messages, it is time to start an actual WebRTC connection. The VideoChatDemo sample contains an example of creating a button and using the NodeDssSignalerUI.cs script to do that, but the task essentially boils down to one of the two peers, and one only, calling PeerConnection.StartConnection() . Once StartConnection() is called, the local peer will asynchronously generate a new SDP offer message, and raise the LocalSdpReadytoSend event to let the signaler send it. The NodeDssSignaler already handles this event and will automatically send it to the node-dss server. At this point the message should appear in the node-dss logs, which will buffer it until the remote peer polls for new data. After delivering the message, the remote peer will process it by calling HandleConnectionMessageAsync() , and generally create an SDP answer message, which will travel back to the local peer via the node-dss server and be handled by HandleConnectionMessageAsync() too. At this point if everything went right the WebRTC connection is established, and the audio and video tracks should start sending and receiving media data."
  },
  "manual/unity/helloworld-unity-createproject.html": {
    "href": "manual/unity/helloworld-unity-createproject.html",
    "title": "Creating a new Unity project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a new Unity project Go to the Unity archive page and download the latest 2018.4 version or 2019.4 version. Other Unity versions might work, but are not officially supported and have not been explicitly tested. Create a new Unity project by selecting the 3D Template and choosing a project name and location. Unity will create several folders at that location, the most important of which is the Assets folder. This contains all source files for the project, and this is where the MixedReality-WebRTC Unity integration will be installed. Next : Importing MixedReality-WebRTC"
  },
  "manual/unity/helloworld-unity-importwebrtc.html": {
    "href": "manual/unity/helloworld-unity-importwebrtc.html",
    "title": "Importing MixedReality-WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Importing MixedReality-WebRTC In order to use the Unity library, the following pieces are required: Native implementation : mrwebrtc.dll (one variant per platform and per architecture) C# library : Microsoft.MixedReality.WebRTC.dll (single universal module for all platforms and architectures) Unity library itself (scripts and assets) The Unity library is distributed as a package containing all those components, including the prebuilt binaries for all supported Unity platforms. The library package itself, as well as the optional samples package, can be imported into an existing Unity project in two ways: Using the Mixed Reality Feature Tool , a free Microsoft utility to manage Mixed Reality packages for Unity. This is the recommended way, which takes care of installing any required dependency and automatically download and install the package(s) into an existing Unity project. It can be downloaded from https://aka.ms/MRFeatureTool . A step-by-step process to discover and install Mixed Reality packages into an existing Unity project is detailed in the official documentation . Manually by downloading the package(s) from the GitHub Releases page . Packages imported that way are referred to by Unity as on-disk packages or local packages , and the installation process is described in the official Unity instructions . Note If an existing Unity project manifest already contains a Microsoft Mixed Reality entry in the scopedRegistries section, is is recommended that it be removed. In the following, we describe the latter method, which essentially consists in manually replicating the steps that the Mixed Reality Feature Tool would otherwise perform automatically for you. Download the latest release of the com.microsoft.mixedreality.webrtc package ( .tgz archive) from the GitHub Releases page . Direct donwload links are available at the bottom of each release section under the Assets drop-down list. Save it under a known location of your choice. At this point there is no need to unpack the archive, as Unity is able to directly import the package compressed. If using Unity 2018, expand the archive to a folder of your choice. Unity 2019+ supports directly installing a package from a .tgz archive, but this option is not available in Unity 2018. Open your Unity project, and in the main menu navigate to Window > Package Manager . In the Package Manager window: For Unity 2019+ , click on the \"+\" button in the top left corner and select Add package from tarball... from the popup menu. Then select the .tgz archive of the com.microsoft.mixedreality.webrtc package downloaded earlier. For Unity 2018 , or if you want to install the package from an expanded archive folder, click on the \"+\" button located in the bottom left corner and select Add package from disk... from the popup menu. Then select the folder where you expanded the .tgz archive of the com.microsoft.mixedreality.webrtc package downloaded earlier. Repeat the process to install the optional com.microsoft.mixedreality.webrtc.samples package if needed, which contains optional samples showing how to use the MixedReality-WebRTC Unity library. Next : Creating a peer connection"
  },
  "manual/unity/helloworld-unity-localvideo.html": {
    "href": "manual/unity/helloworld-unity-localvideo.html",
    "title": "Adding local media | MixedReality-WebRTC Documentation",
    "keywords": "Adding local media In this section we perform 3 tasks: Add local video Add local audio Add a video renderer to render the local video Local video There are three different concepts covered by the term local video : Locally generating some video frames, often by capturing them from a video capture device ( e.g. webcam) available on the local host, or alternatively obtaining those frames by any other mean (procedurally generated, captured from another source, etc. ). This corresponds to the concept of a video track source in WebRTC terminology (or simply video source for short). Sending those video frames to the remote peer, which is the most interesting aspect and requires WebRTC, and corresponds to the concept of video track in WebRTC terminology. Accessing locally the captured video frames, for example to display them for user feedback. This is application-dependent, and even within one given application can generally be toggled ON and OFF by the user. This corresponds to the concept of video frame callback in MixedReality-WebRTC, and is not an official WebRTC concept from the standard. Capturing some video feed from a local video capture device is covered by the WebcamSource component, which represents a video track source capturing its video frames from a webcam. Note The term webcam as used in the context of the WebcamSource component colloquially refers to any local video capture device , and is an abuse of language to shorten the latter. This term therefore includes other capture devices not strictly considered webcams, like for example the HoloLens 1 and HoloLens 2 cameras, or an Android phone built-in camera. Because the WebcamSource component derives from the abstract VideoTrackSource component, which encapsulates a WebRTC video track source, that source can be attached to a media line of a peer connection. By doing so, the media line will automatically create and manage a local video track, which covers the second concept of streaming video to a remote peer. Finally, the VideoTrackSource component can also be rendered locally (in parallel to being streamed to the remote peer) via an optional video player, covering the last concept if needed. Adding a webcam source For clarity, we will create a new game object and add a WebcamSource component. It may sound superfluous at the moment to create a new game object, as we could add the webcam source to the same game object already owning the peer connection component, but this will prove more clear and easy to manipulate later. In the Hierarchy window, select Create > Create Empty . In the Inspector window, rename the newly-created game object to something memorable like \"LocalVideoPlayer\". Press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > Webcam Source . The webcam source component contains several interesting properties. We detail these properties below for reference, but for now we can leave all of them to their default values. Important When using Mixed Reality Capture (MRC) on HoloLens, the Unity project needs to be configured to Enable XR support . This generates an exclusive-mode application (fullscreen), which is required to get access to MRC, for security and privacy reasons. Otherwise Unity by default generates a shared application (slate app), and MRC access will be denied by the OS. Video capture properties The video capture properties configure how the component accesses the webcam and captures video frames from it, independently of WebRTC. The Capture format combobox, which corresponds to the WebcamSource.FormatMode property, allows switching between two different modes to control how the video capture format (resolution and framerate, essentially) is chosen: In Automatic mode, the best video capture format is automatically selected. In general this means the first format available is chosen. On HoloLens devices, some extra steps are taken internally to force the use of the low-power video profile and reduce the capture resolution in order to save the battery and lower the CPU usage. On Android the closest resolution to 720p @ 30fps is selected. In Manual mode, the user can apply a set of constraints to the list of capture formats the device supports. This allows restricting the video capture formats the selection algorithm can choose from to a subset of all the formats supported by the video capture device, and even to a single known format if needed. Note however that over-constraining the algorithm may result in no format being available, and the WebcamSource component failing to open the video capture device. In general it is recommended not to assign hard-coded resolution / framerate constraints at edit time, unless the application only targets a specific kind of device with well-known immutable capture capabilities. Instead it is recommended to enumerate at runtime with DeviceVideoTrackSource.GetCaptureFormatsAsync() the capture formats actually supported by the video capture device, then use constraints to select one or some of those formats. The Enable Mixed Reality Capture (MRC) checkbox, which corresponds to the WebcamSource.EnableMixedRealityCapture property, tells the component it should attempt to open the video capture device with MRC enabled, if supported. If the device does not support MRC, then this is silently ignored. See the important remark about MRC and exclusive mode application above. Local audio The concept of local audio is very similar to the one of local video and can also be decomposed into generating some audio frames, sending them to a remote peer through an audio track, and optionally outputting some local feedback to the user. However currently the last part is not available; there is no local audio callback on audio track sources. This is a limitation of the underlying implementation, but has very few use cases anyway. Capturing some audio feed from a local audio capture device is covered by the MicrophoneSource component, which represents an audio track capturing its audio frames from a microphone. Adding a microphone source Again for clarity we will create a new game object and add a MicrophoneSource component. In the Hierarchy window, select Create > Create Empty . In the Inspector window, rename the newly-created game object to something memorable like \"MicrophoneSource\". Press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > Microphone Source . The microphone source component contains few properties. In particular there is currently a single option to configure the audio capture : enabling Automatic Gain Control . We can leave all properties with their default values, as it is generally a good idea to have gain automatically adjusted by the implementation. Adding transceivers So far the components added are only creating audio and video track sources, but there is no link with any peer connection. This link is achieved through transceivers . A transceiver is a \"media pipe\" (audio or video) through which the peer connection transport the media between the local and remote peers. The transceivers are configured on the PeerConnection component directly, and reference the AudioTrackSource and VideoTrackSource components to be used to send media, and the AudioReceiver and VideoReceiver components to be used to receive media. Let's add an audio transceiver and a video tranceiver, and attach to them the microphone and webcam sources to instruct the peer connection to send those media to the remote peer: In the Hierarchy window, select the game object with the peer connection component. In the Inspector window, under the Transceivers list, press the + Audio and the + Video to add an audio transceiver and a video transceiver, respectively. For the first transceiver (audio), assign the Audio Track Source property to the MicrophoneSource component created previously. For the second transceiver (video), assign the Video Track Source property to the WebcamSource component created previously. You can also change the sender track name by expanding the small arrow on the left of each source. This is optional; the implementation will generate a track name if none is provided. Each transceiver displays an icon indicating the type of media (audio or video). Under that media icon, a direction icon indicates the desired direction of the transceiver based on the configured sources and receivers. Here we added a source to each transceiver, therefore the direction icon points to the right (send) only. Adding a video player We said before that the VideoTrackSource base component can be rendered locally via a video player. The video track source exposes some C# event to access the video frames that the video track consumes (for streaming them to the remote peer); these frames can also be accessed for local rendering. In order to simplify rendering those video frames, MixedReality-WebRTC offers a simple VideoRenderer component which uses some Unity Texture2D to render the video frames. The textures are then applied to the material of a Renderer component via a custom shader to be displayed in Unity on a mesh. Important The local audio captured by the MicrophoneSource component is directly sent to the remote peer, and unlike video there is currently no possible way to access the raw audio frames captured by this component. This is a limitation of the underlying audio capture implementation (see issue #92 ). It is anyway critical to add this component to capture the audio and send it to the remote peer. Note Unity also has a built-in Video Player component; unfortunately that component is designed to play local videos from disk or from known HTTP sources, including transport management and decoding, and cannot be used to simply play already-decoded raw video frames obtained from WebRTC. Let's add a VideoRenderer component on our game object: In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > VideoRenderer This time, however, Unity will not create the component. It will instead display a somewhat complex error message: What the message means is that the VideoRenderer component requires a Renderer component on the same game object, and Unity lists all possible implementation of a renderer (all classes deriving from Renderer ). Although all renderers might work, in our case the most simple is to add a MeshRenderer component. If you are familiar with Unity, you also know that the renderer needs a source mesh in the form of a MeshFilter component. So for each component, in the Inspector window, press the Add Component button at the bottom of the window, and select successively and in order: Mesh > Mesh Filter Mesh > Mesh Renderer MixedReality-WebRTC > Video Renderer After that, set the component properties as follows: In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh. This is a simple square mesh on which the texture containing the video feed will be applied. The built-in Quad mesh size is quite small for rendering a video, so go to the Transform component and increase the scale to (4,3,1) . This will produce a 4:3 aspect ratio for the mesh, which is generally, if not equal to, at least pretty close to the actual video frame aspect ratio, and therefore will reduce/prevent distorsions. In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Runtime/Materials/ folder. This instructs Unity to use that special material and its associated shader to render the video texture on the quad mesh. More on that later. In the Video Renderer component, set the Source property to the webcam source component previously added to the same game object. This instructs the video renderer to connect to the webcam source for retrieving the video frames that it will copy to the video texture for rendering. This should result in a setup looking like this: And the Game view should display a black rectangle, which materializes the quad mesh: A word about the YUVFeedMaterial material here. The video frames coming from the webcam source are encoded using the I420 format. This format contains 3 separate components : the luminance Y, and the chroma U and V. Unity on the other hand, and more specifically the GPU it abstracts, generally don't support directly rendering I420-encoded images. So the VideoRenderer component is uploading the video data for the Y, U, and V channels into 3 separate Texture2D objects, and the YUVFeedMaterial material is using a custom shader called YUVFeedShader (Unlit) to sample those textures and convert the YUV value to an ARGB value on the fly before rendering the quad with it. This GPU-based conversion is very efficient and avoids any software processing on the CPU before uploading the video data to the GPU. This is how WebcamSource is able to directly copy the I420-encoded video frames coming from the WebRTC core implementation into some textures without further processing, and VideoRenderer is able to render them on a quad mesh. Test the local video At this point the webcam source and the media player are configured to open the local video capture device (webcam) of the local machine the Unity Editor is running on, and display the video feed to that quad mesh in the scene. The audio is also captured from the microphone, but since there is no feedback for this, there is no opportunity to observe it. Press the Play button in the Unity Editor. After a few seconds or less (depending on the device) the video should appear over the quad mesh. Next : Adding remote video"
  },
  "manual/unity/helloworld-unity-peerconnection.html": {
    "href": "manual/unity/helloworld-unity-peerconnection.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection From this point we start building the scene. Because the MixedReality-WebRTC components are installed, and because we work now almost exclusively inside Unity, for brevity we will use the term component to designate a Unity component, that is a class deriving from MonoBehaviour . Create a new GameObject with a PeerConnection component: In the Hierarchy window , select Create > Create Empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > PeerConnection to add a PeerConnection component to that new object. At the top of the Inspector window, rename the newly-created game object to something memorable like \"MyPeerConnection\". You can also rename this object in the Hierarchy window directly (for example by pressing F2 when selected). The PeerConnection component provided by the Unity integration of MixedReality-WebRTC has various settings to configure its behaviour. For the moment you can leave the default values. We will come back to it later in particular to add some transceivers. Next : Creating a signaler"
  },
  "manual/unity/helloworld-unity-remotevideo.html": {
    "href": "manual/unity/helloworld-unity-remotevideo.html",
    "title": "Adding remote media | MixedReality-WebRTC Documentation",
    "keywords": "Adding remote media Unlike local video, the remote video track is controlled by the remote peer who will decide if and when to add the track to the connection. On the receiver side, the only thing to decide is whether or not to handle the video data received through that track. Similar observations can be made for audio. The VideoReceiver component is used to expose a remote video track. And similarly to the VideoTrackSource , it can be added to a VideoRenderer component to render the content of the video feed it receives from the remote peer. Same goes for the AudioReceiver component for audio. Remote video Like we did for the local video feed, we create a new game object with a VideoReceiver component: In the Hierarchy window, select Create > Create Empty . Rename it to something memorable like \"RemoteVideoPlayer\". Go to the Transform component and increase the scale to (4,3,1) to get a 4:3 aspect ratio again. In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > Video Receiver . Next we add some video renderer. This is again similar to the video track source: the video receiver only exposes some frame queue which gets populated using the video frames coming from the remote peer, but the component does not do any rendering/output by itself. Instead we can use again a VideoRenderer to render those video frames. The VideoRenderer component exposes the video frames as 3 animated Texture2D , one for each of the YUV components, which need a mesh to be applied to. Add a Mesh Filter component, a Mesh Renderer component, and a Video Renderer component. In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh, which is the most simple mesh we can use (2 triangles) to display a texture. But any other mesh would do. In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Runtime/Materials/ folder. This material uses a custom shader to convert the 3 textures of the VideoRenderer component (YUV, one per video component) into some RGB value for rendering. In the Video Renderer component, set the Source property to the video receiver component previously created. We can optionally assign some text meshes to display statistics if we have some. The Inspector window should now look like this: Currently the two game objects holding the local and remote video players are located at the same position. Adjust them so that they don't overlap; for example set their X position to X=+/-2.5 . We can also add some slight rotation to face the camera, for example RY=+/-10 . Now we should be able to see in the Scene window our two black squares representing the local and remote media players: Remote audio Similar to video, we create a new game object with an AudioReceiver component: In the Hierarchy window, select Create > Create Empty . Rename it to something memorable like \"RemoteAudio\". In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > Audio Receiver . The Audio Receiver component requires a Unity Audio Source component to inject the audio it receives from the remote peer into the Unity DSP pipeline for audio rendering. Setting up the media lines Like for the audio and video sources, the receiver components need to be associated with the peer connection through a transceiver media line. Select the PeerConnection component again, and assign the new receivers to the audio and video media lines previously created. Next : Establishing a connection"
  },
  "manual/unity/helloworld-unity-signaler.html": {
    "href": "manual/unity/helloworld-unity-signaler.html",
    "title": "Creating a signaler | MixedReality-WebRTC Documentation",
    "keywords": "Creating a signaler The WebRTC standard specifies how a peer-to-peer connection can be established using the Session Description Protocol (SDP) , but does not enforce a particular signaling solution to discover and select a remote peer, and to send to and receive from it the SDP messages necessary to establish that connection. MixedReality-WebRTC offers a built-in solution in the form of the NodeDssSignaler component, but also allows any other custom implementation to be used. For this tutorial, we will use the NodeDssSignaler component for simplicity. Caution NodeDssSignaler is very simple and useful for getting started quickly and debugging, but it is worth noting that this is not a production-quality solution and should not be used in production . In particular, this component offers no security whatsoever . All communications are taking place in clear text over HTTP, and with no authentication , as all it requires to connect with a remote peer is to know its identifier. Do not be fooled by the fact that WebRTC supports encryption, as it would be very easy for an attacker to bypass it by compromising the signaler. Remember that any security solution is no better than its weakest link, and NodeDssSignaler is that link. It must be replaced with a secure solution when moving to production. The NodeDssSignaler component uses a Node.js server with a simple polling model where both peers are constantly checking if a message is available. The server implementation is called node-dss and is freely available on GitHub. Install and run node-dss The node-dss repository has instructions on how to install and run the server , which essentially boils down to installing Node.js, downloading the code, and running: set DEBUG=dss* npm install npm start This opens a console window which will output all requests received from all peers. Leave that console window open and the node-dss server running for the following, and go back to the Unity editor. Creating a NodeDssSignaler The NodeDssSignaler component can be added to the existing GameObject , or to a new one. There is no fundamental difference, and this is mostly a matter of taste. For this tutorial we will create a separate game object to separate it from the peer connection in the Hierarchy window. Create a new GameObject with a NodeDssSignaler component: In the Hierarchy window , select Create > Create empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > NodeDssSignaler to add a NodeDssSignaler component to that new object. Optionally rename the game object to something easy to remember like \"MySignaler\" to easily find it in the Hierarchy window. By default the NodeDssSignaler component is configured to connect to a node-dss server running locally on the developper machine at http://127.0.0.1:3000/ and poll the server every 500 milliseconds to query for available messages. If using another machine, the HTTP address must be changed. Connecting the signaler Now that a signaling solution is available, the last step is to assign the Signaler.PeerConnection property of the peer connection component created earlier: In the Hierarchy window, make sure the game object with the peer connection component is selected, then in the Inspector window find the Peer Connection property and click on the circle to its right to bring the asset selection window Select the Scene tab of that window Select the game object with the PeerConnection component on it. The name of the GameObject containing the component now appears next to the Peer Connection property, followed by \" (PeerConnection) \" to indicate the actual value is the component of that game object. The signaler object should now appear in the Inspector window of the peer connection game object. At that point the peer connection is fully configured and ready to be used. However audio and video tracks are not added automatically, so there is little use for that peer connection. Next we will look at connecting a local webcam and microphone to provide some video and audio track to send through to the peer connection. Next : Adding local video"
  },
  "manual/unity/unity-audioreceiver.html": {
    "href": "manual/unity/unity-audioreceiver.html",
    "title": "Unity AudioReceiver component | MixedReality-WebRTC Documentation",
    "keywords": "Unity AudioReceiver component The AudioReceiver Unity component represents a single audio track received from the remote peer through an established peer connection. The AudioReceiver component in itself does not render the received audio from the remote peer. Instead, it makes that audio available via the IAudioSource interface. To render the remote audio, add an AudioRenderer component and assign the AudioStreamStarted and AudioStreamStopped events to its StartRendering() and StopRendering() methods, respectively. This will allow the AudioRenderer to tap into the raw audio received from the remote peer, and redirect it to an AudioSource component located on the same GameObject . Note that the AudioSource component does not have any AudioClip assigned; instead the OnAudioFilterRead() callback is used internally by the AudioRenderer to inject the audio data into the AudioSource component, and therefore into the Unity audio DSP pipeline. Note When using the C# library of MixedReality-WebRTC, by default remote audio tracks are automatically rendered internally to the default audio device. In Unity, the expectation instead is that all audio shall be played via the Unity DSP pipeline using the above setup. So to prevent duplicate audio playback of remote audio tracks, PeerConnection.InitializePluginAsync() will disable the default internal audio rendering by calling OutputToDevice(false) on all RemoteAudioTrack objects created internally."
  },
  "manual/unity/unity-components.html": {
    "href": "manual/unity/unity-components.html",
    "title": "Unity components | MixedReality-WebRTC Documentation",
    "keywords": "Unity components The Unity components provide some idiomatic wrapping over the C# MixedReality-WebRTC library. Component Description PeerConnection Encapsulates a single peer-to-peer connection to a remote peer Signaler Abstract base class of components which manage the signaling messages to allow the peer connection to establish NodeDssSignaler Simple testing / debugging Signaler implementation component based on node-dss VideoSource Component providing a hook to the local and remote video tracks of a peer connection VideoTrackPlayer Component bridging a Unity MeshRenderer with a video track from a VideoSource component Components by feature area Connection The most important component is Microsoft.MixedReality.WebRTC.Unity.PeerConnection which encapsulate the connection to a single remote peer. The peer connection works in coordination with a Signaler component, for example Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler , which handles the message transport for the session establishment. Video The entry point for video tracks is the VideoSource component, which is associated with a given PeerConnection and handles its video-related signals. The video source in turn makes use of one or two VideoTrackPlayer to render the content of the local and remote video tracks if needed, although this is optional. Video frames are provided by the underlying WebRTC.PeerConnection to the VideoSource via its frame events. The video source then fills a VideoFrameQueue for each video track player associated with it. The player use that shared queue to read back the frame and display it using a custom shader. Audio Todo... List of components Microsoft.MixedReality.WebRTC.Unity.PeerConnection This component abstracts a WebRTC peer connection and encapsulates the lower level Microsoft.MixedReality.WebRTC.PeerConnection object from the C# library. This is the main entry point for establishing a connection. It contains the list of ICE servers ( Interactive Connectivity Establishment ) used to punch through NAT, as well as some key events like OnInitialized and OnShutdown which mark the beginning and end of the connection from the point of view of the user. Microsoft.MixedReality.WebRTC.Unity.Signaler This abstract base component is used by the peer connection to establish a connection with a remote peer. The peer connection needs one concrete implementation derived from this class to be specified in its Signaler property. Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler THIS SHOULD NOT BE USED FOR PRODUCTION. This components is used for debugging and testing as a concrete implementation of a Signaler component. It is based on the node-dss protocol and NodeJS service. It is very simple and helps developers starting, but lacks many features. Microsoft.MixedReality.WebRTC.Unity.VideoSource This component controls sending the local video track through the peer connection, and handles receiving a remote video track. For the local video, it controls whether or not a track is added to the WebRTC stream, and optionally provides a VideoTrackPlayer with frames to render that local video. For the remote video, it controls whether or not to handle the received feed and send it to a VideoTrackPlayer for rendering. Note that the local peer cannot control whether or not the remote peer sends a remote track; it can only ignore it if not interested (and ideally should probably tell the remote peer that it should stop sending it, although this is application specific logic). Microsoft.MixedReality.WebRTC.Unity.VideoTrackPlayer This components bridges a raw video frame feed from a VideoSource to a Unity MeshRenderer for display. The component can limit the framerate of the video playback, and optionally display some statistics about it. The associated MeshRenderer on the same GameObject typically uses a YUVFeedMaterial to display the YUV-encoded feed uploaded to the main texture of that material by the video track player component."
  },
  "manual/unity/unity-integration.html": {
    "href": "manual/unity/unity-integration.html",
    "title": "Unity library overview | MixedReality-WebRTC Documentation",
    "keywords": "Unity library overview The Unity library offers a simple way to add real-time communication to an existing Unity application. MixedReality-WebRTC provides a collection of Unity componenents ( MonoBehaviour -derived classes) which encapsulate objects from the underlying C# library , and allow in-editor configuration as well as establishing a connection to a remote peer both in standalone and in Play mode . The PeerConnection component is the entry point for configuring and establishing a peer-to-peer connection. The peer connection component makes use of a signaler (generally derived from the Signaler base class utility) to handle the SDP messages dispatching. This process continues even after the connection started, as it handles all tracks and transceivers (re-)negotiations, even after a direct peer-to-peer connection for media transport is established. Audio and video sources capturing from a local audio (microphone) and video (webcam) capture device are handled by the MicrophoneSource and WebcamSource components, respectively. Those sources can be shared with multiple peer connections. For remote tracks, the AudioReceiver and VideoReceiver respectively handle configuring a remote audio and video track streamed from the remote peer. Unlike the previous track sources, those component encapsulate tracks and are tied with a specific peer connection. Rendering of both local and remote video sources can be handled by the VideoRenderer utility component, which connects to a video renderer source and renders it using a custom shader into a Unity Texture2D object, which can be later applied on a mesh to be rendered in the scene. Note The local audio is never played out locally (no local frame callback), only streamed to the remote peer. The local video can be played locally by registering a frame callback with the video track source."
  },
  "manual/unity/unity-localvideosource.html": {
    "href": "manual/unity/unity-localvideosource.html",
    "title": "Unity WebcamSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity WebcamSource component The WebcamSource Unity component represents a single video track source generating video frames captured from a local video capture device (webcam). Properties Video capture WebcamDevice (code only) Description of the video capture device to use, colloquially referred to as webcam for short, even if other non-webcam capture devices are also supported, like the HoloLens 1 and HoloLens 2 cameras. Valid device unique identifiers can be enumerated with DeviceVideoTrackSource.GetCaptureFormatsAsync() and correspond to the VideoCaptureDevice.id field. Note that this property is not exposed to the Unity editor, as devices should be enumerate at runtime to support the various video capture device configurations of the host device. FormatMode (\"Capture format\" in Inspector) Select between automated and manual video capture format selection mode. In automated mode, the implementation selects the best video capture format. In manual mode, some constraints can be specified to restrict the video capture formats the implementation might consider using, and even force one particular format. Constraints (code only) Optional resolution and framerate constraints to apply when selecting a video capture format. This allows restricting the set of capture formats the implementation considers when selecting a capture format to use, possibly even forcing a single one. Constraints reducing the number of matching capture formats to zero will make opening the device fail, therefore it is recommended to enumerate the supported capture formats with DeviceVideoTrackSource.GetCaptureFormatsAsync . VideoProfileId (code only) [UWP only] Optional unique identifier of the video profile to use to enumerate the video capture formats. This allows selecting a video profile other than the default one, which sometimes enables access to other resolutions and framerates, and is required on HoloLens 2 to use the low-power capture formats. It is recommended to specify either VideoProfileId or VideoProfileKind , but not both. VideoProfileKind (code only) [UWP only] Optional video profile kind to use to enumerate the video capture formats. This allows selecting a video profile other than the default one, which sometimes enables access to other resolutions and framerates, and is required on HoloLens 2 to use the low-power capture formats. It is recommended to specify either VideoProfileId or VideoProfileKind , but not both. EnableMixedRealityCapture [UWP only] On platforms supporting Mixed Reality Capture (MRC) like HoloLens 1st generation and 2nd generation, instruct the video capture module to enable this feature and produce a video stream containing the holograms rendered over the raw webcam feed. This has no effect if the local device does not support MRC. EnableMRCRecordingIndicator [UWP only] On platforms supporting Mixed Reality Capture (MRC) like HoloLens 1st generation and 2nd generation, and when EnableMixedRealityCapture is true , enable the on-screen recording indicator (red circle) on the device while the camera is capturing. VideoStreamStarted Event raised when the video track source starts capturing frames. VideoStreamStopped Event raised when the video track source stops capturing frames."
  },
  "manual/unity/unity-mediaplayer.html": {
    "href": "manual/unity/unity-mediaplayer.html",
    "title": "Unity VideoRenderer component | MixedReality-WebRTC Documentation",
    "keywords": "Unity VideoRenderer component The VideoRenderer Unity component is a utility component to render some video frames into a Unity texture. To use the component, either call its StartRendering() and StopRendering() methods from code, or connect them to Unity events such as the VideoTrackSource.VideoStreamStarted and VideoTrackSource.VideoStreamStopped events. All video source components derive from VideoTrackSource and therefore expose those events. Property Description Video MaxFramerate Maximum number of video frames per second rendered. Extra frames coming from the video source passed to StartRendering() are discarded. Statistics EnableStatistics Enable the collecting of video statistics by the media player. This adds some minor overhead. FrameLoadStatHolder Reference to a TextMesh instance whose text is set to the number of incoming video frames per second pulled from the video source into the media player's internal queue. FramePresentStatHolder Reference to a TextMesh instance whose text is set to the number of video frames per second dequeued from the media player's internal queue and rendered to the texture(s) of the Renderer component associated with this media player. FrameSkipStatHolder Reference to a TextMesh instance whose text is set to the number of video frames per second dropped due to the media player's internal queue being full. This corresponds to frames being enqueued faster than they are dequeued, which happens when the source is overflowing the sink, and the sink cannot render all frames. See also : VideoTrackSource WebcamSource SceneVideoSource"
  },
  "manual/unity/unity-microphonesource.html": {
    "href": "manual/unity/unity-microphonesource.html",
    "title": "Unity MicrophoneSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity MicrophoneSource component The MicrophoneSource Unity component represents a single audio track obtaining its data from a local audio capture device (microphone). The component controls both the capture device and the track it feeds. The Auto Gain Control property allows enabling automated gain control (AGC), an audio processing step which dynamically raises or lowers the audio gain to attempt to maintain a constant volume. The MicrophoneSource must be added to an audio transceiver of a PeerConnection component to allow that peer connection to send the recorded audio to the remote peer."
  },
  "manual/unity/unity-peerconnection.html": {
    "href": "manual/unity/unity-peerconnection.html",
    "title": "Unity PeerConnection component | MixedReality-WebRTC Documentation",
    "keywords": "Unity PeerConnection component The PeerConnection Unity component encapsulates a single peer connection between the local application and another remote Unity peer application. Note The C# library also has a PeerConnection class, which this components build upon. Property Description Behavior settings Auto Log Errors To Unity Console Add an event listener to the OnError event which calls Debug.LogError() to display the error message in the Unity console. Signaling Ice Servers A list of ConfigurableIceServer elements representing the list of ICE servers to use to establish the peer connection. The list can be empty, in which case only a local connection will be possible. Ice Username Optional user name for TURN servers authentication. Ice Credential Optional password for TURN servers authentication. Media Auto Create Offer on Renegotiation Needed Automatically call StartConnection() to create a new offer and start a new session negotiation when the renegotiation needed event is raised. If not set, the user need to call StartConnection() manually instead to (re-)negoitate a session. Events On Initialized Event raised once the peer connection is successfully initialized as a result of the component being enabled, to indicate that the peer connection component is ready for use. On Shutdown Event raised when the peer connection has been destroyed as a result of the component being disabled. After this event is raised the peer connection cannot be used until it is initilized again. On Error Event raised when an error occur in the peer connection."
  },
  "manual/unity/unity-remotevideosource.html": {
    "href": "manual/unity/unity-remotevideosource.html",
    "title": "Unity VideoReceiver component | MixedReality-WebRTC Documentation",
    "keywords": "Unity VideoReceiver component The VideoReceiver Unity component represents a single video track received from the remote peer through an established peer connection."
  },
  "manual/unity/unity-signaler.html": {
    "href": "manual/unity/unity-signaler.html",
    "title": "Unity Signaler component | MixedReality-WebRTC Documentation",
    "keywords": "Unity Signaler component The Signaler Unity component is an abstract base class used as an helper for implementing a custom component for a given signaling solution. It is not strictly required, but provides some utilities which make it easier to write an implementation. Property Description PeerConnection A reference to the PeerConnection Unity component that this signaler should provide signaling for. Implementing a custom signaler A custom signaling solution can derive from the abstract base Signaler class for simplicity, or be any other Unity component or C# class. When deriving from the Signaler class, a derived class needs to: Implement the SendMessageAsync(SdpMessage) and SendMessageAsync(IceCandidate) abstract methods to send messages to the remote peer via the custom signaling solution. Handle incoming messages from the remote peer: call HandleConnectionMessageAsync(SdpMessage) on the Unity peer connection component when receiving an SDP offer or answer message. call AddIceCandidate(IceCandidate) on the underlying C# peer connection object to deliver ICE candidates to the local peer implementation. When implementing a custom signaling solution from scratch without using the Signaler class, the custom implementation must, in addition of the above message handling, replace the work done by SendMessageAsync() : Listen to the LocalSdpReadytoSend and IceCandidateReadytoSend events. Send to the remote peer, by whatever mean devised by the implementation, some messages containing the data of those events, such that the remote peer can handle them as described above."
  }
}